{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["niSmaXftJebn","7Lv-g0akNbpk"],"authorship_tag":"ABX9TyMSIKYdG+0RU2UU9FgdB79P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_IbYupXsMipD"},"source":["### **Transfer Learning - Fine Tuning Pretrained Models**\n","though spacy contains pretrained model, there is a way via which you can retrain those models on your own dataset"]},{"cell_type":"markdown","metadata":{"id":"niSmaXftJebn"},"source":["#### **Reading The Dataset** - **Conllu library**\n","\n","Training dataset contains already tagged sentence, such a file is stored as .conll file and hence to read such a file we need to use a conllu library.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vCEkLuc4Jebp","executionInfo":{"status":"ok","timestamp":1626538837915,"user_tz":-330,"elapsed":4173,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"4b58ef77-a07c-4132-e9c0-3da35d2ebc59"},"source":["!pip install conllu"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting conllu\n","  Downloading https://files.pythonhosted.org/packages/ae/be/be6959c3ff2dbfdd87de4be0ccdff577835b5d08b1d25bf7fd4aaf0d7add/conllu-4.4-py2.py3-none-any.whl\n","Installing collected packages: conllu\n","Successfully installed conllu-4.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lgt5qSstJebq"},"source":["import conllu"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NtwkbRbEaczD"},"source":["##### Reading dataset 1"]},{"cell_type":"code","metadata":{"id":"J8C243wdJebs"},"source":["# reading a conll file with specific fields\n","path = \"/content/drive/MyDrive/001 My Skills/002 CS Engineering   Automated Math (BPHC)/004 Data Science (DS)   Artificial Intelligence (AI)/004 Textual Data (Unstructured Data) (Sequential Data)/001 Mono Lingual Language /001 English/003 NLP Tasks/001 NLU Tasks/1. Word Level Classification/Parts of Speech (POS) Tagging/train_dataset.conll\"\n","\n","import io\n","with open(path, \"r\", encoding=\"utf-8\") as data_file:\n","  sentlist = list(conllu.parse_incr(data_file,fields = ['form','langtag','postag']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VIMobrTtJebt","executionInfo":{"status":"ok","timestamp":1626541753118,"user_tz":-330,"elapsed":332,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"525e3399-bff3-4ee3-fbe8-cf6ec47a5dd1"},"source":["for sent in sentlist:\n","  print(\"Sentence is: {}\".format(sent))\n","  for token in sent:\n","    print(token['form'],token['langtag'], token['postag'])\n","  break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sentence is: TokenList<Pak, is, in, confused, state, of, mind, ., hui, ya, nahi, is, par, contradictory, statements, aa, rahe, hain, ., Nahi, huyi, to, tension, mai, kyon, ?>\n","Pak rest PROPN\n","is en VERB\n","in en ADP\n","confused en ADJ\n","state en NOUN\n","of en ADP\n","mind en VERB\n",". rest X\n","hui hi VERB\n","ya hi CONJ\n","nahi hi PART_NEG\n","is hi DET\n","par hi ADP\n","contradictory en NOUN\n","statements en NOUN\n","aa hi VERB\n","rahe hi VERB\n","hain hi VERB\n",". rest X\n","Nahi hi PART_NEG\n","huyi hi PART\n","to hi PART\n","tension en NOUN\n","mai hi ADP\n","kyon hi PRON_WH\n","? rest X\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hp2575incwqM","executionInfo":{"status":"ok","timestamp":1626541856004,"user_tz":-330,"elapsed":334,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"05e751ee-a64f-466d-8877-d26f828972ce"},"source":["token"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'form': '?', 'langtag': 'rest', 'postag': 'X'}"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"EyO4N0j4agVz"},"source":["##### Reading dataset 2"]},{"cell_type":"code","metadata":{"id":"AtRedU3lJebq"},"source":["# reading a CoNLL file without explicitly specifiying the fields\n","path = \"/content/drive/MyDrive/001 My Skills/002 CS Engineering   Automated Math (BPHC)/004 Data Science (DS)   Artificial Intelligence (AI)/004 Textual Data (Unstructured Data) (Sequential Data)/001 Mono Lingual Language /001 English/003 NLP Tasks/001 NLU Tasks/1. Word Level Classification/Parts of Speech (POS) Tagging/train_dataset_2.conllu\"\n","\n","import io\n","with open(path, \"r\", encoding=\"utf-8\") as data_file:\n","    sentlist = list(conllu.parse_incr(data_file))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RsB1csQQJebr","executionInfo":{"status":"ok","timestamp":1626540941846,"user_tz":-330,"elapsed":408,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"c296622d-ee91-4278-a92f-5025888f5a56"},"source":["# iteraring through all the lines of a conll file read using conllu.parse\n","for sent in sentlist:\n","  print(sent)\n","  for token in sent:\n","    print(token['form'], token['upos'],token['misc']['ChunkId'])\n","    # token = sent[0]\n","  break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TokenList<यह, एशिया, की, सबसे, बड़ी, मस्जिदों, में, से, एक, है, ।>\n","यह DET NP\n","एशिया PROPN NP\n","की ADP NP\n","सबसे ADV NP2\n","बड़ी ADJ NP2\n","मस्जिदों NOUN NP2\n","में ADP NP2\n","से ADP NP2\n","एक NUM NP3\n","है AUX VGF\n","। PUNCT BLK\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ND4dMlBtJebs","executionInfo":{"status":"ok","timestamp":1625581080089,"user_tz":-330,"elapsed":370,"user":{"displayName":"Prashant Kodali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLR7kqr_lvUgRRZBGdR7UZo6oiaBC-TEd7nXt_Xw=s64","userId":"13858115222436552549"}},"outputId":"3d5443ab-34f4-4990-e7b3-20df181a0ec8"},"source":["token"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'deprel': 'punct',\n"," 'deps': None,\n"," 'feats': None,\n"," 'form': '।',\n"," 'head': 9,\n"," 'id': 11,\n"," 'lemma': '।',\n"," 'misc': {'ChunkId': 'BLK', 'ChunkType': 'head', 'Translit': '.'},\n"," 'upos': 'PUNCT',\n"," 'xpos': 'SYM'}"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"7Lv-g0akNbpk"},"source":["#### **Training using the above dataset**\n","Look at spacy documentation to learn how to do this"]},{"cell_type":"code","metadata":{"id":"m1_MtUePNftT"},"source":[],"execution_count":null,"outputs":[]}]}