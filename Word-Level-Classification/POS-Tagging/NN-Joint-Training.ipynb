{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/NLP/blob/main/Word-Level-Classification/POS-Tagging/NN-Joint-Training.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"mwrAZqi10ZUA"},"source":["# Preparing Training Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sim_MkZF0dfq"},"outputs":[],"source":["training_data = [\n","    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n","    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n","]\n","\n","    # Tags are: DET - determiner; NN - noun; V - verb\n","    # For example, the word \"The\" is a determiner"]},{"cell_type":"markdown","metadata":{"id":"B78g8BTqJGZ1"},"source":["# Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nvSoxAjWJKe6"},"outputs":[],"source":["# perform stemming , remove stopwords , ....\n","# here it is not needed and hence I will not be performing these operations in here"]},{"cell_type":"markdown","metadata":{"id":"FhMciqz7J8Wt"},"source":["# Converting Words to Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":322,"status":"ok","timestamp":1629903797127,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"lpWDyhCk0rfX","outputId":"ac9063fd-30a9-42b1-f60f-7e8424115356"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"]}],"source":["word_to_ix = {}\n","# For each words-list (sentence) and tags-list in each tuple of training_data\n","for sent, tags in training_data:\n","    for word in sent:\n","        if word not in word_to_ix:  # word has not been assigned an index yet\n","            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n","print(word_to_ix)"]},{"cell_type":"markdown","metadata":{"id":"TuuMzxVM3Mpb"},"source":["dog : 1 represent the index 1 of the OHE of dog will be 1 and rest all indexes will be zeroes, similarly for others"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JUYzkl-fD1n4"},"outputs":[],"source":["tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n","# Assign each tag with a unique index\n","# Usually in real life you will encounter 32 possible POS tags but we will only consider 3 for now & keep them small, so we can see how the weights change as we train.\n"]},{"cell_type":"markdown","metadata":{"id":"5tXlfumZD1n5"},"source":["# Creating Model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4049,"status":"ok","timestamp":1629904814297,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"OxQI8UDk5ab_","outputId":"14be4b18-6175-47ca-a2fe-651f000d224a"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7f7902e0b7d0>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","torch.manual_seed(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TKnhgflsD1n5"},"outputs":[],"source":["class LSTMTagger(nn.Module):\n","\n","    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n","        super(LSTMTagger, self).__init__()\n","        self.hidden_dim = hidden_dim\n","\n","        # input layer\n","        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n","\n","        # hidden layer - The LSTM takes word embeddings as inputs, and outputs hidden states with dimensionality hidden_dim.\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n","\n","        # The linear layer that maps from hidden state space to tag space\n","        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n","\n","    def forward(self, sentence):\n","        embeds = self.word_embeddings(sentence)\n","        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n","        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n","        tag_scores = F.log_softmax(tag_space, dim=1)\n","        return tag_scores"]},{"cell_type":"markdown","metadata":{"id":"7XdjPhn_4IY9"},"source":["# Training the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXdeh_Y74TwH"},"outputs":[],"source":["EMBEDDING_DIM = 6\n","HIDDEN_DIM = 6\n","model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n","loss_function = nn.NLLLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-ql0n4J0c5M"},"outputs":[],"source":["def prepare_sequence(seq, to_ix):\n","    idxs = [to_ix[w] for w in seq]\n","    return torch.tensor(idxs, dtype=torch.long)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1629905266697,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"7xIaxpBu6Ip_","outputId":"e2297955-11f1-482c-b890-3a51c64b47c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-1.0500, -0.9256, -1.3714],\n","        [-1.0188, -0.9739, -1.3419],\n","        [-1.1330, -0.9662, -1.2126],\n","        [-1.1818, -0.9763, -1.1501],\n","        [-1.0766, -0.9916, -1.2439]])\n"]}],"source":["# See what the scores are before training. Note that element i,j of the output is the score for tag j for word i. Here we don't need to train, so the code is wrapped in torch.no_grad()\n","with torch.no_grad():\n","  inputs = prepare_sequence(training_data[0][0], word_to_ix)\n","  tag_scores = model(inputs)\n","  print(tag_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJ2DsvfR6LMf"},"outputs":[],"source":["for epoch in range(300):  # normally you would NOT do 300 epochs, it is toy data\n","  for sentence, tags in training_data:\n","        # Step 1. Remember that Pytorch accumulates gradients.We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is, turn them into Tensors of word indices.\n","        sentence_in = prepare_sequence(sentence, word_to_ix)\n","        targets = prepare_sequence(tags, tag_to_ix)\n","\n","        # Step 3. Run our forward pass.\n","        tag_scores = model(sentence_in)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()\n","        loss = loss_function(tag_scores, targets)\n","        loss.backward()\n","        optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"MLDZtgfg8U83"},"source":["# Predicting using Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":322,"status":"ok","timestamp":1629905276406,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"GGj2yfnc4RWU","outputId":"4420db84-cc19-429f-b690-9a986a31a977"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-0.3892, -1.2426, -3.3890],\n","        [-2.1082, -0.1328, -5.8464],\n","        [-3.0852, -5.9469, -0.0495],\n","        [-0.0499, -3.4414, -4.0961],\n","        [-2.4540, -0.0929, -5.8799]])\n"]}],"source":["# See what the scores are after training\n","with torch.no_grad():\n","  inputs = prepare_sequence(training_data[0][0], word_to_ix)\n","  tag_scores = model(inputs)\n","\n","  # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j for word i. The predicted tag is the maximum scoring tag.Here, we can see the predicted sequence below is 0 1 2 0 1\n","  # since 0 is index of the maximum value of row 1,1 is the index of maximum value of row 2, etc.Which is DET NOUN VERB DET NOUN, the correct sequence!\n","  print(tag_scores)"]}],"metadata":{"colab":{"collapsed_sections":["mwrAZqi10ZUA","B78g8BTqJGZ1","FhMciqz7J8Wt","5tXlfumZD1n5","7XdjPhn_4IY9","MLDZtgfg8U83"],"provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/d59e2f90fcd8d4aff31637dad011146e/sequence_models_tutorial.ipynb","timestamp":1625744823282}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"}},"nbformat":4,"nbformat_minor":0}
