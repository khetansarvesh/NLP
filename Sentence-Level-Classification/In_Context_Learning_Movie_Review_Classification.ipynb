{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/NLP/blob/main/Sentence-Level-Classification/In_Context_Learning_Movie_Review_Classification.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"4b-Y7lYYNUiz"},"source":["## Installing Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25841,"status":"ok","timestamp":1699450271915,"user":{"displayName":"Sarvesh Khetan","userId":"05805229168588767785"},"user_tz":-330},"id":"LUwXn_K_H7Tp","outputId":"c51da85a-ad87-401c-d4e2-8faf5489d273"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n","Collecting datasets\n","  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.13.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: dill, multiprocess, datasets\n","Successfully installed datasets-2.14.6 dill-0.3.7 multiprocess-0.70.15\n"]}],"source":["!pip install transformers # to get the finetuned models from hugging-face hub\n","!pip install datasets"]},{"cell_type":"markdown","metadata":{"id":"UtA_ZJUNNXWJ"},"source":["## Importing Libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4658,"status":"ok","timestamp":1699450579875,"user":{"displayName":"Sarvesh Khetan","userId":"05805229168588767785"},"user_tz":-330},"id":"oRiMVRHzIKpb"},"outputs":[],"source":["from collections import defaultdict, Counter\n","import json\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification"]},{"cell_type":"markdown","metadata":{"id":"XkKdEnjSN7Zh"},"source":["## Input"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":602,"status":"ok","timestamp":1699451759496,"user":{"displayName":"Sarvesh Khetan","userId":"05805229168588767785"},"user_tz":-330},"id":"KnYrugVILZuT"},"outputs":[],"source":["inputs = \"I'm excited to learn about Hugging Face Transformers!\""]},{"cell_type":"markdown","metadata":{"id":"ZKEeiDNcN3Mv"},"source":["## Tokenization : Text2Numeric"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":906,"status":"ok","timestamp":1699453158043,"user":{"displayName":"Sarvesh Khetan","userId":"05805229168588767785"},"user_tz":-330},"id":"bDDESHh1N5ON","outputId":"76e58a9c-55dd-4854-f658-862250a2ed45"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start :             I'm excited to learn about Hugging Face Transformers!\n","Tokenize :          ['I', \"'m\", 'Ġexcited', 'Ġto', 'Ġlearn', 'Ġabout', 'ĠHug', 'ging', 'ĠFace', 'ĠTransformers', '!']\n","Input Ids :         [0][100, 437, 2283, 7, 1532, 59, 30581, 3923, 12346, 34379, 328][2]\n","Tokens :            {'input_ids': tensor([[    0,   100,   437,  2283,     7,  1532,    59, 30581,  3923, 12346,\n","         34379,   328,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","Decoded :           I'm excited to learn about Hugging Face Transformers!\n","\n","\n","\n","Number of tokens:       13\n","Ids:                    [0, 100, 437, 2283, 7, 1532, 59, 30581, 3923, 12346, 34379, 328, 2]\n","Tokens:                 ['<s>', 'I', \"'m\", 'Ġexcited', 'Ġto', 'Ġlearn', 'Ġabout', 'ĠHug', 'ging', 'ĠFace', 'ĠTransformers', '!', '</s>']\n","Special tokens mask:    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"]}],"source":["print(f'''Start :             {inputs}''')\n","\n","# Initialize the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n","\n","input_tokens = tokenizer.tokenize(inputs)\n","print(f'''Tokenize :          {input_tokens}''')\n","\n","input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n","start_token = tokenizer.convert_tokens_to_ids('<s>')\n","end_token = tokenizer.convert_tokens_to_ids('</s>')\n","print(f'''Input Ids :         [{start_token}]{input_ids}[{end_token}]''')\n","\n","tokenized_inputs = tokenizer(inputs, return_tensors=\"pt\")\n","print(f'''Tokens :            {tokenized_inputs}''')\n","\n","decoded_str = tokenizer.decode(input_ids)\n","print(f'''Decoded :           {decoded_str}''')\n","\n","\n","\n","print()\n","print()\n","print()\n","input_t = tokenizer._tokenizer.encode(inputs)\n","print(f\"Number of tokens:       {len(input_t)}\")\n","print(f\"Ids:                    {input_t.ids}\")\n","print(f\"Tokens:                 {input_t.tokens}\")\n","print(f\"Special tokens mask:    {input_t.special_tokens_mask}\")"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":865,"status":"ok","timestamp":1699454229466,"user":{"displayName":"Sarvesh Khetan","userId":"05805229168588767785"},"user_tz":-330},"id":"iYaP779cTq4T","outputId":"6547593a-a273-48da-c273-ade56b33e745"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pad token: <pad> | Pad token id: 1\n","\n","\n","Batch Encoding\n","{'input_ids': tensor([[    0, 40710,  3923, 12346, 34379,    16,   372,   328,     2,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1],\n","        [    0,   133,  2119,  6219, 23602, 13855,    81,     5, 22414,  2335,\n","             4,     2,     1,     1,     1,     1,     1,     1],\n","        [    0, 12948,     5,  2335,   300,    62,     8,  2075,   409,   142,\n","            79,   399,    75,   101, 23602,   293,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","\n","\n","Batch Decoding\n","['<s>Hugging Face Transformers is great!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '<s>The quick brown fox jumps over the lazy dog.</s><pad><pad><pad><pad><pad><pad>', \"<s>Then the dog got up and ran away because she didn't like foxes.</s>\"]\n","\n","Batch Decode: (no special characters)\n","['<s>Hugging Face Transformers is great!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '<s>The quick brown fox jumps over the lazy dog.</s><pad><pad><pad><pad><pad><pad>', \"<s>Then the dog got up and ran away because she didn't like foxes.</s>\"]\n"]}],"source":["# if you are adding padding to your question then it will be represented as 1 in the numeric form\n","print (f\"Pad token: {tokenizer.pad_token} | Pad token id: {tokenizer.pad_token_id}\")\n","print()\n","print()\n","\n","# You can pass multiple strings into the tokenizer and pad them as you need\n","print('Batch Encoding')\n","model_inputs = tokenizer(\n","                          [ \"Hugging Face Transformers is great!\",\n","                            \"The quick brown fox jumps over the lazy dog.\",\n","                            \"Then the dog got up and ran away because she didn't like foxes.\"],\n","                          return_tensors=\"pt\",\n","                          padding=True,\n","                          truncation=True)\n","\n","print(model_inputs)\n","print()\n","print()\n","\n","# Similary you can also do batch decoding\n","print('Batch Decoding')\n","print(tokenizer.batch_decode(model_inputs.input_ids))\n","print()\n","print( \"Batch Decode: (no special characters)\")\n","print(tokenizer.batch_decode(model_inputs.input_ids, skip_spetial_tokens=True))"]},{"cell_type":"markdown","metadata":{"id":"_g5k911iNrqT"},"source":["## Passing Input to Model"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11407,"status":"ok","timestamp":1699454284508,"user":{"displayName":"Sarvesh Khetan","userId":"05805229168588767785"},"user_tz":-330},"id":"JuUzOORcNt0m","outputId":"11fcc9dc-10db-42d7-ba72-45daa109b73c"},"outputs":[{"name":"stdout","output_type":"stream","text":["SequenceClassifierOutput(loss=None, logits=tensor([[-3.7605,  2.9262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"]}],"source":["# Initialize the model which will take above numerics as input\n","model = AutoModelForSequenceClassification.from_pretrained( \"siebert/sentiment-roberta-large-english\")\n","outputs = model(**tokenized_inputs)\n","print(outputs)"]},{"cell_type":"markdown","metadata":{"id":"kH8f9xYaNmEy"},"source":["## Sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5e38w1FNnnD"},"outputs":[],"source":["labels = ['NEGATIVE', 'POSITIVE']\n","prediction = torch.argmax(outputs.logits)\n","print (f'''Sentiment : {labels[prediction]}''')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO9dcpEYmaymAa//ZIAFuX9","collapsed_sections":["4b-Y7lYYNUiz","UtA_ZJUNNXWJ","XkKdEnjSN7Zh","ZKEeiDNcN3Mv","_g5k911iNrqT","kH8f9xYaNmEy"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
