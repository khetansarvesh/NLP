{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnYwfZ3-6qyZ"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/NLP/blob/main/Sentence-Level-Classification/In_Context_Learning_Movie_Review_Classification.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LUwXn_K_H7Tp"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers # to get the finetuned models from hugging-face hub\n",
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oRiMVRHzIKpb"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "import json\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkKdEnjSN7Zh"
      },
      "source": [
        "## Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KnYrugVILZuT"
      },
      "outputs": [],
      "source": [
        "inputs = \"I'm excited to learn about Hugging Face Transformers!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKEeiDNcN3Mv"
      },
      "source": [
        "## Tokenization : Text2Numeric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "from transformers import DistilBertTokenizer, DistilBertTokenizerFast, AutoTokenizer\n",
        "\n",
        "# Method 1 :\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")      # written in Python\n",
        "print(tokenizer)\n",
        "\n",
        "# Method 2 :\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-cased\")  # written in Rust\n",
        "print(tokenizer)\n",
        "\n",
        "# Method 3 : (most used)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\") # Defaults to Fast\n",
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "U7j2Trpe6wOo",
        "outputId": "6b203f1d-696b-46a2-e3ef-37836e160986",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBertTokenizer(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "DistilBertTokenizerFast(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "DistilBertTokenizerFast(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDDESHh1N5ON",
        "outputId": "ed4c92ab-fd39-447a-b0b6-4ad8ffa7f297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start :             I'm excited to learn about Hugging Face Transformers!\n",
            "Tokenize :          ['I', \"'\", 'm', 'excited', 'to', 'learn', 'about', 'Hu', '##gging', 'Face', 'Transformers', '!']\n",
            "Input Ids :         [101][146, 112, 182, 7215, 1106, 3858, 1164, 20164, 10932, 10289, 25267, 106][102]\n",
            "Tokens :            {'input_ids': tensor([[  101,   146,   112,   182,  7215,  1106,  3858,  1164, 20164, 10932,\n",
            "         10289, 25267,   106,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "Decoded :           I'm excited to learn about Hugging Face Transformers!\n"
          ]
        }
      ],
      "source": [
        "print(f'''Start :             {inputs}''')\n",
        "\n",
        "input_tokens = tokenizer.tokenize(inputs)\n",
        "print(f'''Tokenize :          {input_tokens}''')\n",
        "\n",
        "input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "start_token = tokenizer.cls_token_id\n",
        "end_token = tokenizer.sep_token_id\n",
        "print(f'''Input Ids :         [{start_token}]{input_ids}[{end_token}]''')\n",
        "\n",
        "tokenized_inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
        "print(f'''Tokens :            {tokenized_inputs}''')\n",
        "\n",
        "decoded_str = tokenizer.decode(input_ids)\n",
        "print(f'''Decoded :           {decoded_str}''')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_t = tokenizer._tokenizer.encode(inputs)\n",
        "print(f\"Number of tokens:       {len(input_t)}\")\n",
        "print(f\"Ids:                    {input_t.ids}\")\n",
        "print(f\"Tokens:                 {input_t.tokens}\")\n",
        "print(f\"Special tokens mask:    {input_t.special_tokens_mask}\")"
      ],
      "metadata": {
        "id": "_x3UfjmV7DuE",
        "outputId": "29bd08c1-65a3-425a-e71c-c7bd4855a8c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens:       14\n",
            "Ids:                    [101, 146, 112, 182, 7215, 1106, 3858, 1164, 20164, 10932, 10289, 25267, 106, 102]\n",
            "Tokens:                 ['[CLS]', 'I', \"'\", 'm', 'excited', 'to', 'learn', 'about', 'Hu', '##gging', 'Face', 'Transformers', '!', '[SEP]']\n",
            "Special tokens mask:    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if you are adding padding to your question then it will be represented as 0 in the numeric form\n",
        "print (f\"Pad token: {tokenizer.pad_token} | Pad token id: {tokenizer.pad_token_id}\")"
      ],
      "metadata": {
        "id": "wfOsvcWS_HvP",
        "outputId": "71c37685-3961-4f9b-b7ff-c059ac03f211",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pad token: [PAD] | Pad token id: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYaP779cTq4T",
        "outputId": "75434ddf-2b07-4e9c-9541-765003147b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101, 20164, 10932, 10289, 25267,  1110,  1632,   106,   102,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1109,  3613,  3058, 17594, 15457,  1166,  1103, 16688,  3676,\n",
            "           119,   102,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1599,  1103,  3676,  1400,  1146,  1105,  1868,  1283,  1272,\n",
            "          1131,  1238,   112,   189,  1176, 17594,  1279,   119,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "# BATCH ENCODING : pass multiple strings into the tokenizer and pad them as you need\n",
        "model_inputs = tokenizer(\n",
        "                          [ \"Hugging Face Transformers is great!\",\n",
        "                            \"The quick brown fox jumps over the lazy dog.\",\n",
        "                            \"Then the dog got up and ran away because she didn't like foxes.\"],\n",
        "                          return_tensors=\"pt\",\n",
        "                          padding=True,\n",
        "                          truncation=True)\n",
        "\n",
        "print(model_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BATCH DECODING : Similary you can also do batch decoding\n",
        "print(tokenizer.batch_decode(model_inputs.input_ids))\n",
        "print()\n",
        "print( \"Batch Decode: (no special characters)\")\n",
        "print(tokenizer.batch_decode(model_inputs.input_ids, skip_spetial_tokens=True))"
      ],
      "metadata": {
        "id": "tpopjdG7_LQg",
        "outputId": "a117b20e-f33a-423d-fddd-7640c03bb407",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS] Hugging Face Transformers is great! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] The quick brown fox jumps over the lazy dog. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', \"[CLS] Then the dog got up and ran away because she didn't like foxes. [SEP]\"]\n",
            "\n",
            "Batch Decode: (no special characters)\n",
            "['[CLS] Hugging Face Transformers is great! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] The quick brown fox jumps over the lazy dog. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', \"[CLS] Then the dog got up and ran away because she didn't like foxes. [SEP]\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g5k911iNrqT"
      },
      "source": [
        "## Passing Input to Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the model\n",
        "from transformers import AutoModelForSequenceClassification, DistilBertForSequenceClassification\n",
        "\n",
        "# Method 1 :\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n",
        "\n",
        "# Method 2 :\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n"
      ],
      "metadata": {
        "id": "7UVbuCgoAFUN",
        "outputId": "088a547b-4f0b-434b-eff0-f6bf7ac21655",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuUzOORcNt0m",
        "outputId": "e7c71317-975b-421f-a9e5-174e73fee3c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1370, -0.0432]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model which will take above numerics as input\n",
        "outputs = model(**tokenized_inputs)\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH8f9xYaNmEy"
      },
      "source": [
        "## Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "m5e38w1FNnnD",
        "outputId": "382c1400-b748-43e8-b5e0-4ce8b73600ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment : POSITIVE\n"
          ]
        }
      ],
      "source": [
        "labels = ['NEGATIVE', 'POSITIVE']\n",
        "prediction = torch.argmax(outputs.logits)\n",
        "print (f'''Sentiment : {labels[prediction]}''')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4b-Y7lYYNUiz",
        "UtA_ZJUNNXWJ",
        "XkKdEnjSN7Zh",
        "ZKEeiDNcN3Mv",
        "_g5k911iNrqT",
        "kH8f9xYaNmEy"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}