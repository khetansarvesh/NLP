{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentence Tokenization.ipynb","provenance":[],"collapsed_sections":["7_1zAKj5RxSw","YCdOM7uSWfHX"],"authorship_tag":"ABX9TyMFK1+fAv+GEhPThfGhDh6g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"8Smvd__GWxnP"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7_1zAKj5RxSw"},"source":["# **Sentence Tokenization using NLTK Library**"]},{"cell_type":"code","metadata":{"id":"wuhHlrkuRwI5"},"source":["# You need not download nltk cause it is already downloaded in the root kernel of google colab, you just need to import it\n","import nltk\n","# If you need to download you can use following commands  \n","#conda install -c anaconda nltk       or         pip : pip install -U nltk\n","from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"vcncEOjyTdqI","executionInfo":{"status":"ok","timestamp":1625985926335,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"a878cf09-001d-4204-b45c-48a49d9485cf"},"source":["nltk.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'3.2.5'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ig63XFrR5d4","executionInfo":{"status":"ok","timestamp":1625985927554,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"e8994995-f506-40ea-f14a-0f224694cf44"},"source":["text = \"God is Great! I won a lottery.\"\n","print(sent_tokenize(text))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['God is Great!', 'I won a lottery.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LvlijE59S8OO"},"source":["# Capture from https://en.wikipedia.org/wiki/Lexical_analysis\n","\n","article = 'In computer science, lexical analysis, lexing or tokenization is the process of \\\n","converting a sequence of characters (such as in a computer program or web page) into a \\\n","sequence of tokens (strings with an assigned and thus identified meaning). A program that \\\n","performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner \\\n","is also a term for the first stage of a lexer. A lexer is generally combined with a parser, \\\n","which together analyze the syntax of programming languages, web pages, and so forth.'\n","\n","article2 = 'ConcateStringAnd123 ConcateSepcialCharacter_!@# !@#$%^&*()_+ 0123456'\n","\n","article3 = 'It is a great moment from 10 a.m. to 1 p.m. every weekend.'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MvfpP-3kTjSD","executionInfo":{"status":"ok","timestamp":1625985930403,"user_tz":-330,"elapsed":601,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"5cd6a768-54fd-4a86-8143-429e4f32b2e0"},"source":["for doc in [article, article2, article3]:\n","    print('Original Article: %s' % (doc))\n","    print()\n","\n","    doc = sent_tokenize(doc)\n","    for i, token in enumerate(doc):\n","        print('-->Sentence %d: %s' % (i, token))\n","\n","    print('*'*80)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Original Article: In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\n","\n","-->Sentence 0: In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning).\n","-->Sentence 1: A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer.\n","-->Sentence 2: A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\n","********************************************************************************\n","Original Article: ConcateStringAnd123 ConcateSepcialCharacter_!@# !@#$%^&*()_+ 0123456\n","\n","-->Sentence 0: ConcateStringAnd123 ConcateSepcialCharacter_!\n","-->Sentence 1: @# !\n","-->Sentence 2: @#$%^&*()_+ 0123456\n","********************************************************************************\n","Original Article: It is a great moment from 10 a.m. to 1 p.m. every weekend.\n","\n","-->Sentence 0: It is a great moment from 10 a.m. to 1 p.m. every weekend.\n","********************************************************************************\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YCdOM7uSWfHX"},"source":["# **Sentence Tokenisation Using Regex**"]},{"cell_type":"code","metadata":{"id":"1GUNbVyeWkNF"},"source":["import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WF7nlYe8SbMp","executionInfo":{"status":"ok","timestamp":1625986588601,"user_tz":-330,"elapsed":360,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"35bc3974-5d2d-4679-925c-d155139ad470"},"source":["for doc in [article, article2, article3]:\n","    print('Original Article: %s' % (doc))\n","    print()\n","\n","    sentences = re.split('(\\.|!|\\?)', doc)\n","    \n","    for i, s in enumerate(sentences):\n","        print('-->Sentence %d: %s' % (i, s))\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original Article: In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\n","\n","-->Sentence 0: In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning)\n","-->Sentence 1: .\n","-->Sentence 2:  A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer\n","-->Sentence 3: .\n","-->Sentence 4:  A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth\n","-->Sentence 5: .\n","-->Sentence 6: \n","\n","Original Article: ConcateStringAnd123 ConcateSepcialCharacter_!@# !@#$%^&*()_+ 0123456\n","\n","-->Sentence 0: ConcateStringAnd123 ConcateSepcialCharacter_\n","-->Sentence 1: !\n","-->Sentence 2: @# \n","-->Sentence 3: !\n","-->Sentence 4: @#$%^&*()_+ 0123456\n","\n","Original Article: It is a great moment from 10 a.m. to 1 p.m. every weekend.\n","\n","-->Sentence 0: It is a great moment from 10 a\n","-->Sentence 1: .\n","-->Sentence 2: m\n","-->Sentence 3: .\n","-->Sentence 4:  to 1 p\n","-->Sentence 5: .\n","-->Sentence 6: m\n","-->Sentence 7: .\n","-->Sentence 8:  every weekend\n","-->Sentence 9: .\n","-->Sentence 10: \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OUHtX4-pUFXU"},"source":[""],"execution_count":null,"outputs":[]}]}