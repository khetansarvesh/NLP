{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/NLP/blob/main/Preprocessing/English-Sentence-Tokenization.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Smvd__GWxnP"},"outputs":[],"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""]},{"cell_type":"markdown","metadata":{"id":"7_1zAKj5RxSw"},"source":["# **Sentence Tokenization using NLTK Library**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wuhHlrkuRwI5"},"outputs":[],"source":["# You need not download nltk cause it is already downloaded in the root kernel of google colab, you just need to import it\n","import nltk\n","# If you need to download you can use following commands  \n","#conda install -c anaconda nltk       or         pip : pip install -U nltk\n","from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1625985926335,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"vcncEOjyTdqI","outputId":"a878cf09-001d-4204-b45c-48a49d9485cf"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'3.2.5'"]},"execution_count":3,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["nltk.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1625985927554,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"2ig63XFrR5d4","outputId":"e8994995-f506-40ea-f14a-0f224694cf44"},"outputs":[{"name":"stdout","output_type":"stream","text":["['God is Great!', 'I won a lottery.']\n"]}],"source":["text = \"God is Great! I won a lottery.\"\n","print(sent_tokenize(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LvlijE59S8OO"},"outputs":[],"source":["# Capture from https://en.wikipedia.org/wiki/Lexical_analysis\n","\n","article = 'In computer science, lexical analysis, lexing or tokenization is the process of \\\n","converting a sequence of characters (such as in a computer program or web page) into a \\\n","sequence of tokens (strings with an assigned and thus identified meaning). A program that \\\n","performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner \\\n","is also a term for the first stage of a lexer. A lexer is generally combined with a parser, \\\n","which together analyze the syntax of programming languages, web pages, and so forth.'\n","\n","article2 = 'ConcateStringAnd123 ConcateSepcialCharacter_!@# !@#$%^&*()_+ 0123456'\n","\n","article3 = 'It is a great moment from 10 a.m. to 1 p.m. every weekend.'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":601,"status":"ok","timestamp":1625985930403,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"MvfpP-3kTjSD","outputId":"5cd6a768-54fd-4a86-8143-429e4f32b2e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Original Article: In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\n","\n","-->Sentence 0: In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning).\n","-->Sentence 1: A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer.\n","-->Sentence 2: A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\n","********************************************************************************\n","Original Article: ConcateStringAnd123 ConcateSepcialCharacter_!@# !@#$%^&*()_+ 0123456\n","\n","-->Sentence 0: ConcateStringAnd123 ConcateSepcialCharacter_!\n","-->Sentence 1: @# !\n","-->Sentence 2: @#$%^&*()_+ 0123456\n","********************************************************************************\n","Original Article: It is a great moment from 10 a.m. to 1 p.m. every weekend.\n","\n","-->Sentence 0: It is a great moment from 10 a.m. to 1 p.m. every weekend.\n","********************************************************************************\n"]}],"source":["for doc in [article, article2, article3]:\n","    print('Original Article: %s' % (doc))\n","    print()\n","\n","    doc = sent_tokenize(doc)\n","    for i, token in enumerate(doc):\n","        print('-->Sentence %d: %s' % (i, token))\n","\n","    print('*'*80)"]},{"cell_type":"markdown","metadata":{"id":"YCdOM7uSWfHX"},"source":["# **Sentence Tokenisation Using Regex**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1GUNbVyeWkNF"},"outputs":[],"source":["import re"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":360,"status":"ok","timestamp":1625986588601,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"WF7nlYe8SbMp","outputId":"35bc3974-5d2d-4679-925c-d155139ad470"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Article: In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\n","\n","-->Sentence 0: In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning)\n","-->Sentence 1: .\n","-->Sentence 2:  A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer\n","-->Sentence 3: .\n","-->Sentence 4:  A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth\n","-->Sentence 5: .\n","-->Sentence 6: \n","\n","Original Article: ConcateStringAnd123 ConcateSepcialCharacter_!@# !@#$%^&*()_+ 0123456\n","\n","-->Sentence 0: ConcateStringAnd123 ConcateSepcialCharacter_\n","-->Sentence 1: !\n","-->Sentence 2: @# \n","-->Sentence 3: !\n","-->Sentence 4: @#$%^&*()_+ 0123456\n","\n","Original Article: It is a great moment from 10 a.m. to 1 p.m. every weekend.\n","\n","-->Sentence 0: It is a great moment from 10 a\n","-->Sentence 1: .\n","-->Sentence 2: m\n","-->Sentence 3: .\n","-->Sentence 4:  to 1 p\n","-->Sentence 5: .\n","-->Sentence 6: m\n","-->Sentence 7: .\n","-->Sentence 8:  every weekend\n","-->Sentence 9: .\n","-->Sentence 10: \n","\n"]}],"source":["for doc in [article, article2, article3]:\n","    print('Original Article: %s' % (doc))\n","    print()\n","\n","    sentences = re.split('(\\.|!|\\?)', doc)\n","    \n","    for i, s in enumerate(sentences):\n","        print('-->Sentence %d: %s' % (i, s))\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OUHtX4-pUFXU"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMFK1+fAv+GEhPThfGhDh6g","collapsed_sections":["7_1zAKj5RxSw","YCdOM7uSWfHX"],"name":"Sentence Tokenization.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
