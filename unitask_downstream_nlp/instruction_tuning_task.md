In fact all NLU tasks can be framed as a NLG task, for instance
    - Sentimental Analysis output will be ‘this is a positive sentiment’ rather than classification of positive and negative.
    - Similarly QA can be formed as a NLG task
    - fine tuning base LLMs on multiple tasks to get Instruction Fine Tuned LLMs
    - The idea of fine tuning on many tasks was the assumption that by fine tuning on multiple tasks we can generalize and then hence can perform unseen tasks at the inference time
    - Some famous models are :
        - InstructGPT / ChatGPT by openAI:- Based Model used was GPT3 and in total 30k NLP tasks were trained on this model
        - Google FLANG LaMDA and LaMDA2:- Base Model used was LaMDA
        - Google FLANG PALM:- Base Model used was PALM
        - Google FLANG T5:- Base Model used was T5
        - Google FLANG U PALM:- Base Model used was U PALM
        - Google MED PALM Based Model used was FLAN PALM
        - OPT-IML by Facebook
        - Llama 7B | 13B | 32B | 65B by meta
        - Llama2 7B | 13B| 70B by meta