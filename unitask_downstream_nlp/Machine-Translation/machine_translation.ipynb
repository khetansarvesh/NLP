{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPzh3YcPT0YcIYGJy+N528g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchtext.datasets import Multi30k\n","from torchtext.data import Field, BucketIterator\n","import numpy as np\n","import spacy\n","import random\n","from torchtext.data.metrics import bleu_score\n","import sys\n","\n","\n","\"\"\"\n","To install spacy languages do:\n","python -m spacy download en\n","python -m spacy download de\n","\"\"\"\n","spacy_ger = spacy.load(\"de\")\n","spacy_eng = spacy.load(\"en\")"],"metadata":{"id":"S78Qsqb6VAvU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"2ibEgItAXhOA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset"],"metadata":{"id":"2oRNUeD6V2TN"}},{"cell_type":"code","source":["def tokenize_ger(text):\n","    return [tok.text for tok in spacy_ger.tokenizer(text)]\n","\n","\n","def tokenize_eng(text):\n","    return [tok.text for tok in spacy_eng.tokenizer(text)]\n","\n","\n","german = Field(tokenize=tokenize_ger, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n","\n","english = Field(tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n","\n","train_data, valid_data, test_data = Multi30k.splits(exts=(\".de\", \".en\"), fields=(german, english))\n","\n","german.build_vocab(train_data, max_size=10000, min_freq=2)\n","english.build_vocab(train_data, max_size=10000, min_freq=2)"],"metadata":{"id":"mct4P6I8WBhM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, valid_data, test_data),\n","    batch_size=64,\n","    sort_within_batch=True,\n","    sort_key=lambda x: len(x.src),\n","    device=device,\n",")"],"metadata":{"id":"VNrjxfjlXn64"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Modelling"],"metadata":{"id":"6coMG4LkV3gB"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, input_size):\n","        super(Encoder, self).__init__()\n","        self.dropout = nn.Dropout(0.5)\n","        self.embedding = nn.Embedding(input_size, 300)\n","        self.rnn = nn.LSTM(300, 1024, 2, dropout=0.5)\n","\n","    def forward(self, x):\n","        embedding = self.dropout(self.embedding(x))\n","        outputs, (hidden, cell) = self.rnn(embedding)\n","        return hidden, cell"],"metadata":{"id":"5jjPnCOeV4n4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super(Decoder, self).__init__()\n","        self.dropout = nn.Dropout(0.5)\n","        self.embedding = nn.Embedding(input_size, 300)\n","        self.rnn = nn.LSTM(300, 1024, 2, dropout=0.5)\n","        self.fc = nn.Linear(1024, output_size)\n","\n","    def forward(self, x, hidden, cell):\n","        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n","        # is 1 here because we are sending in a single word and not a sentence\n","        x = x.unsqueeze(0)\n","\n","        embedding = self.dropout(self.embedding(x))\n","\n","        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n","\n","        predictions = self.fc(outputs)\n","\n","        # predictions shape: (1, N, length_target_vocabulary) to send it to\n","        # loss function we want it to be (N, length_target_vocabulary) so we're\n","        # just gonna remove the first dim\n","        predictions = predictions.squeeze(0)\n","\n","        return predictions, hidden, cell"],"metadata":{"id":"-SBBfBj8V9Ri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, source, target, teacher_force_ratio=0.5):\n","        batch_size = source.shape[1]\n","        target_len = target.shape[0]\n","        target_vocab_size = len(english.vocab)\n","\n","        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n","\n","        hidden, cell = self.encoder(source)\n","\n","        # Grab the first input to the Decoder which will be <SOS> token\n","        x = target[0]\n","\n","        for t in range(1, target_len):\n","            # Use previous hidden, cell as context from encoder at start\n","            output, hidden, cell = self.decoder(x, hidden, cell)\n","\n","            # Store next output prediction\n","            outputs[t] = output\n","\n","            # Get the best word the Decoder predicted (index in the vocabulary)\n","            best_guess = output.argmax(1)\n","\n","            # With probability of teacher_force_ratio we take the actual next word\n","            # otherwise we take the word that the Decoder predicted it to be.\n","            # Teacher Forcing is used so that the model gets used to seeing\n","            # similar inputs at training and testing time, if teacher forcing is 1\n","            # then inputs at test time might be completely different than what the\n","            # network is used to. This was a long comment.\n","            x = target[t] if random.random() < teacher_force_ratio else best_guess\n","\n","        return outputs"],"metadata":{"id":"RaaTVHslV95I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_net = Encoder(len(german.vocab)).to(device)\n","decoder_net = Decoder(len(english.vocab), len(english.vocab)).to(device)\n","model = Seq2Seq(encoder_net, decoder_net).to(device)"],"metadata":{"id":"X5nd7Ux1XyD1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"MC8Oyjt5WITx"}},{"cell_type":"code","source":["optimizer = optim.Adam(model.parameters(), lr=0.001)\n","pad_idx = english.vocab.stoi[\"<pad>\"]\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n","sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\"\n","# sentence = \"ein pferd geht unter einer brücke neben einem boot.\"\n","step = 0"],"metadata":{"id":"7VYdjyq6X1u_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"855v8ckkU9MS"},"outputs":[],"source":["model.train()\n","for epoch in range(100):\n","    print(f\"[Epoch {epoch} / 100]\")\n","    losses = []\n","\n","    for batch_idx, batch in enumerate(train_iterator):\n","        # Get input and targets and get to cuda\n","        inp_data = batch.src.to(device)\n","        target = batch.trg.to(device)\n","\n","        # Forward prop\n","        output = model(inp_data, target)\n","        output = output[1:].reshape(-1, output.shape[2])\n","        target = target[1:].reshape(-1)\n","\n","        optimizer.zero_grad()\n","        loss = criterion(output, target)\n","        losses.append(loss.item())\n","\n","        # Back prop\n","        loss.backward()\n","\n","        # Clip to avoid exploding gradient issues, makes sure grads are within a healthy range\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","\n","        # Gradient descent step\n","        optimizer.step()\n","\n","        # Plot to tensorboard\n","        step += 1\n","    mean_loss = sum(losses) / len(losses)"]},{"cell_type":"markdown","source":["# Inference"],"metadata":{"id":"vc9TeiJAYGvA"}},{"cell_type":"code","source":["model.eval()"],"metadata":{"id":"c9Xdy_LGbcol"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def translate_sentence(model, sentence, german, english, device, max_length=50):\n","\n","    # Load german tokenizer\n","    spacy_ger = spacy.load(\"de\")\n","\n","    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n","    if type(sentence) == str:\n","        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n","    else:\n","        tokens = [token.lower() for token in sentence]\n","\n","    # print(tokens)\n","\n","    # sys.exit()\n","    # Add <SOS> and <EOS> in beginning and end respectively\n","    tokens.insert(0, german.init_token)\n","    tokens.append(german.eos_token)\n","\n","    # Go through each german token and convert to an index\n","    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n","\n","    # Convert to Tensor\n","    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)`\n","\n","    # Build encoder hidden, cell state\n","    with torch.no_grad():\n","        hidden, cell = model.encoder(sentence_tensor)\n","\n","    outputs = [english.vocab.stoi[\"<sos>\"]]\n","\n","    for _ in range(max_length):\n","        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n","\n","        with torch.no_grad():\n","            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n","            best_guess = output.argmax(1).item()\n","\n","        outputs.append(best_guess)\n","\n","        # Model predicts it's the end of the sentence\n","        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n","            break\n","\n","    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n","\n","    # remove start token\n","    return translated_sentence[1:]"],"metadata":{"id":"47QkEepfYVU2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(translate_sentence(model, sentence, german, english, device, max_length=50))"],"metadata":{"id":"a6L9GuTnYHvW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Evaluation"],"metadata":{"id":"hS_4QnRrbhSi"}},{"cell_type":"code","source":["def bleu(data, model, german, english, device):\n","    targets = []\n","    outputs = []\n","\n","    for example in data:\n","        src = vars(example)[\"src\"]\n","        trg = vars(example)[\"trg\"]\n","\n","        prediction = translate_sentence(model, src, german, english, device)\n","        prediction = prediction[:-1]  # remove <eos> token\n","\n","        targets.append([trg])\n","        outputs.append(prediction)\n","\n","    return bleu_score(outputs, targets)"],"metadata":{"id":"DxbL4Fx3Vn6n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score = bleu(test_data[1:100], model, german, english, device)\n","print(f\"Bleu score {score*100:.2f}\")"],"metadata":{"id":"l3qYt9lIVnd-"},"execution_count":null,"outputs":[]}]}