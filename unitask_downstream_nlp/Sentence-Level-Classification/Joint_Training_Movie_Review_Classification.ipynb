{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yvMXCVFyBMQ"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/NLP/blob/main/unitask_downstream_nlp/Sentence-Level-Classification/Joint_Training_Movie_Review_Classification.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADtGhvAeWZR1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z340tgd-tVMI"
      },
      "source": [
        "## **Reading Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIRVOR7pyDTC"
      },
      "outputs": [],
      "source": [
        "#downloading the dataset\n",
        "!wget https://github.com/khetansarvesh/NLP/blob/main/Sentence-Level-Classification/SST_Dataset.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfwI-xPbZO0m",
        "outputId": "a8283312-9813-4c05-b3f2-da41bb42c21e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bromwell high is a cartoon comedy . it ran at ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>story of a man who has unnatural feelings for ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>homelessness  or houselessness as george carli...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>airport    starts as a brand new luxury    pla...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brilliant over  acting by lesley ann warren . ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>i saw  descent  last night at the stockholm fi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>a christmas together actually came before my t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>some films that you pick up for a pound turn o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>working  class romantic drama from director ma...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>this is one of the dumbest films  i  ve ever s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review  label\n",
              "0      bromwell high is a cartoon comedy . it ran at ...      1\n",
              "1      story of a man who has unnatural feelings for ...      0\n",
              "2      homelessness  or houselessness as george carli...      1\n",
              "3      airport    starts as a brand new luxury    pla...      0\n",
              "4      brilliant over  acting by lesley ann warren . ...      1\n",
              "...                                                  ...    ...\n",
              "24995  i saw  descent  last night at the stockholm fi...      0\n",
              "24996  a christmas together actually came before my t...      1\n",
              "24997  some films that you pick up for a pound turn o...      0\n",
              "24998  working  class romantic drama from director ma...      1\n",
              "24999  this is one of the dumbest films  i  ve ever s...      0\n",
              "\n",
              "[25000 rows x 2 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reading the dataset\n",
        "df = pd.read_csv(\"SST_Dataset.csv\", encoding = \"ISO-8859-1\")\n",
        "df.dropna(inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNpeFi1Gy69G"
      },
      "source": [
        "The dataset we use in this example is [SST2](https://nlp.stanford.edu/sentiment/index.html), which contains sentences from movie reviews, each labeled as either positive (has the value 1) or negative (has the value 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9xvgBmEGXlp",
        "outputId": "7c29cb0c-35e0-4637-825b-68248b73eb8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    0.5\n",
              "0    0.5\n",
              "Name: label, dtype: float64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"label\"].value_counts()/df.shape[0] #hence we can clearly see that it is a perfectly balanced dataset!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyKjPufwtSH0"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhtxRVMRtjuT"
      },
      "source": [
        "### **Cleaning Text Features**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkHwXr00YspR"
      },
      "source": [
        "like removing stop words, punctions, performing stemming ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPLd9knBZXdb",
        "outputId": "202a1fe9-7ffb-494d-ae2e-7cd6cc957e7b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction import stop_words # or use from nltk.corpus import stopwords\n",
        "stopwords = stop_words.ENGLISH_STOP_WORDS\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "import string\n",
        "import re\n",
        "\n",
        "def clean(doc): #doc is a string of text\n",
        "    doc = doc.replace(\"</br>\", \" \") #This text contains a lot of <br/> tags replacing them with \" \".\n",
        "    doc = \"\".join([char for char in doc if char not in string.punctuation and not char.isdigit()])#remove punctuation and numbers\n",
        "    doc = doc.lower() #lowering all the characters\n",
        "    doc = \" \".join([ps.stem(token) for token in doc.split() if token not in stopwords]) # removing stopwords and doing stemming\n",
        "    return doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "IjQ57-bR2g0v",
        "outputId": "d39543cd-8bcb-435a-9b0e-42cd1a41858a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bromwel high cartoon comedi ran time program s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>stori man unnatur feel pig start open scene te...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>homeless houseless georg carlin state issu yea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>airport start brand new luxuri plane load valu...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brilliant act lesley ann warren best dramat ho...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>saw descent night stockholm film festiv huge d...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>christma actual came time ve rais john denver ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>film pick pound turn good rd centuri film rele...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>work class romant drama director martin ritt u...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>dumbest film ve seen rip nearli type thriller ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review  label\n",
              "0      bromwel high cartoon comedi ran time program s...      1\n",
              "1      stori man unnatur feel pig start open scene te...      0\n",
              "2      homeless houseless georg carlin state issu yea...      1\n",
              "3      airport start brand new luxuri plane load valu...      0\n",
              "4      brilliant act lesley ann warren best dramat ho...      1\n",
              "...                                                  ...    ...\n",
              "24995  saw descent night stockholm film festiv huge d...      0\n",
              "24996  christma actual came time ve rais john denver ...      1\n",
              "24997  film pick pound turn good rd centuri film rele...      0\n",
              "24998  work class romant drama director martin ritt u...      1\n",
              "24999  dumbest film ve seen rip nearli type thriller ...      0\n",
              "\n",
              "[25000 rows x 2 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for i in range(len(df.review.values)):\n",
        "  df.review.values[i] = clean(df.review.values[i]) # puting the cleaded text back into the dataframe\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhea_nKuvbBV"
      },
      "source": [
        "### **One Hot Encoding each review**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIfA_ZHTvfzp"
      },
      "outputs": [],
      "source": [
        "all_text = ' '.join([sent for sent in df['review']])\n",
        "#all_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8N0H3QGAyH6u",
        "outputId": "afbf5b4f-a73a-4a1e-b81e-3b085f46c420"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['bromwel',\n",
              " 'high',\n",
              " 'cartoon',\n",
              " 'comedi',\n",
              " 'ran',\n",
              " 'time',\n",
              " 'program',\n",
              " 'school',\n",
              " 'life',\n",
              " 'teacher',\n",
              " 'year',\n",
              " 'teach',\n",
              " 'profess',\n",
              " 'lead',\n",
              " 'believ',\n",
              " 'bromwel',\n",
              " 'high',\n",
              " 's',\n",
              " 'satir',\n",
              " 'closer',\n",
              " 'realiti',\n",
              " 'teacher',\n",
              " 'scrambl',\n",
              " 'surviv',\n",
              " 'financi',\n",
              " 'insight',\n",
              " 'student',\n",
              " 'right',\n",
              " 'pathet',\n",
              " 'teacher',\n",
              " 'pomp',\n",
              " 'petti',\n",
              " 'situat',\n",
              " 'remind',\n",
              " 'school',\n",
              " 'knew',\n",
              " 'student',\n",
              " 'saw',\n",
              " 'episod',\n",
              " 'student',\n",
              " 'repeatedli',\n",
              " 'tri',\n",
              " 'burn',\n",
              " 'school',\n",
              " 'immedi',\n",
              " 'recal',\n",
              " 'high',\n",
              " 'classic',\n",
              " 'line',\n",
              " 'inspector',\n",
              " 'm',\n",
              " 'sack',\n",
              " 'teacher',\n",
              " 'student',\n",
              " 'welcom',\n",
              " 'bromwel',\n",
              " 'high',\n",
              " 'expect',\n",
              " 'adult',\n",
              " 'age',\n",
              " 'think',\n",
              " 'bromwel',\n",
              " 'high',\n",
              " 'far',\n",
              " 'fetch',\n",
              " 'piti',\n",
              " 'isn',\n",
              " 't',\n",
              " 'stori',\n",
              " 'man',\n",
              " 'unnatur',\n",
              " 'feel',\n",
              " 'pig',\n",
              " 'start',\n",
              " 'open',\n",
              " 'scene',\n",
              " 'terrif',\n",
              " 'exampl',\n",
              " 'absurd',\n",
              " 'comedi',\n",
              " 'formal',\n",
              " 'orchestra',\n",
              " 'audienc',\n",
              " 'turn',\n",
              " 'insan',\n",
              " 'violent',\n",
              " 'mob',\n",
              " 'crazi',\n",
              " 'chant',\n",
              " 's',\n",
              " 'singer',\n",
              " 'unfortun',\n",
              " 'stay',\n",
              " 'absurd',\n",
              " 'time',\n",
              " 'gener',\n",
              " 'narr',\n",
              " 'eventu',\n",
              " 'make',\n",
              " 'just',\n",
              " 'put',\n",
              " 'era',\n",
              " 'turn',\n",
              " 'cryptic',\n",
              " 'dialogu',\n",
              " 'make',\n",
              " 'shakespear',\n",
              " 'easi',\n",
              " 'grader',\n",
              " 'technic',\n",
              " 'level',\n",
              " 's',\n",
              " 'better',\n",
              " 'think',\n",
              " 'good',\n",
              " 'cinematographi',\n",
              " 'futur',\n",
              " 'great',\n",
              " 'vilmo',\n",
              " 'zsigmond',\n",
              " 'futur',\n",
              " 'star',\n",
              " 'salli',\n",
              " 'kirkland',\n",
              " 'freder',\n",
              " 'forrest',\n",
              " 'seen',\n",
              " 'briefli',\n",
              " 'homeless',\n",
              " 'houseless',\n",
              " 'georg',\n",
              " 'carlin',\n",
              " 'state',\n",
              " 'issu',\n",
              " 'year',\n",
              " 'plan',\n",
              " 'help',\n",
              " 'street',\n",
              " 'consid',\n",
              " 'human',\n",
              " 'did',\n",
              " 'go',\n",
              " 'school',\n",
              " 'work',\n",
              " 'vote',\n",
              " 'matter',\n",
              " 'peopl',\n",
              " 'think',\n",
              " 'homeless',\n",
              " 'just',\n",
              " 'lost',\n",
              " 'caus',\n",
              " 'worri',\n",
              " 'thing',\n",
              " 'racism',\n",
              " 'war',\n",
              " 'iraq',\n",
              " 'pressur',\n",
              " 'kid',\n",
              " 'succeed',\n",
              " 'technolog',\n",
              " 'elect',\n",
              " 'inflat',\n",
              " 'worri',\n",
              " 'll',\n",
              " 'end',\n",
              " 'street',\n",
              " 'br',\n",
              " 'br',\n",
              " 'given',\n",
              " 'bet',\n",
              " 'live',\n",
              " 'street',\n",
              " 'month',\n",
              " 'luxuri',\n",
              " 'home',\n",
              " 'entertain',\n",
              " 'set',\n",
              " 'bathroom',\n",
              " 'pictur',\n",
              " 'wall',\n",
              " 'comput',\n",
              " 'treasur',\n",
              " 's',\n",
              " 'like',\n",
              " 'homeless',\n",
              " 'goddard',\n",
              " 'bolt',\n",
              " 's',\n",
              " 'lesson',\n",
              " 'br',\n",
              " 'br',\n",
              " 'mel',\n",
              " 'brook',\n",
              " 'direct',\n",
              " 'star',\n",
              " 'bolt',\n",
              " 'play',\n",
              " 'rich',\n",
              " 'man',\n",
              " 'world',\n",
              " 'decid',\n",
              " 'make',\n",
              " 'bet',\n",
              " 'sissi',\n",
              " 'rival',\n",
              " 'jefferi',\n",
              " 'tambor',\n",
              " 'live',\n",
              " 'street',\n",
              " 'thirti',\n",
              " 'day',\n",
              " 'luxuri',\n",
              " 'bolt',\n",
              " 'succe',\n",
              " 'want',\n",
              " 'futur',\n",
              " 'project',\n",
              " 'make',\n",
              " 'build',\n",
              " 'bet',\n",
              " 's',\n",
              " 'bolt',\n",
              " 'thrown',\n",
              " 'street',\n",
              " 'bracelet',\n",
              " 'leg',\n",
              " 'monitor',\n",
              " 't',\n",
              " 'step',\n",
              " 'sidewalk',\n",
              " 's',\n",
              " 'given',\n",
              " 'nicknam',\n",
              " 'pepto',\n",
              " 'vagrant',\n",
              " 's',\n",
              " 'written',\n",
              " 'forehead',\n",
              " 'bolt',\n",
              " 'meet',\n",
              " 'charact',\n",
              " 'includ',\n",
              " 'woman',\n",
              " 'molli',\n",
              " 'lesley',\n",
              " 'ann',\n",
              " 'warren',\n",
              " 'ex',\n",
              " 'dancer',\n",
              " 'got',\n",
              " 'divorc',\n",
              " 'lose',\n",
              " 'home',\n",
              " 'pal',\n",
              " 'sailor',\n",
              " 'howard',\n",
              " 'morri',\n",
              " 'fume',\n",
              " 'teddi',\n",
              " 'wilson',\n",
              " 'use',\n",
              " 'street',\n",
              " 'survivor',\n",
              " 'bolt',\n",
              " 'isn',\n",
              " 't',\n",
              " 's',\n",
              " 'use',\n",
              " 'reach',\n",
              " 'mutual',\n",
              " 'agreement',\n",
              " 'like',\n",
              " 'did',\n",
              " 'rich',\n",
              " 's',\n",
              " 'fight',\n",
              " 'flight',\n",
              " 'kill',\n",
              " 'kill',\n",
              " 'br',\n",
              " 'br',\n",
              " 'love',\n",
              " 'connect',\n",
              " 'molli',\n",
              " 'bolt',\n",
              " 'wasn',\n",
              " 't',\n",
              " 'necessari',\n",
              " 'plot',\n",
              " 'life',\n",
              " 'stink',\n",
              " 'mel',\n",
              " 'brook',\n",
              " 'observ',\n",
              " 'film',\n",
              " 'prior',\n",
              " 'comedi',\n",
              " 'show',\n",
              " 'tender',\n",
              " 'compar',\n",
              " 'slapstick',\n",
              " 'work',\n",
              " 'blaze',\n",
              " 'saddl',\n",
              " 'young',\n",
              " 'frankenstein',\n",
              " 'spacebal',\n",
              " 'matter',\n",
              " 's',\n",
              " 'like',\n",
              " 'have',\n",
              " 'valuabl',\n",
              " 'lose',\n",
              " 'day',\n",
              " 'hand',\n",
              " 'make',\n",
              " 'stupid',\n",
              " 'bet',\n",
              " 'like',\n",
              " 'rich',\n",
              " 'peopl',\n",
              " 'don',\n",
              " 't',\n",
              " 'know',\n",
              " 'money',\n",
              " 'mayb',\n",
              " 'homeless',\n",
              " 'instead',\n",
              " 'use',\n",
              " 'like',\n",
              " 'monopoli',\n",
              " 'money',\n",
              " 'br',\n",
              " 'br',\n",
              " 'mayb',\n",
              " 'film',\n",
              " 'inspir',\n",
              " 'help',\n",
              " 'airport',\n",
              " 'start',\n",
              " 'brand',\n",
              " 'new',\n",
              " 'luxuri',\n",
              " 'plane',\n",
              " 'load',\n",
              " 'valuabl',\n",
              " 'paint',\n",
              " 'belong',\n",
              " 'rich',\n",
              " 'businessman',\n",
              " 'philip',\n",
              " 'steven',\n",
              " 'jame',\n",
              " 'stewart',\n",
              " 'fli',\n",
              " 'bunch',\n",
              " 'vip',\n",
              " 's',\n",
              " 'estat',\n",
              " 'prepar',\n",
              " 'open',\n",
              " 'public',\n",
              " 'museum',\n",
              " 'board',\n",
              " 'steven',\n",
              " 'daughter',\n",
              " 'juli',\n",
              " 'kathleen',\n",
              " 'quinlan',\n",
              " 'son',\n",
              " 'luxuri',\n",
              " 'jetlin',\n",
              " 'take',\n",
              " 'plan',\n",
              " 'mid',\n",
              " 'air',\n",
              " 'plane',\n",
              " 'hi',\n",
              " 'jack',\n",
              " 'pilot',\n",
              " 'chamber',\n",
              " 'robert',\n",
              " 'foxworth',\n",
              " 'accomplic',\n",
              " 's',\n",
              " 'banker',\n",
              " 'mont',\n",
              " 'markham',\n",
              " 'wilson',\n",
              " 'michael',\n",
              " 'pataki',\n",
              " 'knock',\n",
              " 'passeng',\n",
              " 'crew',\n",
              " 'sleep',\n",
              " 'ga',\n",
              " 'plan',\n",
              " 'steal',\n",
              " 'valuabl',\n",
              " 'cargo',\n",
              " 'land',\n",
              " 'disus',\n",
              " 'plane',\n",
              " 'strip',\n",
              " 'isol',\n",
              " 'island',\n",
              " 'make',\n",
              " 'descent',\n",
              " 'chamber',\n",
              " 'hit',\n",
              " 'oil',\n",
              " 'rig',\n",
              " 'ocean',\n",
              " 'lose',\n",
              " 'control',\n",
              " 'plane',\n",
              " 'send',\n",
              " 'crash',\n",
              " 'sea',\n",
              " 'sink',\n",
              " 'right',\n",
              " 'bang',\n",
              " 'middl',\n",
              " 'bermuda',\n",
              " 'triangl',\n",
              " 'air',\n",
              " 'short',\n",
              " 'suppli',\n",
              " 'water',\n",
              " 'leak',\n",
              " 'have',\n",
              " 'flown',\n",
              " 'mile',\n",
              " 'cours',\n",
              " 'problem',\n",
              " 'mount',\n",
              " 'survivor',\n",
              " 's',\n",
              " 'await',\n",
              " 'help',\n",
              " 'time',\n",
              " 'fast',\n",
              " 'run',\n",
              " 'br',\n",
              " 'br',\n",
              " 'known',\n",
              " 'slightli',\n",
              " 'differ',\n",
              " 'tile',\n",
              " 'airport',\n",
              " 'second',\n",
              " 'sequel',\n",
              " 'smash',\n",
              " 'hit',\n",
              " 'disast',\n",
              " 'thriller',\n",
              " 'airport',\n",
              " 'direct',\n",
              " 'jerri',\n",
              " 'jameson',\n",
              " 'like',\n",
              " 's',\n",
              " 'predecessor',\n",
              " 't',\n",
              " 'say',\n",
              " 'airport',\n",
              " 'sort',\n",
              " 'forgotten',\n",
              " 'classic',\n",
              " 'entertain',\n",
              " 'necessarili',\n",
              " 'right',\n",
              " 'reason',\n",
              " 'airport',\n",
              " 'film',\n",
              " 'seen',\n",
              " 'far',\n",
              " 'actual',\n",
              " 'like',\n",
              " 'best',\n",
              " 'just',\n",
              " 'favourit',\n",
              " 'plot',\n",
              " 'nice',\n",
              " 'mid',\n",
              " 'air',\n",
              " 'hi',\n",
              " 'jack',\n",
              " 'crash',\n",
              " 'didn',\n",
              " 't',\n",
              " 'oil',\n",
              " 'rig',\n",
              " 'sink',\n",
              " 'mayb',\n",
              " 'maker',\n",
              " 'tri',\n",
              " 'cross',\n",
              " 'origin',\n",
              " 'airport',\n",
              " 'popular',\n",
              " 'disast',\n",
              " 'flick',\n",
              " 'period',\n",
              " 'poseidon',\n",
              " 'adventur',\n",
              " 'submerg',\n",
              " 'stay',\n",
              " 'end',\n",
              " 'stark',\n",
              " 'dilemma',\n",
              " 'face',\n",
              " 'trap',\n",
              " 'insid',\n",
              " 'suffoc',\n",
              " 'air',\n",
              " 'run',\n",
              " 'drown',\n",
              " 'flood',\n",
              " 'door',\n",
              " 'open',\n",
              " 's',\n",
              " 'decent',\n",
              " 'idea',\n",
              " 'great',\n",
              " 'littl',\n",
              " 'disast',\n",
              " 'flick',\n",
              " 'bad',\n",
              " 'unsympathet',\n",
              " 'charact',\n",
              " 's',\n",
              " 'dull',\n",
              " 'dialogu',\n",
              " 'letharg',\n",
              " 'set',\n",
              " 'piec',\n",
              " 'real',\n",
              " 'lack',\n",
              " 'danger',\n",
              " 'suspens',\n",
              " 'tension',\n",
              " 'mean',\n",
              " 'miss',\n",
              " 'opportun',\n",
              " 'sluggish',\n",
              " 'plot',\n",
              " 'keep',\n",
              " 'entertain',\n",
              " 'odd',\n",
              " 'minut',\n",
              " 'happen',\n",
              " 'plane',\n",
              " 'sink',\n",
              " 's',\n",
              " 'urgenc',\n",
              " 'thought',\n",
              " 'navi',\n",
              " 'involv',\n",
              " 'thing',\n",
              " 'don',\n",
              " 't',\n",
              " 'pick',\n",
              " 'shot',\n",
              " 'huge',\n",
              " 'ship',\n",
              " 'helicopt',\n",
              " 'fli',\n",
              " 's',\n",
              " 'just',\n",
              " 'lack',\n",
              " 'georg',\n",
              " 'kennedi',\n",
              " 'jinx',\n",
              " 'airlin',\n",
              " 'worker',\n",
              " 'joe',\n",
              " 'patroni',\n",
              " 'get',\n",
              " 'coupl',\n",
              " 'scene',\n",
              " 'bare',\n",
              " 'say',\n",
              " 'prefer',\n",
              " 'just',\n",
              " 'look',\n",
              " 'worri',\n",
              " 'background',\n",
              " 'br',\n",
              " 'br',\n",
              " 'home',\n",
              " 'video',\n",
              " 'theatric',\n",
              " 'version',\n",
              " 'airport',\n",
              " 'run',\n",
              " 'minut',\n",
              " 'tv',\n",
              " 'version',\n",
              " 'add',\n",
              " 'extra',\n",
              " 'hour',\n",
              " 'footag',\n",
              " 'includ',\n",
              " 'new',\n",
              " 'open',\n",
              " 'credit',\n",
              " 'sequenc',\n",
              " 'scene',\n",
              " 'georg',\n",
              " 'kennedi',\n",
              " 'patroni',\n",
              " 'flashback',\n",
              " 'flesh',\n",
              " 'charact',\n",
              " 's',\n",
              " 'longer',\n",
              " 'rescu',\n",
              " 'scene',\n",
              " 'discoveri',\n",
              " 'coupl',\n",
              " 'dead',\n",
              " 'bodi',\n",
              " 'includ',\n",
              " 'navig',\n",
              " 'like',\n",
              " 'extra',\n",
              " 'footag',\n",
              " 'sure',\n",
              " 'sit',\n",
              " 'near',\n",
              " 'hour',\n",
              " 'cut',\n",
              " 'airport',\n",
              " 'expect',\n",
              " 'film',\n",
              " 'date',\n",
              " 'badli',\n",
              " 'horribl',\n",
              " 'fashion',\n",
              " 'interior',\n",
              " 'design',\n",
              " 'choic',\n",
              " 'say',\n",
              " 'toy',\n",
              " 'plane',\n",
              " 'model',\n",
              " 'effect',\n",
              " 'aren',\n",
              " 't',\n",
              " 'great',\n",
              " 'airport',\n",
              " 'sequel',\n",
              " 'take',\n",
              " 'pride',\n",
              " 'place',\n",
              " 'razzi',\n",
              " 'award',\n",
              " 's',\n",
              " 'hall',\n",
              " 'shame',\n",
              " 'think',\n",
              " 'lot',\n",
              " 'wors',\n",
              " 'film',\n",
              " 'reckon',\n",
              " 's',\n",
              " 'littl',\n",
              " 'harsh',\n",
              " 'action',\n",
              " 'scene',\n",
              " 'littl',\n",
              " 'dull',\n",
              " 'unfortun',\n",
              " 'pace',\n",
              " 'slow',\n",
              " 'excit',\n",
              " 'tension',\n",
              " 'gener',\n",
              " 'shame',\n",
              " 'reckon',\n",
              " 'pretti',\n",
              " 'good',\n",
              " 'film',\n",
              " 'properli',\n",
              " 'br',\n",
              " 'br',\n",
              " 'product',\n",
              " 'valu',\n",
              " 'alright',\n",
              " 'spectacular',\n",
              " 'act',\n",
              " 'isn',\n",
              " 't',\n",
              " 'great',\n",
              " 'time',\n",
              " 'oscar',\n",
              " 'winner',\n",
              " 'jack',\n",
              " 'lemmon',\n",
              " 'said',\n",
              " 'mistak',\n",
              " 'star',\n",
              " 'time',\n",
              " 'oscar',\n",
              " 'winner',\n",
              " 'jame',\n",
              " 'stewart',\n",
              " 'look',\n",
              " 'old',\n",
              " 'frail',\n",
              " 'time',\n",
              " 'oscar',\n",
              " 'winner',\n",
              " 'lee',\n",
              " 'grant',\n",
              " 'look',\n",
              " 'drunk',\n",
              " 'sir',\n",
              " 'christoph',\n",
              " 'lee',\n",
              " 'given',\n",
              " 'littl',\n",
              " 'plenti',\n",
              " 'familiar',\n",
              " 'face',\n",
              " 'look',\n",
              " 'br',\n",
              " 'br',\n",
              " 'airport',\n",
              " 'disast',\n",
              " 'orient',\n",
              " 'airport',\n",
              " 'film',\n",
              " 'far',\n",
              " 'like',\n",
              " 'idea',\n",
              " 'bit',\n",
              " 'silli',\n",
              " 'product',\n",
              " 'bland',\n",
              " 'direct',\n",
              " 'doesn',\n",
              " 't',\n",
              " 'help',\n",
              " 'film',\n",
              " 'sunken',\n",
              " 'plane',\n",
              " 'just',\n",
              " 'shouldn',\n",
              " 't',\n",
              " 'bore',\n",
              " 'letharg',\n",
              " 'follow',\n",
              " 'concord',\n",
              " 'airport',\n",
              " 'brilliant',\n",
              " 'act',\n",
              " 'lesley',\n",
              " 'ann',\n",
              " 'warren',\n",
              " 'best',\n",
              " 'dramat',\n",
              " 'hobo',\n",
              " 'ladi',\n",
              " 'seen',\n",
              " 'love',\n",
              " 'scene',\n",
              " 'cloth',\n",
              " 'warehous',\n",
              " 'second',\n",
              " 'corn',\n",
              " 'face',\n",
              " 'classic',\n",
              " 'good',\n",
              " 'blaze',\n",
              " 'saddl',\n",
              " 'lawyer',\n",
              " 'superb',\n",
              " 'accus',\n",
              " 'turncoat',\n",
              " 'sell',\n",
              " 'boss',\n",
              " 'dishonest',\n",
              " 'lawyer',\n",
              " 'pepto',\n",
              " 'bolt',\n",
              " 'shrug',\n",
              " 'indiffer',\n",
              " 'm',\n",
              " 'lawyer',\n",
              " 'say',\n",
              " 'funni',\n",
              " 'word',\n",
              " 'jeffrey',\n",
              " 'tambor',\n",
              " 'favorit',\n",
              " 'later',\n",
              " 'larri',\n",
              " 'sander',\n",
              " 'fantast',\n",
              " 'mad',\n",
              " 'millionair',\n",
              " 'want',\n",
              " 'crush',\n",
              " 'ghetto',\n",
              " 'charact',\n",
              " 'malevol',\n",
              " 'usual',\n",
              " 'hospit',\n",
              " 'scene',\n",
              " 'scene',\n",
              " 'homeless',\n",
              " 'invad',\n",
              " 'demolit',\n",
              " 'site',\n",
              " 'time',\n",
              " 'classic',\n",
              " 'look',\n",
              " 'leg',\n",
              " 'scene',\n",
              " 'big',\n",
              " 'digger',\n",
              " 'fight',\n",
              " 'bleed',\n",
              " 'movi',\n",
              " 'get',\n",
              " 'better',\n",
              " 'time',\n",
              " 'quit',\n",
              " 'film',\n",
              " 'lack',\n",
              " 'couldn',\n",
              " 't',\n",
              " 'finger',\n",
              " 'charisma',\n",
              " 'lead',\n",
              " 'actress',\n",
              " 'inevit',\n",
              " 'translat',\n",
              " 'lack',\n",
              " 'chemistri',\n",
              " 'share',\n",
              " 'screen',\n",
              " 'lead',\n",
              " 'man',\n",
              " 'romant',\n",
              " 'scene',\n",
              " 'came',\n",
              " 'mere',\n",
              " 'actor',\n",
              " 'play',\n",
              " 'director',\n",
              " 'miscalcul',\n",
              " 'need',\n",
              " 'actor',\n",
              " 'just',\n",
              " 'don',\n",
              " 't',\n",
              " 'know',\n",
              " 'br',\n",
              " 'br',\n",
              " 'screenplay',\n",
              " 'just',\n",
              " 'exactli',\n",
              " 'chef',\n",
              " 'love',\n",
              " 'enamor',\n",
              " 'culinari',\n",
              " 'skill',\n",
              " 'restaur',\n",
              " 'ultim',\n",
              " 'youth',\n",
              " 'exploit',\n",
              " 'anybodi',\n",
              " 'convinc',\n",
              " 'love',\n",
              " 'princess',\n",
              " 'br',\n",
              " 'br',\n",
              " 'disappoint',\n",
              " 'movi',\n",
              " 'don',\n",
              " 't',\n",
              " 'forget',\n",
              " 'nomin',\n",
              " 'oscar',\n",
              " 'judg',\n",
              " 'easili',\n",
              " 'underr',\n",
              " 'film',\n",
              " 'inn',\n",
              " 'brook',\n",
              " 'cannon',\n",
              " 'sure',\n",
              " 'flaw',\n",
              " 'doe',\n",
              " 'realist',\n",
              " 'view',\n",
              " 'homeless',\n",
              " 'unlik',\n",
              " 'say',\n",
              " 'citizen',\n",
              " 'kane',\n",
              " 'gave',\n",
              " 'realist',\n",
              " 'view',\n",
              " 'loung',\n",
              " 'singer',\n",
              " 'titan',\n",
              " 'gave',\n",
              " 'realist',\n",
              " 'view',\n",
              " 'italian',\n",
              " 'idiot',\n",
              " 'joke',\n",
              " 'fall',\n",
              " 'flat',\n",
              " 'film',\n",
              " 'lovabl',\n",
              " 'way',\n",
              " 'comedi',\n",
              " 'pull',\n",
              " 'stori',\n",
              " 'tradit',\n",
              " 'revil',\n",
              " 'member',\n",
              " 'societi',\n",
              " 'truli',\n",
              " 'impress',\n",
              " 'fisher',\n",
              " 'king',\n",
              " 'crap',\n",
              " 'complaint',\n",
              " 'brook',\n",
              " 'cast',\n",
              " 'lead',\n",
              " 'love',\n",
              " 'mel',\n",
              " 'director',\n",
              " 'writer',\n",
              " 'lead',\n",
              " 'sorri',\n",
              " 'know',\n",
              " 'suppos',\n",
              " 'art',\n",
              " 'film',\n",
              " 'wow',\n",
              " 'hand',\n",
              " 'gun',\n",
              " 'screen',\n",
              " 'peopl',\n",
              " 'blow',\n",
              " 'brain',\n",
              " 'watch',\n",
              " 'scene',\n",
              " 'design',\n",
              " 'photograph',\n",
              " 'direct',\n",
              " 'excel',\n",
              " 'stori',\n",
              " 'pain',\n",
              " 'watch',\n",
              " 'absenc',\n",
              " 'sound',\n",
              " 'track',\n",
              " 'brutal',\n",
              " 'loooonnnnng',\n",
              " 'shot',\n",
              " 'long',\n",
              " 'long',\n",
              " 'watch',\n",
              " 'peopl',\n",
              " 'just',\n",
              " 'sit',\n",
              " 'talk',\n",
              " 'especi',\n",
              " 'dialogu',\n",
              " 'peopl',\n",
              " 'complain',\n",
              " 'realli',\n",
              " 'hard',\n",
              " 'time',\n",
              " 'just',\n",
              " 'get',\n",
              " 'film',\n",
              " 'perform',\n",
              " 'excel',\n",
              " 'dark',\n",
              " 'sombr',\n",
              " 'uninspir',\n",
              " 'stuff',\n",
              " 'thing',\n",
              " 'like',\n",
              " 'maureen',\n",
              " 'stapleton',\n",
              " 'red',\n",
              " 'dress',\n",
              " 'danc',\n",
              " ...]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words = all_text.split()\n",
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUQX0-TNyZ5Y"
      },
      "source": [
        "One Hot Encoding (OHE) words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjVL_sr3yg22",
        "outputId": "866eea9c-0193-449d-f809-ce8137bfa2f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'br': 1,\n",
              " 's': 2,\n",
              " 'movi': 3,\n",
              " 'film': 4,\n",
              " 't': 5,\n",
              " 'like': 6,\n",
              " 'just': 7,\n",
              " 'time': 8,\n",
              " 'good': 9,\n",
              " 'make': 10,\n",
              " 'charact': 11,\n",
              " 'watch': 12,\n",
              " 'stori': 13,\n",
              " 'realli': 14,\n",
              " 'scene': 15,\n",
              " 'look': 16,\n",
              " 'end': 17,\n",
              " 'peopl': 18,\n",
              " 'bad': 19,\n",
              " 'great': 20,\n",
              " 'love': 21,\n",
              " 'think': 22,\n",
              " 'way': 23,\n",
              " 'don': 24,\n",
              " 'act': 25,\n",
              " 'play': 26,\n",
              " 'thing': 27,\n",
              " 'know': 28,\n",
              " 'say': 29,\n",
              " 'work': 30,\n",
              " 'plot': 31,\n",
              " 'year': 32,\n",
              " 'actor': 33,\n",
              " 'come': 34,\n",
              " 'seen': 35,\n",
              " 'want': 36,\n",
              " 'life': 37,\n",
              " 'littl': 38,\n",
              " 'best': 39,\n",
              " 'tri': 40,\n",
              " 'did': 41,\n",
              " 'man': 42,\n",
              " 'doe': 43,\n",
              " 'better': 44,\n",
              " 'perform': 45,\n",
              " 'feel': 46,\n",
              " 've': 47,\n",
              " 'use': 48,\n",
              " 'director': 49,\n",
              " 'actual': 50,\n",
              " 'm': 51,\n",
              " 'get': 52,\n",
              " 'lot': 53,\n",
              " 'real': 54,\n",
              " 'old': 55,\n",
              " 'cast': 56,\n",
              " 'doesn': 57,\n",
              " 'live': 58,\n",
              " 'star': 59,\n",
              " 'enjoy': 60,\n",
              " 'guy': 61,\n",
              " 'didn': 62,\n",
              " 'new': 63,\n",
              " 'role': 64,\n",
              " 'funni': 65,\n",
              " 'music': 66,\n",
              " 'point': 67,\n",
              " 'start': 68,\n",
              " 'go': 69,\n",
              " 'set': 70,\n",
              " 'girl': 71,\n",
              " 'origin': 72,\n",
              " 'day': 73,\n",
              " 'world': 74,\n",
              " 'believ': 75,\n",
              " 'turn': 76,\n",
              " 'interest': 77,\n",
              " 'quit': 78,\n",
              " 'direct': 79,\n",
              " 'thought': 80,\n",
              " 'fact': 81,\n",
              " 'minut': 82,\n",
              " 'horror': 83,\n",
              " 'kill': 84,\n",
              " 'action': 85,\n",
              " 'comedi': 86,\n",
              " 'pretti': 87,\n",
              " 'young': 88,\n",
              " 'wonder': 89,\n",
              " 'happen': 90,\n",
              " 'got': 91,\n",
              " 'show': 92,\n",
              " 'effect': 93,\n",
              " 'right': 94,\n",
              " 'long': 95,\n",
              " 'big': 96,\n",
              " 'line': 97,\n",
              " 'famili': 98,\n",
              " 'seri': 99,\n",
              " 'need': 100,\n",
              " 'fan': 101,\n",
              " 'bit': 102,\n",
              " 'script': 103,\n",
              " 'beauti': 104,\n",
              " 'person': 105,\n",
              " 'friend': 106,\n",
              " 'tell': 107,\n",
              " 'reason': 108,\n",
              " 'saw': 109,\n",
              " 'isn': 110,\n",
              " 'kid': 111,\n",
              " 'final': 112,\n",
              " 'take': 113,\n",
              " 'sure': 114,\n",
              " 'd': 115,\n",
              " 'place': 116,\n",
              " 'complet': 117,\n",
              " 'expect': 118,\n",
              " 'kind': 119,\n",
              " 'differ': 120,\n",
              " 'shot': 121,\n",
              " 'far': 122,\n",
              " 'mean': 123,\n",
              " 'book': 124,\n",
              " 'laugh': 125,\n",
              " 'begin': 126,\n",
              " 'll': 127,\n",
              " 'probabl': 128,\n",
              " 'woman': 129,\n",
              " 'entertain': 130,\n",
              " 'help': 131,\n",
              " 'let': 132,\n",
              " 'screen': 133,\n",
              " 'tv': 134,\n",
              " 'moment': 135,\n",
              " 'away': 136,\n",
              " 'read': 137,\n",
              " 'worst': 138,\n",
              " 'run': 139,\n",
              " 'fun': 140,\n",
              " 'lead': 141,\n",
              " 'hard': 142,\n",
              " 'audienc': 143,\n",
              " 'idea': 144,\n",
              " 'see': 145,\n",
              " 'episod': 146,\n",
              " 'american': 147,\n",
              " 'appear': 148,\n",
              " 'bore': 149,\n",
              " 'have': 150,\n",
              " 'especi': 151,\n",
              " 'hope': 152,\n",
              " 'cours': 153,\n",
              " 'anim': 154,\n",
              " 'job': 155,\n",
              " 'goe': 156,\n",
              " 'give': 157,\n",
              " 'sens': 158,\n",
              " 'dvd': 159,\n",
              " 'version': 160,\n",
              " 'war': 161,\n",
              " 'money': 162,\n",
              " 'mind': 163,\n",
              " 'mayb': 164,\n",
              " 'problem': 165,\n",
              " 'true': 166,\n",
              " 'hous': 167,\n",
              " 'second': 168,\n",
              " 'nice': 169,\n",
              " 'wasn': 170,\n",
              " 'rate': 171,\n",
              " 'follow': 172,\n",
              " 'night': 173,\n",
              " 'face': 174,\n",
              " 'recommend': 175,\n",
              " 'product': 176,\n",
              " 'main': 177,\n",
              " 'worth': 178,\n",
              " 'leav': 179,\n",
              " 'human': 180,\n",
              " 'special': 181,\n",
              " 'excel': 182,\n",
              " 'sound': 183,\n",
              " 'wast': 184,\n",
              " 'hand': 185,\n",
              " 'john': 186,\n",
              " 'father': 187,\n",
              " 'eye': 188,\n",
              " 'later': 189,\n",
              " 'said': 190,\n",
              " 'view': 191,\n",
              " 'instead': 192,\n",
              " 'review': 193,\n",
              " 'boy': 194,\n",
              " 'high': 195,\n",
              " 'hour': 196,\n",
              " 'classic': 197,\n",
              " 'talk': 198,\n",
              " 'miss': 199,\n",
              " 'wife': 200,\n",
              " 'understand': 201,\n",
              " 'left': 202,\n",
              " 'care': 203,\n",
              " 'black': 204,\n",
              " 'death': 205,\n",
              " 'open': 206,\n",
              " 'murder': 207,\n",
              " 'write': 208,\n",
              " 'half': 209,\n",
              " 'head': 210,\n",
              " 'rememb': 211,\n",
              " 'chang': 212,\n",
              " 'viewer': 213,\n",
              " 'fight': 214,\n",
              " 'surpris': 215,\n",
              " 'gener': 216,\n",
              " 'short': 217,\n",
              " 'includ': 218,\n",
              " 'die': 219,\n",
              " 'fall': 220,\n",
              " 'entir': 221,\n",
              " 'piec': 222,\n",
              " 'involv': 223,\n",
              " 'pictur': 224,\n",
              " 'simpli': 225,\n",
              " 'home': 226,\n",
              " 'power': 227,\n",
              " 'total': 228,\n",
              " 'usual': 229,\n",
              " 'budget': 230,\n",
              " 'attempt': 231,\n",
              " 'suppos': 232,\n",
              " 'releas': 233,\n",
              " 'hollywood': 234,\n",
              " 'terribl': 235,\n",
              " 'song': 236,\n",
              " 'men': 237,\n",
              " 'possibl': 238,\n",
              " 'portray': 239,\n",
              " 'featur': 240,\n",
              " 'disappoint': 241,\n",
              " 'poor': 242,\n",
              " 'coupl': 243,\n",
              " 'stupid': 244,\n",
              " 'camera': 245,\n",
              " 'dead': 246,\n",
              " 'wrong': 247,\n",
              " 'produc': 248,\n",
              " 'low': 249,\n",
              " 'call': 250,\n",
              " 'video': 251,\n",
              " 'aw': 252,\n",
              " 'definit': 253,\n",
              " 'rest': 254,\n",
              " 'given': 255,\n",
              " 'absolut': 256,\n",
              " 'women': 257,\n",
              " 'lack': 258,\n",
              " 'word': 259,\n",
              " 'writer': 260,\n",
              " 'titl': 261,\n",
              " 'talent': 262,\n",
              " 'decid': 263,\n",
              " 'perfect': 264,\n",
              " 'style': 265,\n",
              " 'close': 266,\n",
              " 'truli': 267,\n",
              " 'school': 268,\n",
              " 'emot': 269,\n",
              " 'save': 270,\n",
              " 'sex': 271,\n",
              " 'age': 272,\n",
              " 'bring': 273,\n",
              " 'mr': 274,\n",
              " 'move': 275,\n",
              " 'case': 276,\n",
              " 'killer': 277,\n",
              " 'heart': 278,\n",
              " 'comment': 279,\n",
              " 'sort': 280,\n",
              " 'creat': 281,\n",
              " 'won': 282,\n",
              " 'came': 283,\n",
              " 'brother': 284,\n",
              " 'joke': 285,\n",
              " 'game': 286,\n",
              " 'dialogu': 287,\n",
              " 'art': 288,\n",
              " 'small': 289,\n",
              " 'base': 290,\n",
              " 'flick': 291,\n",
              " 'do': 292,\n",
              " 'written': 293,\n",
              " 'meet': 294,\n",
              " 'sequenc': 295,\n",
              " 'earli': 296,\n",
              " 'mother': 297,\n",
              " 'develop': 298,\n",
              " 'humor': 299,\n",
              " 'actress': 300,\n",
              " 'consid': 301,\n",
              " 'dark': 302,\n",
              " 'guess': 303,\n",
              " 'unfortun': 304,\n",
              " 'amaz': 305,\n",
              " 'light': 306,\n",
              " 'cinema': 307,\n",
              " 'lost': 308,\n",
              " 'exampl': 309,\n",
              " 'drama': 310,\n",
              " 'white': 311,\n",
              " 'ye': 312,\n",
              " 'experi': 313,\n",
              " 'imagin': 314,\n",
              " 'mention': 315,\n",
              " 'stop': 316,\n",
              " 'natur': 317,\n",
              " 'forc': 318,\n",
              " 'manag': 319,\n",
              " 'felt': 320,\n",
              " 'present': 321,\n",
              " 'cut': 322,\n",
              " 'children': 323,\n",
              " 'fail': 324,\n",
              " 'qualiti': 325,\n",
              " 'son': 326,\n",
              " 'support': 327,\n",
              " 'car': 328,\n",
              " 'ask': 329,\n",
              " 'hit': 330,\n",
              " 'couldn': 331,\n",
              " 'voic': 332,\n",
              " 'extrem': 333,\n",
              " 'impress': 334,\n",
              " 'wors': 335,\n",
              " 'evil': 336,\n",
              " 'certainli': 337,\n",
              " 'stand': 338,\n",
              " 'went': 339,\n",
              " 'basic': 340,\n",
              " 'oh': 341,\n",
              " 'overal': 342,\n",
              " 'favorit': 343,\n",
              " 'horribl': 344,\n",
              " 'mysteri': 345,\n",
              " 'number': 346,\n",
              " 'type': 347,\n",
              " 'danc': 348,\n",
              " 'wait': 349,\n",
              " 'hero': 350,\n",
              " 'learn': 351,\n",
              " 'matter': 352,\n",
              " 'michael': 353,\n",
              " 'genr': 354,\n",
              " 'fine': 355,\n",
              " 'despit': 356,\n",
              " 'walk': 357,\n",
              " 'success': 358,\n",
              " 'histori': 359,\n",
              " 'question': 360,\n",
              " 'zombi': 361,\n",
              " 'child': 362,\n",
              " 'town': 363,\n",
              " 'realiz': 364,\n",
              " 'relationship': 365,\n",
              " 'past': 366,\n",
              " 'find': 367,\n",
              " 'daughter': 368,\n",
              " 'late': 369,\n",
              " 'wish': 370,\n",
              " 'credit': 371,\n",
              " 'event': 372,\n",
              " 'hate': 373,\n",
              " 'theme': 374,\n",
              " 'citi': 375,\n",
              " 'touch': 376,\n",
              " 'today': 377,\n",
              " 'god': 378,\n",
              " 'twist': 379,\n",
              " 'sit': 380,\n",
              " 'annoy': 381,\n",
              " 'deal': 382,\n",
              " 'stay': 383,\n",
              " 'abl': 384,\n",
              " 'rent': 385,\n",
              " 'edit': 386,\n",
              " 'blood': 387,\n",
              " 'deserv': 388,\n",
              " 'comic': 389,\n",
              " 'appar': 390,\n",
              " 'soon': 391,\n",
              " 'name': 392,\n",
              " 'gave': 393,\n",
              " 'part': 394,\n",
              " 'b': 395,\n",
              " 'level': 396,\n",
              " 'slow': 397,\n",
              " 'chanc': 398,\n",
              " 'score': 399,\n",
              " 'bodi': 400,\n",
              " 'brilliant': 401,\n",
              " 'incred': 402,\n",
              " 'figur': 403,\n",
              " 'situat': 404,\n",
              " 'self': 405,\n",
              " 'major': 406,\n",
              " 'stuff': 407,\n",
              " 'decent': 408,\n",
              " 'element': 409,\n",
              " 'return': 410,\n",
              " 'dream': 411,\n",
              " 'obvious': 412,\n",
              " 'continu': 413,\n",
              " 'pace': 414,\n",
              " 'order': 415,\n",
              " 'ridicul': 416,\n",
              " 'happi': 417,\n",
              " 'highli': 418,\n",
              " 'add': 419,\n",
              " 'thank': 420,\n",
              " 'group': 421,\n",
              " 'ladi': 422,\n",
              " 'pain': 423,\n",
              " 'speak': 424,\n",
              " 'novel': 425,\n",
              " 'career': 426,\n",
              " 'strang': 427,\n",
              " 'shoot': 428,\n",
              " 'heard': 429,\n",
              " 'sad': 430,\n",
              " 'polic': 431,\n",
              " 'husband': 432,\n",
              " 'import': 433,\n",
              " 'break': 434,\n",
              " 'took': 435,\n",
              " 'strong': 436,\n",
              " 'predict': 437,\n",
              " 'robert': 438,\n",
              " 'violenc': 439,\n",
              " 'recent': 440,\n",
              " 'hilari': 441,\n",
              " 'countri': 442,\n",
              " 'known': 443,\n",
              " 'pick': 444,\n",
              " 'particularli': 445,\n",
              " 'season': 446,\n",
              " 'documentari': 447,\n",
              " 'critic': 448,\n",
              " 'jame': 449,\n",
              " 'compar': 450,\n",
              " 'obviou': 451,\n",
              " 'told': 452,\n",
              " 'state': 453,\n",
              " 'gore': 454,\n",
              " 'theater': 455,\n",
              " 'visual': 456,\n",
              " 'offer': 457,\n",
              " 'exist': 458,\n",
              " 'rock': 459,\n",
              " 'opinion': 460,\n",
              " 'crap': 461,\n",
              " 'hold': 462,\n",
              " 'wouldn': 463,\n",
              " 'hear': 464,\n",
              " 'result': 465,\n",
              " 'realiti': 466,\n",
              " 'room': 467,\n",
              " 'thriller': 468,\n",
              " 'effort': 469,\n",
              " 'caus': 470,\n",
              " 'sequel': 471,\n",
              " 'explain': 472,\n",
              " 'serious': 473,\n",
              " 'local': 474,\n",
              " 'ago': 475,\n",
              " 'king': 476,\n",
              " 'hell': 477,\n",
              " 'note': 478,\n",
              " 'allow': 479,\n",
              " 'sister': 480,\n",
              " 'femal': 481,\n",
              " 'david': 482,\n",
              " 'deliv': 483,\n",
              " 'simpl': 484,\n",
              " 'ok': 485,\n",
              " 'check': 486,\n",
              " 'convinc': 487,\n",
              " 'class': 488,\n",
              " 'suspens': 489,\n",
              " 'oscar': 490,\n",
              " 'win': 491,\n",
              " 'buy': 492,\n",
              " 'huge': 493,\n",
              " 'valu': 494,\n",
              " 'sexual': 495,\n",
              " 'scari': 496,\n",
              " 'cool': 497,\n",
              " 'similar': 498,\n",
              " 'excit': 499,\n",
              " 'exactli': 500,\n",
              " 'apart': 501,\n",
              " 'provid': 502,\n",
              " 'avoid': 503,\n",
              " 'shown': 504,\n",
              " 'taken': 505,\n",
              " 'spoiler': 506,\n",
              " 'english': 507,\n",
              " 'cinematographi': 508,\n",
              " 'polit': 509,\n",
              " 'shock': 510,\n",
              " 'offic': 511,\n",
              " 'middl': 512,\n",
              " 'street': 513,\n",
              " 'pass': 514,\n",
              " 'silli': 515,\n",
              " 'messag': 516,\n",
              " 'somewhat': 517,\n",
              " 'charm': 518,\n",
              " 'modern': 519,\n",
              " 'filmmak': 520,\n",
              " 'confus': 521,\n",
              " 'form': 522,\n",
              " 'tale': 523,\n",
              " 'one': 524,\n",
              " 'singl': 525,\n",
              " 'jack': 526,\n",
              " 'sing': 527,\n",
              " 'william': 528,\n",
              " 'attent': 529,\n",
              " 'subject': 530,\n",
              " 'prove': 531,\n",
              " 'carri': 532,\n",
              " 'richard': 533,\n",
              " 'team': 534,\n",
              " 'georg': 535,\n",
              " 'stage': 536,\n",
              " 'cop': 537,\n",
              " 'unlik': 538,\n",
              " 'monster': 539,\n",
              " 'televis': 540,\n",
              " 'earth': 541,\n",
              " 'villain': 542,\n",
              " 'cover': 543,\n",
              " 'pay': 544,\n",
              " 'marri': 545,\n",
              " 'build': 546,\n",
              " 'pull': 547,\n",
              " 'parent': 548,\n",
              " 'keep': 549,\n",
              " 'respect': 550,\n",
              " 'dialog': 551,\n",
              " 'remind': 552,\n",
              " 'futur': 553,\n",
              " 'weak': 554,\n",
              " 'typic': 555,\n",
              " 'cheap': 556,\n",
              " 'intellig': 557,\n",
              " 'atmospher': 558,\n",
              " 'british': 559,\n",
              " 'fast': 560,\n",
              " 'non': 561,\n",
              " 'clearli': 562,\n",
              " 'knew': 563,\n",
              " 'paul': 564,\n",
              " 'dog': 565,\n",
              " 'artist': 566,\n",
              " 'crime': 567,\n",
              " 'easili': 568,\n",
              " 'adult': 569,\n",
              " 'escap': 570,\n",
              " 'doubt': 571,\n",
              " 'aren': 572,\n",
              " 'date': 573,\n",
              " 'romant': 574,\n",
              " 'member': 575,\n",
              " 'drive': 576,\n",
              " 'gun': 577,\n",
              " 'straight': 578,\n",
              " 'clich': 579,\n",
              " 'attack': 580,\n",
              " 'fit': 581,\n",
              " 'o': 582,\n",
              " 'imag': 583,\n",
              " 'posit': 584,\n",
              " 'fantast': 585,\n",
              " 'peter': 586,\n",
              " 'aspect': 587,\n",
              " 'captur': 588,\n",
              " 'appreci': 589,\n",
              " 'plan': 590,\n",
              " 'discov': 591,\n",
              " 'remain': 592,\n",
              " 'period': 593,\n",
              " 'near': 594,\n",
              " 'realist': 595,\n",
              " 'air': 596,\n",
              " 'mark': 597,\n",
              " 'adapt': 598,\n",
              " 'dull': 599,\n",
              " 'red': 600,\n",
              " 'spend': 601,\n",
              " 'lose': 602,\n",
              " 'materi': 603,\n",
              " 'color': 604,\n",
              " 'forget': 605,\n",
              " 'chase': 606,\n",
              " 'mari': 607,\n",
              " 'storylin': 608,\n",
              " 'bunch': 609,\n",
              " 'clear': 610,\n",
              " 'lee': 611,\n",
              " 'match': 612,\n",
              " 'victim': 613,\n",
              " 'nearli': 614,\n",
              " 'box': 615,\n",
              " 'york': 616,\n",
              " 'inspir': 617,\n",
              " 'finish': 618,\n",
              " 'haven': 619,\n",
              " 'mess': 620,\n",
              " 'standard': 621,\n",
              " 'easi': 622,\n",
              " 'truth': 623,\n",
              " 'busi': 624,\n",
              " 'suffer': 625,\n",
              " 'dramat': 626,\n",
              " 'space': 627,\n",
              " 'list': 628,\n",
              " 'western': 629,\n",
              " 'battl': 630,\n",
              " 'notic': 631,\n",
              " 'ad': 632,\n",
              " 'french': 633,\n",
              " 'tom': 634,\n",
              " 'larg': 635,\n",
              " 'eventu': 636,\n",
              " 'accept': 637,\n",
              " 'train': 638,\n",
              " 'agre': 639,\n",
              " 'soundtrack': 640,\n",
              " 'spirit': 641,\n",
              " 'teenag': 642,\n",
              " 'adventur': 643,\n",
              " 'famou': 644,\n",
              " 'sorri': 645,\n",
              " 'drug': 646,\n",
              " 'soldier': 647,\n",
              " 'suggest': 648,\n",
              " 'normal': 649,\n",
              " 'ultim': 650,\n",
              " 'troubl': 651,\n",
              " 'babi': 652,\n",
              " 'certain': 653,\n",
              " 'contain': 654,\n",
              " 'cultur': 655,\n",
              " 'romanc': 656,\n",
              " 'rare': 657,\n",
              " 'lame': 658,\n",
              " 'mix': 659,\n",
              " 'disney': 660,\n",
              " 'gone': 661,\n",
              " 'cartoon': 662,\n",
              " 'student': 663,\n",
              " 'put': 664,\n",
              " 'reveal': 665,\n",
              " 'fear': 666,\n",
              " 'kept': 667,\n",
              " 'attract': 668,\n",
              " 'suck': 669,\n",
              " 'appeal': 670,\n",
              " 'premis': 671,\n",
              " 'design': 672,\n",
              " 'secret': 673,\n",
              " 'greatest': 674,\n",
              " 'th': 675,\n",
              " 'shame': 676,\n",
              " 'copi': 677,\n",
              " 'throw': 678,\n",
              " 'scare': 679,\n",
              " 'wit': 680,\n",
              " 'admit': 681,\n",
              " 'america': 682,\n",
              " 'relat': 683,\n",
              " 'brought': 684,\n",
              " 'particular': 685,\n",
              " 'screenplay': 686,\n",
              " 'pure': 687,\n",
              " 'averag': 688,\n",
              " 'master': 689,\n",
              " 'harri': 690,\n",
              " 'male': 691,\n",
              " 'treat': 692,\n",
              " 'issu': 693,\n",
              " 'except': 694,\n",
              " 'fantasi': 695,\n",
              " 'background': 696,\n",
              " 'warn': 697,\n",
              " 'forward': 698,\n",
              " 'free': 699,\n",
              " 'project': 700,\n",
              " 'japanes': 701,\n",
              " 'poorli': 702,\n",
              " 'memor': 703,\n",
              " 'okay': 704,\n",
              " 'award': 705,\n",
              " 'locat': 706,\n",
              " 'amus': 707,\n",
              " 'potenti': 708,\n",
              " 'societi': 709,\n",
              " 'magic': 710,\n",
              " 'struggl': 711,\n",
              " 'weird': 712,\n",
              " 'accent': 713,\n",
              " 'express': 714,\n",
              " 'doctor': 715,\n",
              " 'water': 716,\n",
              " 'odd': 717,\n",
              " 'imdb': 718,\n",
              " 'alien': 719,\n",
              " 'choic': 720,\n",
              " 'hot': 721,\n",
              " 'dr': 722,\n",
              " 'crazi': 723,\n",
              " 'control': 724,\n",
              " 'fiction': 725,\n",
              " 'studio': 726,\n",
              " 'masterpiec': 727,\n",
              " 'fli': 728,\n",
              " 'difficult': 729,\n",
              " 'joe': 730,\n",
              " 'scream': 731,\n",
              " 'refer': 732,\n",
              " 'lover': 733,\n",
              " 'uniqu': 734,\n",
              " 'costum': 735,\n",
              " 'fill': 736,\n",
              " 'remak': 737,\n",
              " 'girlfriend': 738,\n",
              " 'vampir': 739,\n",
              " 'prison': 740,\n",
              " 'execut': 741,\n",
              " 'wear': 742,\n",
              " 'jump': 743,\n",
              " 'wood': 744,\n",
              " 'unless': 745,\n",
              " 'cheesi': 746,\n",
              " 'creepi': 747,\n",
              " 'superb': 748,\n",
              " 'parti': 749,\n",
              " 'roll': 750,\n",
              " 'ghost': 751,\n",
              " 'public': 752,\n",
              " 'mad': 753,\n",
              " 'badli': 754,\n",
              " 'earlier': 755,\n",
              " 'depict': 756,\n",
              " 'week': 757,\n",
              " 'moral': 758,\n",
              " 'jane': 759,\n",
              " 'flaw': 760,\n",
              " 'fi': 761,\n",
              " 'grow': 762,\n",
              " 'dumb': 763,\n",
              " 'sci': 764,\n",
              " 'maker': 765,\n",
              " 'deep': 766,\n",
              " 'cat': 767,\n",
              " 'connect': 768,\n",
              " 'footag': 769,\n",
              " 'bother': 770,\n",
              " 'older': 771,\n",
              " 'plenti': 772,\n",
              " 'outsid': 773,\n",
              " 'stick': 774,\n",
              " 'gay': 775,\n",
              " 'catch': 776,\n",
              " 'plu': 777,\n",
              " 'popular': 778,\n",
              " 'equal': 779,\n",
              " 'disturb': 780,\n",
              " 'social': 781,\n",
              " 'quickli': 782,\n",
              " 'perfectli': 783,\n",
              " 'dress': 784,\n",
              " 'era': 785,\n",
              " 'mistak': 786,\n",
              " 'previou': 787,\n",
              " 'ride': 788,\n",
              " 'lie': 789,\n",
              " 'combin': 790,\n",
              " 'concept': 791,\n",
              " 'band': 792,\n",
              " 'surviv': 793,\n",
              " 'rich': 794,\n",
              " 'answer': 795,\n",
              " 'christma': 796,\n",
              " 'insid': 797,\n",
              " 'sweet': 798,\n",
              " 'e': 799,\n",
              " 'eat': 800,\n",
              " 'concern': 801,\n",
              " 'bare': 802,\n",
              " 'ben': 803,\n",
              " 'listen': 804,\n",
              " 'beat': 805,\n",
              " 'term': 806,\n",
              " 'meant': 807,\n",
              " 'serv': 808,\n",
              " 'german': 809,\n",
              " 'stereotyp': 810,\n",
              " 'la': 811,\n",
              " 'hardli': 812,\n",
              " 'innoc': 813,\n",
              " 'desper': 814,\n",
              " 'law': 815,\n",
              " 'promis': 816,\n",
              " 'memori': 817,\n",
              " 'cute': 818,\n",
              " 'intent': 819,\n",
              " 'variou': 820,\n",
              " 'steal': 821,\n",
              " 'inform': 822,\n",
              " 'brain': 823,\n",
              " 'post': 824,\n",
              " 'island': 825,\n",
              " 'tone': 826,\n",
              " 'nuditi': 827,\n",
              " 'track': 828,\n",
              " 'store': 829,\n",
              " 'compani': 830,\n",
              " 'claim': 831,\n",
              " 'flat': 832,\n",
              " 'hair': 833,\n",
              " 'land': 834,\n",
              " 'univers': 835,\n",
              " 'scott': 836,\n",
              " 'kick': 837,\n",
              " 'fairli': 838,\n",
              " 'danger': 839,\n",
              " 'player': 840,\n",
              " 'step': 841,\n",
              " 'plain': 842,\n",
              " 'crew': 843,\n",
              " 'toni': 844,\n",
              " 'share': 845,\n",
              " 'centuri': 846,\n",
              " 'tast': 847,\n",
              " 'achiev': 848,\n",
              " 'engag': 849,\n",
              " 'travel': 850,\n",
              " 'rip': 851,\n",
              " 'cold': 852,\n",
              " 'record': 853,\n",
              " 'c': 854,\n",
              " 'suit': 855,\n",
              " 'tension': 856,\n",
              " 'wrote': 857,\n",
              " 'manner': 858,\n",
              " 'spot': 859,\n",
              " 'sadli': 860,\n",
              " 'intens': 861,\n",
              " 'familiar': 862,\n",
              " 'fascin': 863,\n",
              " 'histor': 864,\n",
              " 'depth': 865,\n",
              " 'remark': 866,\n",
              " 'destroy': 867,\n",
              " 'sleep': 868,\n",
              " 'languag': 869,\n",
              " 'burn': 870,\n",
              " 'purpos': 871,\n",
              " 'ignor': 872,\n",
              " 'ruin': 873,\n",
              " 'unbeliev': 874,\n",
              " 'delight': 875,\n",
              " 'italian': 876,\n",
              " 'abil': 877,\n",
              " 'collect': 878,\n",
              " 'soul': 879,\n",
              " 'clever': 880,\n",
              " 'detect': 881,\n",
              " 'violent': 882,\n",
              " 'reach': 883,\n",
              " 'rape': 884,\n",
              " 'door': 885,\n",
              " 'liter': 886,\n",
              " 'scienc': 887,\n",
              " 'trash': 888,\n",
              " 'caught': 889,\n",
              " 'commun': 890,\n",
              " 'reveng': 891,\n",
              " 'creatur': 892,\n",
              " 'trip': 893,\n",
              " 'approach': 894,\n",
              " 'intrigu': 895,\n",
              " 'channel': 896,\n",
              " 'paint': 897,\n",
              " 'fashion': 898,\n",
              " 'skill': 899,\n",
              " 'introduc': 900,\n",
              " 'complex': 901,\n",
              " 'million': 902,\n",
              " 'camp': 903,\n",
              " 'christian': 904,\n",
              " 'comput': 905,\n",
              " 'immedi': 906,\n",
              " 'ann': 907,\n",
              " 'mental': 908,\n",
              " 'limit': 909,\n",
              " 'slightli': 910,\n",
              " 'extra': 911,\n",
              " 'mere': 912,\n",
              " 'detail': 913,\n",
              " 'teen': 914,\n",
              " 'slasher': 915,\n",
              " 'suddenli': 916,\n",
              " 'conclus': 917,\n",
              " 'crimin': 918,\n",
              " 'spent': 919,\n",
              " 'nation': 920,\n",
              " 'physic': 921,\n",
              " 'hole': 922,\n",
              " 'respons': 923,\n",
              " 'imposs': 924,\n",
              " 'fake': 925,\n",
              " 'planet': 926,\n",
              " 'receiv': 927,\n",
              " 'sick': 928,\n",
              " 'blue': 929,\n",
              " 'bizarr': 930,\n",
              " 'embarrass': 931,\n",
              " 'indian': 932,\n",
              " 'pop': 933,\n",
              " 'ring': 934,\n",
              " 'drop': 935,\n",
              " 'drag': 936,\n",
              " 'haunt': 937,\n",
              " 'pointless': 938,\n",
              " 'suspect': 939,\n",
              " 'edg': 940,\n",
              " 'handl': 941,\n",
              " 'search': 942,\n",
              " 'biggest': 943,\n",
              " 'common': 944,\n",
              " 'hurt': 945,\n",
              " 'arriv': 946,\n",
              " 'faith': 947,\n",
              " 'technic': 948,\n",
              " 'angel': 949,\n",
              " 'genuin': 950,\n",
              " 'dad': 951,\n",
              " 'awesom': 952,\n",
              " 'solid': 953,\n",
              " 'van': 954,\n",
              " 'colleg': 955,\n",
              " 'focu': 956,\n",
              " 'count': 957,\n",
              " 'heavi': 958,\n",
              " 'tear': 959,\n",
              " 'laughabl': 960,\n",
              " 'wall': 961,\n",
              " 'rais': 962,\n",
              " 'visit': 963,\n",
              " 'younger': 964,\n",
              " 'sign': 965,\n",
              " 'fair': 966,\n",
              " 'excus': 967,\n",
              " 'key': 968,\n",
              " 'cult': 969,\n",
              " 'motion': 970,\n",
              " 'stun': 971,\n",
              " 'super': 972,\n",
              " 'tough': 973,\n",
              " 'desir': 974,\n",
              " 'addit': 975,\n",
              " 'cloth': 976,\n",
              " 'exploit': 977,\n",
              " 'tortur': 978,\n",
              " 'smith': 979,\n",
              " 'davi': 980,\n",
              " 'race': 981,\n",
              " 'cross': 982,\n",
              " 'author': 983,\n",
              " 'minor': 984,\n",
              " 'compel': 985,\n",
              " 'weren': 986,\n",
              " 'focus': 987,\n",
              " 'consist': 988,\n",
              " 'pathet': 989,\n",
              " 'chemistri': 990,\n",
              " 'commit': 991,\n",
              " 'jim': 992,\n",
              " 'tradit': 993,\n",
              " 'park': 994,\n",
              " 'frank': 995,\n",
              " 'obsess': 996,\n",
              " 'asid': 997,\n",
              " 'grade': 998,\n",
              " 'brutal': 999,\n",
              " 'u': 1000,\n",
              " ...}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "## Build a dictionary that maps words to integers\n",
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab,1)}\n",
        "vocab_to_int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqtxWRspfVVq",
        "outputId": "7c43c591-739d-4ab0-f578-7debe5fde40b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50352"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab_to_int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHqvkP2nGx-g"
      },
      "source": [
        "cast : 56 means cast is a OHE vector where index 56 is 1 and rest all indexes have 0s & it is a 1*50352 dimension vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "o7pDoQkOy8dw",
        "outputId": "e5105d88-6221-4801-eee0-e24c725b474b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[14889, 195, 662, 86, 1829, 8, 1244, 268, 37, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[13, 42, 5222, 46, 2649, 68, 206, 15, 1061, 30...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[2621, 30969, 535, 12308, 453, 693, 32, 590, 1...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[3553, 68, 2608, 63, 4681, 1242, 1456, 3515, 8...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[401, 25, 11812, 907, 3074, 39, 626, 11354, 42...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>[109, 3693, 173, 14695, 4, 1035, 493, 241, 241...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>[796, 50, 283, 8, 47, 962, 186, 6719, 236, 181...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>[4, 444, 2496, 76, 9, 3061, 846, 4, 233, 1510,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>[30, 488, 574, 310, 49, 1251, 26109, 874, 34, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>[5398, 4, 47, 35, 851, 614, 347, 468, 319, 10,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review  label\n",
              "0      [14889, 195, 662, 86, 1829, 8, 1244, 268, 37, ...      1\n",
              "1      [13, 42, 5222, 46, 2649, 68, 206, 15, 1061, 30...      0\n",
              "2      [2621, 30969, 535, 12308, 453, 693, 32, 590, 1...      1\n",
              "3      [3553, 68, 2608, 63, 4681, 1242, 1456, 3515, 8...      0\n",
              "4      [401, 25, 11812, 907, 3074, 39, 626, 11354, 42...      1\n",
              "...                                                  ...    ...\n",
              "24995  [109, 3693, 173, 14695, 4, 1035, 493, 241, 241...      0\n",
              "24996  [796, 50, 283, 8, 47, 962, 186, 6719, 236, 181...      1\n",
              "24997  [4, 444, 2496, 76, 9, 3061, 846, 4, 233, 1510,...      0\n",
              "24998  [30, 488, 574, 310, 49, 1251, 26109, 874, 34, ...      1\n",
              "24999  [5398, 4, 47, 35, 851, 614, 347, 468, 319, 10,...      0\n",
              "\n",
              "[25000 rows x 2 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## use the above dictionary to tokenize each review in reviews_split - store the tokenized reviews in reviews_ints\n",
        "reviews_ints = []\n",
        "for review in df['review']:\n",
        "  reviews_ints.append([vocab_to_int[word] for word in review.split()])\n",
        "\n",
        "df['review'] = reviews_ints\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B10ZKhh03nXu"
      },
      "source": [
        "### **Padding / Truncating each review**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGknqjZR339b"
      },
      "source": [
        "As an additional pre-processing step, we want to make sure that our reviews are in good shape for standard processing. That is, our network will expect a standard input text size, and so, we'll want to shape our reviews into a specific length. We'll approach this task in two main steps:\n",
        "\n",
        "1. Getting rid of extremely long or short reviews; the outliers\n",
        "2. Padding/truncating the remaining data so that we have reviews of the same length.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tad_SruO7FsZ"
      },
      "source": [
        "#### 1. Getting rid of extremely long or short reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np0s1u4P3row",
        "outputId": "437e7959-5e18-4e75-cd98-4220467d12da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zero-length reviews: 0\n",
            "Maximum review length: 1358\n"
          ]
        }
      ],
      "source": [
        "# Before we pad our review text, we should check for reviews of extremely short or long lengths; outliers that may mess with our training\n",
        "# outlier review stats\n",
        "review_lens = Counter([len(x) for x in reviews_ints])\n",
        "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
        "print(\"Maximum review length: {}\".format(max(review_lens)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDZS76RRpmwB"
      },
      "source": [
        "Okay, a couple issues here. We seem to have no review with zero length which is a good thing. And, the maximum review length is way too many time steps for our RNN. We'll have to remove any super short reviews and truncate super long reviews. This removes outliers and should allow our model to train more efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vfr4gP0M5Alw"
      },
      "outputs": [],
      "source": [
        "# If we had any review which is of zero length then we would have to remove them first and their corresponding label\n",
        "# but not required with this dataset because we dont have any zero length review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvoiDgvH5zj1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS6nx_687Kyz"
      },
      "source": [
        "#### 2. Padding / Truncating the remaining data so that we have reviews of same length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwlWvd0B8dJx"
      },
      "source": [
        "\n",
        "To deal with both short and very long reviews, we'll pad very short reviews and truncate very long reviews to a specific length.\n",
        "______________\n",
        "___________________________________________________________________________\n",
        "\n",
        "For reviews shorter than some `seq_length`, we'll **left pad** with 0s.\n",
        "As a small example, if the `seq_length=10` and an input review is:\n",
        "```\n",
        "['best', 'movie', 'ever']` = `[117, 18, 128]` as integers\n",
        "```\n",
        "The resultant, padded sequence should be:\n",
        "\n",
        "```\n",
        "[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]\n",
        "```\n",
        "(you can also pad at the right instead of left - your wish)\n",
        "___________________________________________________________________________\n",
        "___\n",
        "For reviews longer than `seq_length`, we can truncate them to the **first** `seq_length` words.\n",
        "___\n",
        "___\n",
        " A good `seq_length`, in this case, is 200.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Your final `features` array should be a 2D array, with as many rows as there are reviews, and as many columns as the specified `seq_length`.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHxXC1B_8crU"
      },
      "outputs": [],
      "source": [
        "# Define a function that returns an array `features` that contains the padded data, of a standard size, that we'll pass to the network.\n",
        "# The data should come from `review_ints`, since we want to feed integers to the network.\n",
        "# Each row should be `seq_length` elements long.\n",
        "\n",
        "def pad_features(reviews_ints, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's\n",
        "        or truncated to the input seq_length.\n",
        "    '''\n",
        "    ## getting the correct rows x cols shape\n",
        "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "\n",
        "    ## for each review, I grab that review\n",
        "    for i, row in enumerate(reviews_ints):\n",
        "      features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5lkvSF_7Osx",
        "outputId": "2a11d87f-038c-4d84-be4f-f0f04cc69fda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[    0,     0,     0, ...,  1403,   110,     5],\n",
              "       [    0,     0,     0, ...,  5286,    35,  2770],\n",
              "       [ 2621, 30969,   535, ...,   162,   164,  2621],\n",
              "       ...,\n",
              "       [    0,     0,     0, ..., 12394, 12054,   503],\n",
              "       [    0,     0,     0, ...,    14,  1151,    23],\n",
              "       [    0,     0,     0, ...,   184,     8,   423]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq_length = 200\n",
        "\n",
        "features = pad_features(reviews_ints, seq_length=seq_length)\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2C7BvKSCgZU",
        "outputId": "3add9433-2b81-467f-cd8f-9f113cad41e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [ 2621 30969   535 12308   453   693    32   590   131   513]\n",
            " [ 3553    68  2608    63  4681  1242  1456  3515   897  1574]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [  448  3056  1263   468   290   166   372  3076  1757   528]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [  173   804  1757   528   844  5101  2609 12819  7926  6281]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]]\n"
          ]
        }
      ],
      "source": [
        "print(features[:30,:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "va2E3KVkA-No"
      },
      "outputs": [],
      "source": [
        "temp = []\n",
        "for i in features:\n",
        "  temp2 = []\n",
        "  for j in i:\n",
        "    temp2.append(j)\n",
        "  temp.append(temp2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGuTEVrcB1w_",
        "outputId": "df03459a-8829-41f7-eee0-fd5bfeb1937c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[2621, 30969, 535, 12308, 453, 693, 32, 590, 1...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[3553, 68, 2608, 63, 4681, 1242, 1456, 3515, 8...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review  label\n",
              "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1\n",
              "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0\n",
              "2      [2621, 30969, 535, 12308, 453, 693, 32, 590, 1...      1\n",
              "3      [3553, 68, 2608, 63, 4681, 1242, 1456, 3515, 8...      0\n",
              "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1\n",
              "...                                                  ...    ...\n",
              "24995  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0\n",
              "24996  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1\n",
              "24997  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0\n",
              "24998  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1\n",
              "24999  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0\n",
              "\n",
              "[25000 rows x 2 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['review'] = temp\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SkLEd_rurQl"
      },
      "source": [
        "### **Dependent and independent features split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_2HYtmQuwF5"
      },
      "outputs": [],
      "source": [
        "X = features\n",
        "Y = df.label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxAFBz9RvUp1",
        "outputId": "4182d184-665b-4b1d-946b-c554145174c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[    0,     0,     0, ...,  1403,   110,     5],\n",
              "       [    0,     0,     0, ...,  5286,    35,  2770],\n",
              "       [ 2621, 30969,   535, ...,   162,   164,  2621],\n",
              "       ...,\n",
              "       [    0,     0,     0, ..., 12394, 12054,   503],\n",
              "       [    0,     0,     0, ...,    14,  1151,    23],\n",
              "       [    0,     0,     0, ...,   184,     8,   423]])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wu55A6-qvb7k",
        "outputId": "933633bb-97a9-4ffe-8ee7-613c1d4508e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        1\n",
              "1        0\n",
              "2        1\n",
              "3        0\n",
              "4        1\n",
              "        ..\n",
              "24995    0\n",
              "24996    1\n",
              "24997    0\n",
              "24998    1\n",
              "24999    0\n",
              "Name: label, Length: 25000, dtype: int64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNj6CdHxtrTA"
      },
      "source": [
        "### **Train Validation Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEFwTAsOKJJJ"
      },
      "outputs": [],
      "source": [
        "# idk why but using the sklearn library for train test split is crashing the notebook hence I am doing it manually\n",
        "# using 80% as training data, next 10% as validation data and remaining 10% as testing data\n",
        "\n",
        "X_train = X[:int(len(X)*0.8)]\n",
        "X_validation = X[int(len(X)*0.8):int(len(X)*0.9)]\n",
        "X_test = X[int(len(X)*0.9):]\n",
        "\n",
        "Y_train = Y[:int(len(Y)*0.8)]\n",
        "Y_validation = Y[int(len(X)*0.8):int(len(X)*0.9)]\n",
        "Y_test = Y[int(len(Y)*0.9):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH0Y2N0sNiug",
        "outputId": "3cb3a489-fc85-4ad5-d132-eaffbc898c11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20000, 200) (2500, 200) (2500, 200) (20000,) (2500,) (2500,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape,X_validation.shape,X_test.shape, Y_train.shape,Y_validation.shape,Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIVSWIxypmwS"
      },
      "source": [
        "### **DataLoaders**\n",
        "DataLoaders for this data can be created by following two steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCBVUXOmHGHZ"
      },
      "source": [
        "###### 1. Create a known format for accessing our data, using [TensorDataset](https://pytorch.org/docs/stable/data.html#) which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiSx-Ww4G6cQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpZdIItNpmwU"
      },
      "outputs": [],
      "source": [
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(np.array(X_train)), torch.from_numpy(np.array(Y_train)))\n",
        "valid_data = TensorDataset(torch.from_numpy(np.array(X_validation)), torch.from_numpy(np.array(Y_validation)))\n",
        "test_data = TensorDataset(torch.from_numpy(np.array(X_test)), torch.from_numpy(np.array(Y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F69y1tuAHK2b"
      },
      "source": [
        "###### 2. Create DataLoaders and batch our training, validation, and test Tensor datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMQu24ccG8ya"
      },
      "outputs": [],
      "source": [
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBzNbDFSHgLm"
      },
      "source": [
        "### **Batching**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en1OTAlXpmwZ",
        "outputId": "dca5f2e0-d712-49f4-e7ca-89ae672ba5cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 200])\n",
            "Sample input: \n",
            " tensor([[    0,     0,     0,  ...,   303,   127,    99],\n",
            "        [    0,     0,     0,  ...,  1388,   233, 10101],\n",
            "        [    6,   241,    72,  ...,    28,   428,   882],\n",
            "        ...,\n",
            "        [ 2110,   499,  1218,  ...,  5625,   984,  1069],\n",
            "        [    0,     0,     0,  ...,    18,   503,   486],\n",
            "        [  227,   411,    89,  ...,    31,   287,   437]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
            "        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
            "        1, 1])\n"
          ]
        }
      ],
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1UEfwcaEUPs"
      },
      "source": [
        "## **Training and Predicting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQMdUUkts9x6"
      },
      "source": [
        "### **Stacked LSTM RNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xIULz1Zpmwf",
        "outputId": "64fd93bb-b420-469a-c50f-d2be519a3276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ]
        }
      ],
      "source": [
        "# First checking if GPU is available or not\n",
        "import torch\n",
        "\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ULOkAgAJTz8"
      },
      "source": [
        "#### Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mv2SGxVsH3M"
      },
      "outputs": [],
      "source": [
        "# Defining Our Model which will perform sentimental analysis\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class Sentiment_Stacked_LSTM_RNN(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "\n",
        "    super(Sentiment_Stacked_LSTM_RNN, self).__init__()\n",
        "\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    # ----------------------------------------------------input layer-------------------------------------------------\n",
        "    \"\"\"An [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) that converts our word tokens (integers) into OHE embeddings of a specific size\"\"\"\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # ----------------------------------------------------hidden layer-------------------------------------------------\n",
        "    \"\"\"\n",
        "    An [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) defined by a hidden_state size and number of layers\n",
        "    We'll create an LSTM to use in our recurrent network, which takes in an input_size, a hidden_dim, a number of layers,\n",
        "    a dropout probability (for dropout between multiple layers), and a batch_first parameter.Most of the time, you're network will have better performance with more layers;\n",
        "    between 2-3. Adding more layers allows the network to learn really complex relationships.\n",
        "    \"\"\"\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,dropout=drop_prob, batch_first=True, bidirectional = False )\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    # --------------------------------------------------output layer - linear + sigmoid layer--------------------------------------\n",
        "    \"\"\" A fully-connected output layer that maps the LSTM layer outputs to a desired output_size\"\"\"\n",
        "    self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    \"\"\" A sigmoid activation layer which turns all outputs into a value 0-1; return **only the last sigmoid output** as the output of this network.\"\"\"\n",
        "    self.sig = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    \"\"\"\n",
        "    Perform a forward pass of our model on some input and hidden state.\n",
        "    \"\"\"\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    # embeddings and lstm_out\n",
        "    embeds = self.embedding(x) #doing contextual embedding here\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "    # stack up lstm outputs\n",
        "    lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "    # dropout and fully connected layer\n",
        "    out = self.dropout(lstm_out)\n",
        "    out = self.fc(out)\n",
        "\n",
        "    # sigmoid function\n",
        "    sig_out = self.sig(out)\n",
        "\n",
        "    # reshape to be batch_size first\n",
        "    sig_out = sig_out.view(batch_size, -1)\n",
        "    sig_out = sig_out[:, -1] # get last batch of labels\n",
        "\n",
        "    # return last sigmoid output and hidden state\n",
        "    return sig_out, hidden\n",
        "\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    ''' Initializes hidden state '''\n",
        "    # Create two new tensors with sizes n_layers x batch_size x hidden_dim,initialized to zero, for hidden state and cell state of LSTM\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    if(train_on_gpu):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "\n",
        "    return hidden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZARxJ74pmwn",
        "outputId": "967a83bb-e28a-4119-ccbb-61ba895181d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(50353, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the model with hyperparameters\n",
        "\n",
        "# vocab_size: Size of our vocabulary or the range of values for our input, word tokens.\n",
        "vocab_size = len(vocab_to_int) + 1 # +1 for zero padding + our word tokens\n",
        "\n",
        "# output_size: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
        "output_size = 1\n",
        "\n",
        "# embedding_dim: Number of columns in the embedding lookup table; size of our embeddings.\n",
        "embedding_dim = 400\n",
        "\n",
        "# hidden_dim: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
        "hidden_dim = 256\n",
        "\n",
        "# n_layers: Number of LSTM layers in the network. Typically between 1-3\n",
        "n_layers = 2\n",
        "\n",
        "# learning rate for optimizer\n",
        "lr=0.001\n",
        "\n",
        "# loss function - binary cross entropy, [BCELoss](https://pytorch.org/docs/stable/nn.html#bceloss) is designed to work with a single sigmoid output\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "net = Sentiment_Stacked_LSTM_RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "print(net)\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqUJfLXCJXhQ"
      },
      "source": [
        "#### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwKz-VSv64Z4"
      },
      "outputs": [],
      "source": [
        "# training parameters\n",
        "\n",
        "# Epochs - No of times to iterate through the training dataset - 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "epochs = 4\n",
        "\n",
        "counter = 0\n",
        "\n",
        "print_every = 100\n",
        "\n",
        "# gradient clipping - The maximum gradient value to clip at (to prevent exploding gradients).\n",
        "clip=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbyqcNsw7Gn3"
      },
      "outputs": [],
      "source": [
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "  net.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJqrvKg0pmwu",
        "outputId": "8acb9f1d-6eb8-4705-fdc7-9537431c95de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/4... Step: 100... Loss: 0.651117... Val Loss: 0.610166\n",
            "Epoch: 1/4... Step: 200... Loss: 0.573071... Val Loss: 0.595148\n",
            "Epoch: 1/4... Step: 300... Loss: 0.477250... Val Loss: 0.500567\n",
            "Epoch: 1/4... Step: 400... Loss: 0.676299... Val Loss: 0.677923\n",
            "Epoch: 2/4... Step: 500... Loss: 0.512277... Val Loss: 0.560526\n",
            "Epoch: 2/4... Step: 600... Loss: 0.307610... Val Loss: 0.438921\n",
            "Epoch: 2/4... Step: 700... Loss: 0.257671... Val Loss: 0.406977\n",
            "Epoch: 2/4... Step: 800... Loss: 0.317289... Val Loss: 0.423925\n",
            "Epoch: 3/4... Step: 900... Loss: 0.145008... Val Loss: 0.466749\n",
            "Epoch: 3/4... Step: 1000... Loss: 0.174054... Val Loss: 0.459306\n",
            "Epoch: 3/4... Step: 1100... Loss: 0.076392... Val Loss: 0.401823\n",
            "Epoch: 3/4... Step: 1200... Loss: 0.316865... Val Loss: 0.514169\n",
            "Epoch: 4/4... Step: 1300... Loss: 0.052432... Val Loss: 0.476855\n",
            "Epoch: 4/4... Step: 1400... Loss: 0.070502... Val Loss: 0.527440\n",
            "Epoch: 4/4... Step: 1500... Loss: 0.102899... Val Loss: 0.486298\n",
            "Epoch: 4/4... Step: 1600... Loss: 0.129784... Val Loss: 0.527302\n"
          ]
        }
      ],
      "source": [
        "net.train()\n",
        "\n",
        "# Training for some number of epochs\n",
        "\n",
        "for e in range(epochs):\n",
        "  # initialize hidden state\n",
        "  h = net.init_hidden(batch_size)\n",
        "\n",
        "  # batch loop\n",
        "  for inputs, labels in train_loader:\n",
        "    counter += 1\n",
        "\n",
        "    if(train_on_gpu):\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise we'd backprop through the entire training history\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    # zero accumulated gradients\n",
        "    net.zero_grad()\n",
        "\n",
        "    # get the output from the model\n",
        "    output, h = net(inputs, h)\n",
        "\n",
        "    # calculate the loss and perform backprop\n",
        "    loss = criterion(output.squeeze(), labels.float())\n",
        "    loss.backward()\n",
        "\n",
        "    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "    nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    # loss stats\n",
        "    if counter % print_every == 0:\n",
        "      # Get validation loss\n",
        "      val_h = net.init_hidden(batch_size)\n",
        "      val_losses = []\n",
        "      net.eval()\n",
        "      for inputs, labels in valid_loader:\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise we'd backprop through the entire training history\n",
        "        val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "        if(train_on_gpu):\n",
        "          inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        output, val_h = net(inputs, val_h)\n",
        "        val_loss = criterion(output.squeeze(), labels.float())\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "      net.train()\n",
        "      print(\"Epoch: {}/{}...\".format(e+1, epochs),\"Step: {}...\".format(counter),\"Loss: {:.6f}...\".format(loss.item()),\"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZEn0LHZpmwx"
      },
      "source": [
        "#### Testing the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koNCy_awtrV0"
      },
      "source": [
        "There are a few ways to test your network.\n",
        "\n",
        "* **Test data performance:** First, we'll see how our trained model performs on all of our defined test_data, above. We'll calculate the average loss and accuracy over the test data.\n",
        "\n",
        "* **Inference on user-generated data:** Second, we'll see if we can input just one example review at a time (without a label), and see what the trained model predicts. Looking at new, user input data like this, and predicting an output label, is called **inference**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd_GWDE0pmwz",
        "outputId": "a25ea349-3dd1-46d9-beb2-5d280a7f3111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.550\n",
            "Test accuracy: 0.816\n"
          ]
        }
      ],
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    # get predicted outputs\n",
        "    output, h = net(inputs, h)\n",
        "\n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "\n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irgW-x9Zpmw1"
      },
      "source": [
        "#### Inference on a test review\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAbB-XPsuEuK"
      },
      "source": [
        "\n",
        "You can change this test_review to any text that you want. Read it and think: is it pos or neg? Then see if your model predicts correctly!\n",
        "    \n",
        "> **Exercise:** Write a `predict` function that takes in a trained net, a plain text_review, and a sequence length, and prints out a custom statement for a positive or negative review!\n",
        "* You can use any functions that you've already defined or define any helper functions you want to complete `predict`, but it should just take in a trained net, a text review, and a sequence length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h6qY0fIpmw2"
      },
      "outputs": [],
      "source": [
        "# negative test review\n",
        "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcrLL0YZYvPo",
        "outputId": "73280829-190c-4320-b6b7-abd7f2e9719d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[138, 3, 35, 25, 235, 36, 162, 3, 19, 25, 287, 397]]\n"
          ]
        }
      ],
      "source": [
        "from string import punctuation\n",
        "\n",
        "def tokenize_review(test_review):\n",
        "    test_review = test_review.lower() # lowercase\n",
        "\n",
        "    test_text = \"\".join([char for char in test_review if char not in string.punctuation and not char.isdigit()])#remove punctuation and numbers\n",
        "\n",
        "    test_text = \" \".join([ps.stem(token) for token in test_text.split() if token not in stopwords]) # removing stopwords and doing stemming\n",
        "\n",
        "    # splitting by spaces\n",
        "    test_words = test_text.split()\n",
        "\n",
        "    # tokens\n",
        "    test_ints = []\n",
        "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
        "\n",
        "    return test_ints\n",
        "\n",
        "# test code and generate tokenized review\n",
        "test_ints = tokenize_review(test_review_neg)\n",
        "print(test_ints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZymOrN3bZYm",
        "outputId": "9ae0cb11-d42c-4f09-d463-7d38fe25161a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0 138   3  35  25 235  36 162   3  19  25\n",
            "  287 397]]\n"
          ]
        }
      ],
      "source": [
        "# test sequence padding\n",
        "seq_length = 200\n",
        "features = pad_features(test_ints, seq_length)\n",
        "\n",
        "print(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "j80t9nPNbmHf",
        "outputId": "16a0c19c-0750-48e7-c147-c5dc838449e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 200])\n"
          ]
        }
      ],
      "source": [
        "# test conversion to tensor and pass it to model\n",
        "feature_tensor = torch.from_numpy(features)\n",
        "print(feature_tensor.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axUZJ4mCpmw6"
      },
      "outputs": [],
      "source": [
        "def predict(net, test_review, sequence_length=200):\n",
        "    ''' Prints out whether a give review is predicted to be\n",
        "        positive or negative in sentiment, using a trained model.\n",
        "\n",
        "        params:\n",
        "        net - A trained net\n",
        "        test_review - a review made of normal text and punctuation\n",
        "        sequence_length - the padded length of a review\n",
        "        '''\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    # tokenize review\n",
        "    test_ints = tokenize_review(test_review)\n",
        "\n",
        "    # pad tokenize sequence\n",
        "    seq_length = sequence_length\n",
        "    features = pad_features(test_ints, seq_length)\n",
        "\n",
        "    # convert to tensor to pass to model\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "\n",
        "    batch_size = feature_tensor.size(0)\n",
        "\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    if(train_on_gpu):\n",
        "      feature_tensor = feature_tensor.cuda()\n",
        "\n",
        "    # get the output from the model\n",
        "    output, h = net(feature_tensor, h)\n",
        "\n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())\n",
        "    # printing output value, before rounding\n",
        "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
        "\n",
        "    # print custom response based on whether test_review is pos/neg\n",
        "    if(pred.item()==1):\n",
        "      print('Positive review detected!')\n",
        "    else:\n",
        "      print('Negative review detected!')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6eLet-tpmw9"
      },
      "outputs": [],
      "source": [
        "# positive test review\n",
        "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIB-pSP1pmxB",
        "outputId": "32c22614-ce29-4a3f-bcdd-65f810b9fa76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction value, pre-rounding: 0.005493\n",
            "Negative review detected!\n",
            "Prediction value, pre-rounding: 0.921936\n",
            "Positive review detected!\n"
          ]
        }
      ],
      "source": [
        "# call function\n",
        "# try negative and positive reviews!\n",
        "seq_length=200\n",
        "predict(net, test_review_neg, seq_length)\n",
        "predict(net, test_review_pos, seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzumecHbYoT7"
      },
      "source": [
        "### **Using PreTrained Model - BERT**\n",
        "Refer the next notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7oxoVUVWKRs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Z340tgd-tVMI",
        "MyKjPufwtSH0",
        "ZhtxRVMRtjuT",
        "qhea_nKuvbBV",
        "B10ZKhh03nXu",
        "tad_SruO7FsZ",
        "kS6nx_687Kyz",
        "1SkLEd_rurQl",
        "UNj6CdHxtrTA",
        "RIVSWIxypmwS",
        "qCBVUXOmHGHZ",
        "F69y1tuAHK2b",
        "xBzNbDFSHgLm",
        "B1UEfwcaEUPs",
        "nQMdUUkts9x6",
        "6ULOkAgAJTz8",
        "TqUJfLXCJXhQ",
        "4ZEn0LHZpmwx",
        "irgW-x9Zpmw1",
        "nzumecHbYoT7"
      ],
      "name": "7. Sentimental Analysis (Movie Review Classification) using Joint Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
