{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/NLP/blob/main/Word-Level-Classification/POS-Tagging/NN-Fine-Tuning.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"_IbYupXsMipD"},"source":["### **Transfer Learning - Fine Tuning Pretrained Models**\n","though spacy contains pretrained model, there is a way via which you can retrain those models on your own dataset"]},{"cell_type":"markdown","metadata":{"id":"niSmaXftJebn"},"source":["#### **Reading The Dataset** - **Conllu library**\n","\n","Training dataset contains already tagged sentence, such a file is stored as .conll file and hence to read such a file we need to use a conllu library.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4173,"status":"ok","timestamp":1626538837915,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"vCEkLuc4Jebp","outputId":"4b58ef77-a07c-4132-e9c0-3da35d2ebc59"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting conllu\n","  Downloading https://files.pythonhosted.org/packages/ae/be/be6959c3ff2dbfdd87de4be0ccdff577835b5d08b1d25bf7fd4aaf0d7add/conllu-4.4-py2.py3-none-any.whl\n","Installing collected packages: conllu\n","Successfully installed conllu-4.4\n"]}],"source":["!pip install conllu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lgt5qSstJebq"},"outputs":[],"source":["import conllu"]},{"cell_type":"markdown","metadata":{"id":"NtwkbRbEaczD"},"source":["##### Reading dataset 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8C243wdJebs"},"outputs":[],"source":["# reading a conll file with specific fields\n","path = \"/content/drive/MyDrive/001 My Skills/002 CS Engineering   Automated Math (BPHC)/004 Data Science (DS)   Artificial Intelligence (AI)/004 Textual Data (Unstructured Data) (Sequential Data)/001 Mono Lingual Language /001 English/003 NLP Tasks/001 NLU Tasks/1. Word Level Classification/Parts of Speech (POS) Tagging/train_dataset.conll\"\n","\n","import io\n","with open(path, \"r\", encoding=\"utf-8\") as data_file:\n","  sentlist = list(conllu.parse_incr(data_file,fields = ['form','langtag','postag']))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":332,"status":"ok","timestamp":1626541753118,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"VIMobrTtJebt","outputId":"525e3399-bff3-4ee3-fbe8-cf6ec47a5dd1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sentence is: TokenList<Pak, is, in, confused, state, of, mind, ., hui, ya, nahi, is, par, contradictory, statements, aa, rahe, hain, ., Nahi, huyi, to, tension, mai, kyon, ?>\n","Pak rest PROPN\n","is en VERB\n","in en ADP\n","confused en ADJ\n","state en NOUN\n","of en ADP\n","mind en VERB\n",". rest X\n","hui hi VERB\n","ya hi CONJ\n","nahi hi PART_NEG\n","is hi DET\n","par hi ADP\n","contradictory en NOUN\n","statements en NOUN\n","aa hi VERB\n","rahe hi VERB\n","hain hi VERB\n",". rest X\n","Nahi hi PART_NEG\n","huyi hi PART\n","to hi PART\n","tension en NOUN\n","mai hi ADP\n","kyon hi PRON_WH\n","? rest X\n"]}],"source":["for sent in sentlist:\n","  print(\"Sentence is: {}\".format(sent))\n","  for token in sent:\n","    print(token['form'],token['langtag'], token['postag'])\n","  break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":334,"status":"ok","timestamp":1626541856004,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"hp2575incwqM","outputId":"05e751ee-a64f-466d-8877-d26f828972ce"},"outputs":[{"data":{"text/plain":["{'form': '?', 'langtag': 'rest', 'postag': 'X'}"]},"execution_count":29,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["token"]},{"cell_type":"markdown","metadata":{"id":"EyO4N0j4agVz"},"source":["##### Reading dataset 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AtRedU3lJebq"},"outputs":[],"source":["# reading a CoNLL file without explicitly specifiying the fields\n","path = \"/content/drive/MyDrive/001 My Skills/002 CS Engineering   Automated Math (BPHC)/004 Data Science (DS)   Artificial Intelligence (AI)/004 Textual Data (Unstructured Data) (Sequential Data)/001 Mono Lingual Language /001 English/003 NLP Tasks/001 NLU Tasks/1. Word Level Classification/Parts of Speech (POS) Tagging/train_dataset_2.conllu\"\n","\n","import io\n","with open(path, \"r\", encoding=\"utf-8\") as data_file:\n","    sentlist = list(conllu.parse_incr(data_file))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":408,"status":"ok","timestamp":1626540941846,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"RsB1csQQJebr","outputId":"c296622d-ee91-4278-a92f-5025888f5a56"},"outputs":[{"name":"stdout","output_type":"stream","text":["TokenList<यह, एशिया, की, सबसे, बड़ी, मस्जिदों, में, से, एक, है, ।>\n","यह DET NP\n","एशिया PROPN NP\n","की ADP NP\n","सबसे ADV NP2\n","बड़ी ADJ NP2\n","मस्जिदों NOUN NP2\n","में ADP NP2\n","से ADP NP2\n","एक NUM NP3\n","है AUX VGF\n","। PUNCT BLK\n"]}],"source":["# iteraring through all the lines of a conll file read using conllu.parse\n","for sent in sentlist:\n","  print(sent)\n","  for token in sent:\n","    print(token['form'], token['upos'],token['misc']['ChunkId'])\n","    # token = sent[0]\n","  break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":370,"status":"ok","timestamp":1625581080089,"user":{"displayName":"Prashant Kodali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLR7kqr_lvUgRRZBGdR7UZo6oiaBC-TEd7nXt_Xw=s64","userId":"13858115222436552549"},"user_tz":-330},"id":"ND4dMlBtJebs","outputId":"3d5443ab-34f4-4990-e7b3-20df181a0ec8"},"outputs":[{"data":{"text/plain":["{'deprel': 'punct',\n"," 'deps': None,\n"," 'feats': None,\n"," 'form': '।',\n"," 'head': 9,\n"," 'id': 11,\n"," 'lemma': '।',\n"," 'misc': {'ChunkId': 'BLK', 'ChunkType': 'head', 'Translit': '.'},\n"," 'upos': 'PUNCT',\n"," 'xpos': 'SYM'}"]},"execution_count":33,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["token"]},{"cell_type":"markdown","metadata":{"id":"7Lv-g0akNbpk"},"source":["#### **Training using the above dataset**\n","Look at spacy documentation to learn how to do this"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1_MtUePNftT"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMSIKYdG+0RU2UU9FgdB79P","collapsed_sections":["niSmaXftJebn","7Lv-g0akNbpk"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
