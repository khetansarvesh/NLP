{"cells":[{"cell_type":"markdown","metadata":{"id":"VvL-FbmnFq2W"},"source":["# **N-Grams Language Modelling (LM) with Smoothening using NLTK Library**\n","NLTK library for Language modelling can be used for any language i.e. english, hindi, chinese, .....\n","\n","NLTK Library does not works great if we have a hyge amount of data, hence as an alternative to NLTK library we can use [\"KenLM (Kneser-Ney) Library\"](https://kheafield.com/code/kenlm/) for implementing LM models if we have huge amout of courpus since its implementation is speedy and efficient. \n","\n","KenLM does not have too many LM models hence you can use another alternative to KenLM which works great with large corpus and has many LM models -- SRILM. [SRILM - The SRI Language Modeling Toolkit](http://www.speech.sri.com/projects/srilm/), [SRLIM python package](https://srilm-python.readthedocs.io/en/latest/)"]},{"cell_type":"code","execution_count":2,"metadata":{"_uuid":"10bbe6360f9659217f9f40455301aa72d6b614a6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15455,"status":"ok","timestamp":1625944294683,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"FEy5BvPDdvyh","outputId":"acb10068-cd94-4ffe-ab4a-d930f6ddb718","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pip\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/ca/f0d790b6e18b3a6f3bd5e80c2ee4edbb5807286c21cdd0862ca933f751dd/pip-21.1.3-py3-none-any.whl (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.6MB 7.9MB/s \n","\u001b[?25hInstalling collected packages: pip\n","  Found existing installation: pip 19.3.1\n","    Uninstalling pip-19.3.1:\n","      Successfully uninstalled pip-19.3.1\n","Successfully installed pip-21.1.3\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (0.3.4)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","Collecting nltk==3.4\n","  Downloading nltk-3.4.zip (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 8.7 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4) (1.15.0)\n","Collecting singledispatch\n","  Downloading singledispatch-3.6.2-py2.py3-none-any.whl (8.2 kB)\n","Building wheels for collected packages: nltk\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.4-py3-none-any.whl size=1436397 sha256=96601ee2c1f109d1fddb5edd335a05cd95f5e4cfb20dad0a265de18b1ed32d76\n","  Stored in directory: /root/.cache/pip/wheels/13/b8/81/2349be11dd144dc7b68ab983b58cd2fae353cdc50bbdeb09d0\n","Successfully built nltk\n","Installing collected packages: singledispatch, nltk\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed nltk-3.4 singledispatch-3.6.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}],"source":["# downloading some required libraries because they are not present in the root kernel of google colab by default\n","!pip install -U pip\n","!pip install -U dill\n","!pip install -U nltk==3.4"]},{"cell_type":"markdown","metadata":{"id":"34FRx8gnelah"},"source":["## **PreRequisites**"]},{"cell_type":"code","execution_count":3,"metadata":{"_uuid":"1b213fb0dd5534ce82d6f1c716e9695ba5cf9758","executionInfo":{"elapsed":1958,"status":"ok","timestamp":1625944453205,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"4fClFf2Odvyn","trusted":true},"outputs":[],"source":["from nltk.util import bigrams\n","from nltk.util import ngrams"]},{"cell_type":"markdown","metadata":{"_uuid":"800f7632b6a834d36b95211a912b390fcb9dc11b","id":"_mlmEDd-dvyo"},"source":["If we want to train a bigram model, we need to turn this text into bigrams. Here's what the first sentence of our text would look like if we use the `ngrams` function from NLTK for this."]},{"cell_type":"code","execution_count":4,"metadata":{"_uuid":"1c1716261478af5e75bba7799bfefc10bd1ea4ea","executionInfo":{"elapsed":17,"status":"ok","timestamp":1625944453206,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"5BCFTNmFdvyp","trusted":true},"outputs":[],"source":["text = [['a', 'b', 'c'], ['a', 'c', 'd', 'c', 'e', 'f']]"]},{"cell_type":"code","execution_count":5,"metadata":{"_uuid":"a5a1c775bab7cf9c67656d4349d8bfca02a80738","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1625944453206,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"e4CjXAs5dvyp","outputId":"a9b3d90b-0ea1-4981-a25f-e75131970289","trusted":true},"outputs":[{"data":{"text/plain":["[('a', 'b'), ('b', 'c')]"]},"execution_count":5,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["list(bigrams(text[0]))"]},{"cell_type":"code","execution_count":6,"metadata":{"_uuid":"0219988318e39dd0576913b8087af8b35cbdab2f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1625944453207,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"vJol5nIpdvyq","outputId":"09f6cb15-397a-4a6c-dd32-0cb686af2149","trusted":true},"outputs":[{"data":{"text/plain":["[('a', 'c', 'd'), ('c', 'd', 'c'), ('d', 'c', 'e'), ('c', 'e', 'f')]"]},"execution_count":6,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["list(ngrams(text[1], n=3))"]},{"cell_type":"markdown","metadata":{"_uuid":"e3a92b25dc25f3ae86fd265880b7f410e981c670","id":"-MvoNQK7dvyq"},"source":["Notice how \"b\" occurs both as the first and second member of different bigrams but \"a\" and \"c\" don't? \n","\n","Wouldn't it be nice to somehow indicate how often sentences start with \"a\" and end with \"c\"?\n","\n","\n","A standard way to deal with this is to add special \"padding\" symbols to the sentence before splitting it into ngrams. Fortunately, NLTK also has a function for that, let's see what it does to the first sentence.Padding is done basically just to indicate the start of a sentence and end of a sentence\n"]},{"cell_type":"code","execution_count":7,"metadata":{"_uuid":"1ec58c94f58429c9160b4496c939f211f88ade54","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1625944453208,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"IlsZ9n27dvyr","outputId":"a255ac48-ec82-46c9-c752-86b79f0d41e4","trusted":true},"outputs":[{"data":{"text/plain":["['<start>', '<start>', 'a', 'b', 'c', '</end>', '</end>']"]},"execution_count":7,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["from nltk.util import pad_sequence\n","list(pad_sequence(text[0],\n","                  pad_left=True, left_pad_symbol=\"<start>\",\n","                  pad_right=True, right_pad_symbol=\"</end>\",\n","                  n=3)) # The n order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc. "]},{"cell_type":"code","execution_count":8,"metadata":{"_uuid":"4cda3fcbd1b8ab48431240e04168011953ebc913","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1625944453208,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"bhHYn5tFdvys","outputId":"43539d4d-5a2f-495c-b33a-d5ee1a38226e","trusted":true},"outputs":[{"data":{"text/plain":["['<s>', '<s>', 'a', 'b', 'c', '</s>', '</s>']"]},"execution_count":8,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["list(pad_sequence(text[0],\n","                  pad_left=True, left_pad_symbol=\"<s>\",\n","                  pad_right=True, right_pad_symbol=\"</s>\",\n","                  n=3)) # The n order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc. "]},{"cell_type":"code","execution_count":9,"metadata":{"_uuid":"ae38644238aa46ac100381c0de046972090b093b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1625944453208,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"AyTFFYUgdvys","outputId":"cf6e959f-b046-4341-fbba-389dd7af935d","trusted":true},"outputs":[{"data":{"text/plain":["[('<s>', 'a'), ('a', 'b'), ('b', 'c'), ('c', '</s>')]"]},"execution_count":9,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["padded_sent = list(pad_sequence(text[0], pad_left=True, left_pad_symbol=\"<s>\", \n","                                pad_right=True, right_pad_symbol=\"</s>\", n=2))\n","list(ngrams(padded_sent, n=2))"]},{"cell_type":"code","execution_count":10,"metadata":{"_uuid":"bd572a57371a716471d156144e7ebc136072de01","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1625944453209,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"-FIJiUO0dvys","outputId":"0b3709cf-9c51-4a2f-ac1e-47639dbcabb4","trusted":true},"outputs":[{"data":{"text/plain":["[('<s>', '<s>', 'a'),\n"," ('<s>', 'a', 'b'),\n"," ('a', 'b', 'c'),\n"," ('b', 'c', '</s>'),\n"," ('c', '</s>', '</s>')]"]},"execution_count":10,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["padded_sent = list(pad_sequence(text[0], pad_left=True, left_pad_symbol=\"<s>\", \n","                                pad_right=True, right_pad_symbol=\"</s>\", n=3))\n","list(ngrams(padded_sent, n=3))"]},{"cell_type":"markdown","metadata":{"_uuid":"e4d80e5707a2efa7ea5cee601d07bca23bc0ae87","id":"3bdtVeQZdvyt"},"source":["Note the `n` argument, that tells the function we need padding for bigrams.\n","\n","Now, passing all these parameters every time is tedious and in most cases they can be safely assumed as defaults anyway.\n","\n","Thus the `nltk.lm` module provides a convenience function that has all these arguments already set while the other arguments remain the same as for `pad_sequence`."]},{"cell_type":"code","execution_count":11,"metadata":{"_uuid":"eb64c3c2a205c5fc5e1fce6f63c3e9038f0d8c4a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1625944453209,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"c0417umHdvyu","outputId":"60a257c6-b4c8-416c-c600-3f1c1ffce803","trusted":true},"outputs":[{"data":{"text/plain":["['<s>', 'a', 'b', 'c', '</s>']"]},"execution_count":11,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["from nltk.lm.preprocessing import pad_both_ends\n","list(pad_both_ends(text[0], n=2))"]},{"cell_type":"markdown","metadata":{"_uuid":"2d2581fae73d38d623a77b5d523c957df2dc8478","id":"joR0DPJHdvyu"},"source":["Combining the two parts discussed so far we get the following preparation steps for one sentence."]},{"cell_type":"code","execution_count":12,"metadata":{"_uuid":"1168917cc340d400f6dba9b2703561785481d8e4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1625944453209,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"j6TNQoSrdvyu","outputId":"d99096ea-cf80-4ee5-ea2c-02cd02b793b7","trusted":true},"outputs":[{"data":{"text/plain":["[('<s>', 'a'), ('a', 'b'), ('b', 'c'), ('c', '</s>')]"]},"execution_count":12,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["list(bigrams(pad_both_ends(text[0], n=2)))"]},{"cell_type":"markdown","metadata":{"_uuid":"2778a5e5b8d95bc395ba41af491818140c7f680e","id":"qtERMlAJdvyv"},"source":["To make our model more robust we could also train it on unigrams (single words) as well as bigrams, its main source of information.\n","NLTK once again helpfully provides a function called `everygrams`.\n","\n","While not the most efficient, it is conceptually simple."]},{"cell_type":"code","execution_count":13,"metadata":{"_uuid":"dfdc9effca718a7881f1fdc4562cdfb29164b96c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1625944453210,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"HXFEFKkHdvyv","outputId":"9a4f278a-5e20-46ce-d5dc-bf20a8f58e51","trusted":true},"outputs":[{"data":{"text/plain":["[('<s>',),\n"," ('a',),\n"," ('b',),\n"," ('c',),\n"," ('</s>',),\n"," ('<s>', 'a'),\n"," ('a', 'b'),\n"," ('b', 'c'),\n"," ('c', '</s>')]"]},"execution_count":13,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["from nltk.util import everygrams\n","padded_bigrams = list(pad_both_ends(text[0], n=2))\n","list(everygrams(padded_bigrams, max_len=2))"]},{"cell_type":"markdown","metadata":{"_uuid":"d74b0ba683b733d5a83756361a905625a841e68d","id":"on22ZtSIdvyv"},"source":["During training and evaluation our model will rely on a vocabulary that defines which words are \"known\" to the model.\n","\n","To create this vocabulary we need to pad our sentences (just like for counting ngrams) and then combine the sentences into one flat stream of words.\n"]},{"cell_type":"code","execution_count":14,"metadata":{"_uuid":"310c937ea7cc847a26f6c3d08c7169e8cd6b354a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1625944453210,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"eJeAY4tXdvyw","outputId":"23b27f1e-8353-4aff-a960-369ad2f7232d","trusted":true},"outputs":[{"data":{"text/plain":["['<s>', 'a', 'b', 'c', '</s>', '<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']"]},"execution_count":14,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["from nltk.lm.preprocessing import flatten\n","list(flatten(pad_both_ends(sent, n=2) for sent in text))"]},{"cell_type":"markdown","metadata":{"_uuid":"5bab7d656c474d945669e6a86bf4beec3a44189e","id":"l9SFnx_gdvyw"},"source":["In most cases we want to use the same text as the source for both vocabulary and ngram counts.\n","\n","Now that we understand what this means for our preprocessing, we can simply import a function that does everything for us."]},{"cell_type":"code","execution_count":15,"metadata":{"_uuid":"147ccdd961ac97492ee9f0eadf3dfce7325c6790","executionInfo":{"elapsed":11,"status":"ok","timestamp":1625944453210,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"dANcJhehdvyx","trusted":true},"outputs":[],"source":["from nltk.lm.preprocessing import padded_everygram_pipeline\n","train, vocab = padded_everygram_pipeline(2, text)"]},{"cell_type":"markdown","metadata":{"_uuid":"b1b15504f2fb4487deec19de1c5ca786e49a42d6","id":"q7Xc2jxudvyx"},"source":["So as to avoid re-creating the text in memory, both `train` and `vocab` are lazy iterators. They are evaluated on demand at training time.\n","\n","For the sake of understanding the output of `padded_everygram_pipeline`, we'll \"materialize\" the lazy iterators by casting them into a list."]},{"cell_type":"code","execution_count":16,"metadata":{"_uuid":"aa18a8a4a700926c30488c94cf519d8900de8ddf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1625944453211,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"NgEGmKGqdvyy","outputId":"e0beebcb-2b97-4cbb-8cca-593f21ccd544","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[('<s>',), ('a',), ('b',), ('c',), ('</s>',), ('<s>', 'a'), ('a', 'b'), ('b', 'c'), ('c', '</s>')]\n","\n","[('<s>',), ('a',), ('c',), ('d',), ('c',), ('e',), ('f',), ('</s>',), ('<s>', 'a'), ('a', 'c'), ('c', 'd'), ('d', 'c'), ('c', 'e'), ('e', 'f'), ('f', '</s>')]\n","\n","#############\n"]},{"data":{"text/plain":["['<s>', 'a', 'b', 'c', '</s>', '<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']"]},"execution_count":16,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["training_ngrams, padded_sentences = padded_everygram_pipeline(2, text)\n","for ngramlize_sent in training_ngrams:\n","    print(list(ngramlize_sent))\n","    print()\n","print('#############')\n","list(padded_sentences)"]},{"cell_type":"markdown","metadata":{"id":"v3m4z0duBjYF"},"source":["## **Example 1**"]},{"cell_type":"markdown","metadata":{"_uuid":"4638a871a58694a77473a590728ebf25047cd6eb","id":"-0ujSJbydvyz"},"source":["### **Lets get some real data and tokenize it**"]},{"cell_type":"code","execution_count":23,"metadata":{"_uuid":"c679084566e711286d861a622e21ca5c04db7bee","executionInfo":{"elapsed":365,"status":"ok","timestamp":1625945250023,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"nhrm6TSKdvyz","trusted":true},"outputs":[],"source":["try: # Use the default NLTK tokenizer.\n","    from nltk import word_tokenize, sent_tokenize \n","    # Testing whether it works. Sometimes it doesn't work on some machines because of setup issues.\n","    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n","except: # Use a naive sentence tokenizer and toktok.\n","    import re\n","    from nltk.tokenize import ToktokTokenizer\n","    # See https://stackoverflow.com/a/25736515/610569\n","    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n","    # Use the toktok tokenizer that requires no dependencies.\n","    toktok = ToktokTokenizer()\n","    word_tokenize = word_tokenize = toktok.tokenize"]},{"cell_type":"code","execution_count":24,"metadata":{"_uuid":"9bb181f26fb777464b7d4e6d08beac724864cefe","executionInfo":{"elapsed":4,"status":"ok","timestamp":1625945250410,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"m-63I0P-dvy0","trusted":true},"outputs":[],"source":["import os\n","import requests\n","import io \n","\n","# Text version of https://kilgarriff.co.uk/Publications/2005-K-lineer.pdf\n","if os.path.isfile('language-never-random.txt'):\n","    with io.open('language-never-random.txt', encoding='utf8') as fin:\n","        text = fin.read()\n","else:\n","    url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n","    text = requests.get(url).content.decode('utf8')\n","    with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n","        fout.write(text)"]},{"cell_type":"code","execution_count":25,"metadata":{"_uuid":"91c93337cfea53b0c15a6e8617395dab872fb140","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1625945250410,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"okaEG50Rdvy1","outputId":"2d1fe7d8-7f03-4a9f-8330-2c73c339c850","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                       Language is never, ever, ever, random\n","\n","                                                               ADAM KILGARRIFF\n","\n","\n","\n","\n","Abstract\n","Language users never choose words randomly, and language is essentially\n","non-random. Statistical hypothesis testing uses a null hypothesis, which\n","posits randomness. Hence, when we look at linguistic phenomena in cor-\n","pora, the null hypothesis will never be true. Moreover, where there is enough\n","data, we shall (almost) always be able to establish \n"]}],"source":["print(text[:500])"]},{"cell_type":"code","execution_count":26,"metadata":{"_uuid":"af4102fcbf643e09319ca6394c175ca64082fd19","executionInfo":{"elapsed":613,"status":"ok","timestamp":1625945251021,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"uva2j-dtdvy1","trusted":true},"outputs":[],"source":["# Tokenize the text.\n","tokenized_text = [list(map(str.lower, word_tokenize(sent))) \n","                  for sent in sent_tokenize(text)]"]},{"cell_type":"code","execution_count":27,"metadata":{"_uuid":"9daa634f65b63882f7192a9489fba198970f3771","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1625945251022,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"3BnftySjdvy1","outputId":"f0d96c92-4da6-46d7-b319-ba942b2698d3","trusted":true},"outputs":[{"data":{"text/plain":["['language',\n"," 'is',\n"," 'never',\n"," ',',\n"," 'ever',\n"," ',',\n"," 'ever',\n"," ',',\n"," 'random',\n"," 'adam',\n"," 'kilgarriff',\n"," 'abstract',\n"," 'language',\n"," 'users',\n"," 'never',\n"," 'choose',\n"," 'words',\n"," 'randomly',\n"," ',',\n"," 'and',\n"," 'language',\n"," 'is',\n"," 'essentially',\n"," 'non-random',\n"," '.']"]},"execution_count":27,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["tokenized_text[0]"]},{"cell_type":"code","execution_count":28,"metadata":{"_uuid":"3eef00b8b1a95043e1c8bdd6912ae22060198e5c","executionInfo":{"elapsed":3,"status":"ok","timestamp":1625945251023,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"-sSp8USqdvy2","trusted":true},"outputs":[],"source":["# Preprocess the tokenized text for 3-grams language modelling\n","n = 3\n","train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)"]},{"cell_type":"markdown","metadata":{"_uuid":"32c5ad89e3e61b143e940d50a0d0ee602dadfe3f","id":"uWPwuldadvy2"},"source":["### **Training an N-gram Model**"]},{"cell_type":"markdown","metadata":{"_uuid":"ff1ff506e3a14df8283cb93b0a45f86862d3e3c7","id":"XS1akXR5dvy2"},"source":["Having prepared our data we are ready to start training a model. As a simple example, let us train a Maximum Likelihood Estimator (MLE).\n","\n","We only need to specify the highest ngram order to instantiate it."]},{"cell_type":"code","execution_count":29,"metadata":{"_uuid":"2635458b0f3fed618bfae6a0705ccb11555bfb0b","executionInfo":{"elapsed":447,"status":"ok","timestamp":1625945253610,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"Si8G3g6vdvy2","trusted":true},"outputs":[],"source":["from nltk.lm import MLE\n","model = MLE(n) # Lets train a 3-grams model, previously we set n=3\n","# there are numerous language models in nltk.lm, MLE happens to be one of those. Some of the other models are\n","# `Lidstone`: Provides Lidstone-smoothed scores.\n","# `Laplace`: Implements Laplace (add one) smoothing.\n","# `InterpolatedLanguageModel`: Logic common to all interpolated language models (Chen & Goodman 1995).\n","# `WittenBellInterpolated`: Interpolated version of Witten-Bell smoothing.\n","\n","#  In all these models everything remains same just the way the probabilities are calculated changes -> i.e. way the smoothening is done changes\n","#For more details take a look at these objects from `nltk.lm.models`-(https://github.com/nltk/nltk/blob/develop/nltk/lm/models.py):\n","\n"]},{"cell_type":"markdown","metadata":{"_uuid":"69cdd0e5babaf059a7b9bc87dc0a6e261cef6cb2","id":"waDZwYUCdvy3"},"source":["Initializing the MLE model, creates an empty vocabulary"]},{"cell_type":"code","execution_count":30,"metadata":{"_uuid":"93b95f89f020585dabfbcd6c41273c823a91e882","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1625945253610,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"y7XzryYydvy3","outputId":"5670409b-a924-4d6d-eee9-2b489e7b55f3","trusted":true},"outputs":[{"data":{"text/plain":["0"]},"execution_count":30,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["len(model.vocab) #hence before trining we have zero vocabulary size"]},{"cell_type":"markdown","metadata":{"_uuid":"fb567e332a0089c86e388a8c93f32d1a31737a29","id":"bRNd6fKjdvy3"},"source":["... which gets filled as we fit the model."]},{"cell_type":"code","execution_count":31,"metadata":{"_uuid":"a31ecc7f6df30a42df9ed6a79039b50f734299cf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1625945253610,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"5Nd-AFmRdvy3","outputId":"c6b38d99-ed70-47a1-ad4c-f2fa8dc2533b","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<Vocabulary with cutoff=1 unk_label='<UNK>' and 1429 items>\n"]}],"source":["model.fit(train_data, padded_sents)\n","print(model.vocab)"]},{"cell_type":"code","execution_count":32,"metadata":{"_uuid":"6fd7e68c2cdacbe1901fd3b8c05c440706d19100","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1625945253611,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"fhbS2-IPdvy4","outputId":"7081928d-5612-4e59-b97a-1a4566b47f6c","trusted":true},"outputs":[{"data":{"text/plain":["1429"]},"execution_count":32,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["len(model.vocab) # hence after training we get a non zero vocabulary size"]},{"cell_type":"markdown","metadata":{"_uuid":"74622c58a2df2ac08c8b0c79644d1f87f4014f44","id":"L9ynklhFdvy4"},"source":["The vocabulary helps us handle words that have not occurred during training."]},{"cell_type":"code","execution_count":33,"metadata":{"_uuid":"e5670e34f9d1950f6a1515d57db18092d39484f2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1625945253611,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"z0nSY-1Qdvy4","outputId":"a92c2794-e8e3-4822-deae-fbb4bfd9ef10","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["('language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.')\n"]}],"source":["print(model.vocab.lookup(tokenized_text[0])) #looking at the vocabulary"]},{"cell_type":"code","execution_count":34,"metadata":{"_uuid":"cfec70768e8976073dda05f702f3f210a33dc1c3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1625945253611,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"wwQFEwUudvy4","outputId":"9ef8ef59-fc7b-4cb6-eb9d-dcdd5ea78df0","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["('language', 'is', 'never', 'random', '<UNK>', '.')\n"]}],"source":["# If we lookup the vocab on unseen sentences not from the training data, it automatically replace words not in the vocabulary with `<UNK>`.\n","print(model.vocab.lookup('language is never random lah .'.split()))"]},{"cell_type":"markdown","metadata":{"_uuid":"83a71e4b25b53795dedce91c1a5686b7e22279e0","id":"0cArb13ydvy5"},"source":["*Moreover*, in some cases we want to ignore words that we did see during training but that didn't occur frequently enough, to provide us useful information. \n","\n","You can tell the vocabulary to ignore such words using the `unk_cutoff` argument for the vocabulary lookup, To find out how that works, check out the docs for the [`nltk.lm.vocabulary.Vocabulary` class](https://github.com/nltk/nltk/blob/develop/nltk/lm/vocabulary.py)"]},{"cell_type":"markdown","metadata":{"_uuid":"af5a0851a2f8707ea2e172681342ed3ecd872328","id":"3HO4aUhldvy5"},"source":["### **Using the N-gram Language Model**"]},{"cell_type":"markdown","metadata":{"_uuid":"d098fd4686bcdc0ab9aa72dbbd4b62a17ee2c332","id":"X_-kyJD9dvy6"},"source":["When it comes to ngram models the training boils down to counting up the ngrams from the training corpus."]},{"cell_type":"code","execution_count":35,"metadata":{"_uuid":"bfc60d61539269298390044c0d3415bfd28e2b1e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":388,"status":"ok","timestamp":1625945261770,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"ydyKVBtedvy6","outputId":"ddcb8ee4-3718-4fe8-a8a0-bffe3dcc096c","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<NgramCounter with 3 ngram orders and 18687 ngrams>\n"]}],"source":["print(model.counts) # vocabulary was around 1400 but counts are 18687 because count takes into account same tokens also "]},{"cell_type":"markdown","metadata":{"_uuid":"e9b8cac68de80aee9c7cf7b7f2faf2b199ad27cc","id":"Q5mUSc8Wdvy6"},"source":["This provides a convenient interface to access counts for unigrams..."]},{"cell_type":"code","execution_count":36,"metadata":{"_uuid":"90f4611580c41118747a42cfac4c9d729f02523f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1625945262175,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"Sjn7md3vdvy6","outputId":"c0144a62-62cf-4ab1-ee8c-9a939f568e3b","trusted":true},"outputs":[{"data":{"text/plain":["25"]},"execution_count":36,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model.counts['language'] # i.e. Count('language') # means language occured 25 times "]},{"cell_type":"markdown","metadata":{"_uuid":"ccdf1bf83ba16f0a43f04eb96ca224a968cc81d2","id":"S3LOLzsrdvy7"},"source":["...and bigrams for the phrase \"language is\""]},{"cell_type":"code","execution_count":37,"metadata":{"_uuid":"3ba55a83d7c052d1f14923c4046a5f6c86d72c8c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1625945262176,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"En_Zfl6odvy7","outputId":"564fe59c-3bf3-4563-fa92-5b5af461a471","trusted":true},"outputs":[{"data":{"text/plain":["11"]},"execution_count":37,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model.counts[['language']]['is'] # i.e. Count('is'|'language')# language is # means language is occured 11 times"]},{"cell_type":"markdown","metadata":{"_uuid":"88bceda0a01e9f2642dd30a6b341c24a606720b9","id":"UHCVsjx0dvy7"},"source":["... and trigrams for the phrase \"language is never\""]},{"cell_type":"code","execution_count":38,"metadata":{"_uuid":"813e30e1fb153002cfb020d430def2510bf43bbc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1625945262176,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"y7vsNmYkdvy7","outputId":"59cb0fd3-a92b-4b68-be25-372ebd0a440e","trusted":true},"outputs":[{"data":{"text/plain":["7"]},"execution_count":38,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model.counts[['language', 'is']]['never'] # i.e. Count('never'|'language is') #lang is never # means language is never occured 7 times"]},{"cell_type":"markdown","metadata":{"_uuid":"2c4338d351cfc3cd785673ad9581ecab161b24c6","id":"M0TGmhJkdvy8"},"source":["And so on. However, the real purpose of training a language model is to have it score how probable words are in certain contexts.\n","\n","This being MLE, the model returns the item's relative frequency as its score."]},{"cell_type":"code","execution_count":39,"metadata":{"_uuid":"051ef5a06c004a8e8ddb6168ca318bc8c0c9abf4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1625945262177,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"roCBb2_Fdvy8","outputId":"387d7498-c925-4fa9-f523-e8f6c86ba317","trusted":true},"outputs":[{"data":{"text/plain":["0.003916040100250626"]},"execution_count":39,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model.score('language') # P('language') # score is a probability value while count is not a probability value"]},{"cell_type":"code","execution_count":40,"metadata":{"_uuid":"ab3d48919b8f074342e624a1da441621829a4dbd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1625945262177,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"8cCfBvADdvy8","outputId":"ff87d07b-d4de-4211-9152-9091d7febedd","trusted":true},"outputs":[{"data":{"text/plain":["0.44"]},"execution_count":40,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model.score('is', 'language'.split())  # P('is'|'language')"]},{"cell_type":"code","execution_count":41,"metadata":{"_uuid":"73955908d499f459e19c2cbfaec7d94c513acb91","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":558,"status":"ok","timestamp":1625945262728,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"KSAs98oLdvy9","outputId":"bce05b98-fd51-4ac3-cb90-af3378745dda","trusted":true},"outputs":[{"data":{"text/plain":["0.6363636363636364"]},"execution_count":41,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model.score('never', 'language is'.split())  # P('never'|'language is')"]},{"cell_type":"markdown","metadata":{"_uuid":"946dde75e8d5b8d8878271359b0d91926abeb199","id":"-Lz_wkS6dvy9"},"source":["Items that are not seen during training are mapped to the vocabulary's \"unknown label\" token.  This is \"<UNK>\" by default.\n"]},{"cell_type":"code","execution_count":42,"metadata":{"_uuid":"44038e8cd8d078b734a6b1a803d0795517e2fa82","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1625945262729,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"xh7TQyhQdvy9","outputId":"3fad3442-5b0d-4df2-b14a-7024746c1adb","trusted":true},"outputs":[{"data":{"text/plain":["True"]},"execution_count":42,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model.score(\"<UNK>\") == model.score(\"lah\")"]},{"cell_type":"code","execution_count":43,"metadata":{"_uuid":"f2f170a0bbca4b33b5eb1a55a55f0e1b1b18a634","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1625945262729,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"Lq4vw0fLdvy-","outputId":"b21b78c6-4fbc-4dab-bdd8-92e12d67cf29","trusted":true},"outputs":[{"data":{"text/plain":["True"]},"execution_count":43,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model.score(\"<UNK>\") == model.score(\"leh\")"]},{"cell_type":"code","execution_count":44,"metadata":{"_uuid":"9107d1a7c27f30a449e9f370002a6406d5a1396d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1625945262730,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"cty2gzPWdvy-","outputId":"69cfa7b0-8bb0-4b4a-f6cd-97d76c0d3475","trusted":true},"outputs":[{"data":{"text/plain":["True"]},"execution_count":44,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model.score(\"<UNK>\") == model.score(\"lor\")"]},{"cell_type":"markdown","metadata":{"_uuid":"1a9ad0be90c72ece6c2a1c00314ea60f41a00f81","id":"hySNx76Ldvy-"},"source":["To avoid underflow when working with many small score values it makes sense to take their logarithm. \n","\n","For convenience this can be done with the `logscore` method.\n"]},{"cell_type":"code","execution_count":45,"metadata":{"_uuid":"20ef81b7e161df3b9ab27236c33559713b3077ce","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1625945262731,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"SM8UaDnzdvy_","outputId":"e58a382f-f52b-4e8d-a8ff-c67cdf10dfac","trusted":true},"outputs":[{"data":{"text/plain":["-0.6520766965796932"]},"execution_count":45,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model.logscore(\"never\", \"language is\".split()) #log base 10"]},{"cell_type":"markdown","metadata":{"_uuid":"18f9e0a8d0aba302532b8a84f342d1bf4d3e202a","id":"l0ooKFlEdvy_"},"source":["### **Generation using N-gram Language Model**"]},{"cell_type":"markdown","metadata":{"_uuid":"f893159fe093aef484c07b37d922de4ac5727834","id":"sFT0tVyqdvzA"},"source":["One cool feature of ngram models is that they can be used to generate text."]},{"cell_type":"code","execution_count":46,"metadata":{"_uuid":"98989ec4bae592fc98332e759daf9e42bac4213e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":664,"status":"ok","timestamp":1625945374935,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"2pSO07--dvzA","outputId":"6589e28a-18ad-4f63-f6e2-a887427ea5e7","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['ate', 'inferences', 'are', 'drawn.', '2', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"]}],"source":["print(model.generate(20, random_seed=7))"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1625945374936,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"hzsCn2lrOcGZ","outputId":"65a0e65a-e4ea-4859-ca35-07b9f90e0719"},"outputs":[{"name":"stdout","output_type":"stream","text":["['exact', 'method', 'can', 'be', 'applied', 'to', 'em-', 'pirical', 'linguistics', 'in', 'gale', 'and', 'sampson', '(', '1995', ')', ',', '33⫺46.', 'brent', ',']\n"]}],"source":["print(model.generate(20,text_seed=\"the problem is\", random_seed=7))"]},{"cell_type":"markdown","metadata":{"_uuid":"3b8d07eaf3afae978131573ae127fa0ec3f2e50d","id":"iHPaExTCdvzA"},"source":["We can do some cleaning to the generated tokens to make it human-like."]},{"cell_type":"code","execution_count":48,"metadata":{"_uuid":"73d0d5e0029e64876100e0f2e368b4835a99efcd","executionInfo":{"elapsed":13,"status":"ok","timestamp":1625945374936,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"LBpE_e1OdvzB","trusted":true},"outputs":[],"source":["from nltk.tokenize.treebank import TreebankWordDetokenizer\n","\n","detokenize = TreebankWordDetokenizer().detokenize\n","\n","def generate_sent(model, num_words, random_seed=42):\n","    \"\"\"\n","    :param model: An ngram language model from `nltk.lm.model`.\n","    :param num_words: Max no. of words to generate.\n","    :param random_seed: Seed value for random.\n","    \"\"\"\n","    content = []\n","    for token in model.generate(num_words, random_seed=random_seed):\n","        if token == '<s>':\n","            continue\n","        if token == '</s>':\n","            break\n","        content.append(token)\n","    return detokenize(content)"]},{"cell_type":"code","execution_count":49,"metadata":{"_uuid":"949378240cbc8247579b79946b113a3afd039b39","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1625945374938,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"myRJfD82dvzB","outputId":"d8e469d3-4136-4851-ceb0-4aca17637d56","trusted":true},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'ate inferences are drawn. 2.'"]},"execution_count":49,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["generate_sent(model, 20, random_seed=7)"]},{"cell_type":"code","execution_count":50,"metadata":{"_uuid":"9041c514e0458236132fe9b3c42ac2ce651beb92","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1625945374939,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"hZ62W5wjdvzB","outputId":"3110bdf9-fad9-4202-e2dc-1de393c75029","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['the', 'trouble', 'with', 'quantitative', 'studies', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"]}],"source":["print(model.generate(28, random_seed=0))"]},{"cell_type":"code","execution_count":51,"metadata":{"_uuid":"523e7c2373f6a4c4b6a542f1e82369fedb6cbd21","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1625945374939,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"R3bQM8a1dvzC","outputId":"ea4cedcb-f9c9-48f4-a5a5-de83f5ba6e76","trusted":true},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'the trouble with quantitative studies.'"]},"execution_count":51,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["generate_sent(model, 28, random_seed=0)"]},{"cell_type":"code","execution_count":52,"metadata":{"_uuid":"bce2885bd22d0bb1f79c3861f10fde7df6714e40","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1625945374940,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"tJXmZq4JdvzC","outputId":"f7979544-7adb-4b62-e5b4-30b556392193","trusted":true},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'29⫺50. manning, christopher and hinrich schütze 1999 foundations of statistical independence.'"]},"execution_count":52,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["generate_sent(model, 20, random_seed=1)"]},{"cell_type":"code","execution_count":53,"metadata":{"_uuid":"90ddb0e7d0fb52bc77d08425331944c55ee0885e","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1625945374940,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"qBZL9K8MdvzC","outputId":"9ce3e380-c9d4-4918-91fa-86111769d207","trusted":true},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'information glut, is inappropriate, particularly where counts are low.'"]},"execution_count":53,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["generate_sent(model, 20, random_seed=30)"]},{"cell_type":"code","execution_count":54,"metadata":{"_uuid":"ba9546c6800d9e0b0f0af4f0fd31a141af93b12f","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1625945374940,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"lIYQxoHpdvzC","outputId":"355f92c6-1466-4982-f26e-e325cbe30799","trusted":true},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'not random, or to refer to items that are more common or more salient in the last paragraph is'"]},"execution_count":54,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["generate_sent(model, 20, random_seed=42)"]},{"cell_type":"markdown","metadata":{"id":"nQepoZVXdvzD"},"source":["### **Saving the model** \n","\n","The native Python's pickle may not save the lambda functions in the  model, so we can use the `dill` library in place of pickle to save and load the language model.\n"]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":555,"status":"ok","timestamp":1625946026828,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"lyUt_pXJdvzD","trusted":true},"outputs":[],"source":["import dill as pickle \n","\n","#saving the model\n","with open('kilgariff_ngram_model.pkl', 'wb') as fout:\n","    pickle.dump(model, fout)"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1625946027208,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"tSm88l4JdvzD","trusted":true},"outputs":[],"source":["#using the save model\n","with open('kilgariff_ngram_model.pkl', 'rb') as fin:\n","    model_loaded = pickle.load(fin)"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1625946027209,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"oxDn_DY9dvzD","outputId":"785202af-f301-46b5-f2e4-ff56ad734557","trusted":true},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'not random, or to refer to items that are more common or more salient in the last paragraph is'"]},"execution_count":57,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["#saved model works :))))\n","generate_sent(model_loaded, 20, random_seed=42)"]},{"cell_type":"markdown","metadata":{"_uuid":"e4f13ed7359c87e397229104c1bcf18eb20603ad","id":"0A4tb2gGdvzE"},"source":["## **Example 2**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"caf5fea33f84e06c3cf613136ec33f80e774fc98","colab":{"base_uri":"https://localhost:8080/","height":569},"executionInfo":{"elapsed":526,"status":"ok","timestamp":1625648639671,"user":{"displayName":"Prashant Kodali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLR7kqr_lvUgRRZBGdR7UZo6oiaBC-TEd7nXt_Xw=s64","userId":"13858115222436552549"},"user_tz":-330},"id":"UOQz-p7NdvzE","outputId":"462c9488-f7ad-45ec-c08f-cd8680a0e039","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>Time</th>\n","      <th>Tweet_Text</th>\n","      <th>Type</th>\n","      <th>Media_Type</th>\n","      <th>Hashtags</th>\n","      <th>Tweet_Id</th>\n","      <th>Tweet_Url</th>\n","      <th>twt_favourites_IS_THIS_LIKE_QUESTION_MARK</th>\n","      <th>Retweets</th>\n","      <th>Unnamed: 10</th>\n","      <th>Unnamed: 11</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>16-11-11</td>\n","      <td>15:26:37</td>\n","      <td>Today we express our deepest gratitude to all ...</td>\n","      <td>text</td>\n","      <td>photo</td>\n","      <td>ThankAVet</td>\n","      <td>7.970000e+17</td>\n","      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n","      <td>127213</td>\n","      <td>41112</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>16-11-11</td>\n","      <td>13:33:35</td>\n","      <td>Busy day planned in New York. Will soon be mak...</td>\n","      <td>text</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>7.970000e+17</td>\n","      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n","      <td>141527</td>\n","      <td>28654</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>16-11-11</td>\n","      <td>11:14:20</td>\n","      <td>Love the fact that the small groups of protest...</td>\n","      <td>text</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>7.970000e+17</td>\n","      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n","      <td>183729</td>\n","      <td>50039</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>16-11-11</td>\n","      <td>2:19:44</td>\n","      <td>Just had a very open and successful presidenti...</td>\n","      <td>text</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>7.970000e+17</td>\n","      <td>https://twitter.com/realDonaldTrump/status/796...</td>\n","      <td>214001</td>\n","      <td>67010</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>16-11-11</td>\n","      <td>2:10:46</td>\n","      <td>A fantastic day in D.C. Met with President Oba...</td>\n","      <td>text</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>7.970000e+17</td>\n","      <td>https://twitter.com/realDonaldTrump/status/796...</td>\n","      <td>178499</td>\n","      <td>36688</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Date      Time  ... Unnamed: 10 Unnamed: 11\n","0  16-11-11  15:26:37  ...         NaN         NaN\n","1  16-11-11  13:33:35  ...         NaN         NaN\n","2  16-11-11  11:14:20  ...         NaN         NaN\n","3  16-11-11   2:19:44  ...         NaN         NaN\n","4  16-11-11   2:10:46  ...         NaN         NaN\n","\n","[5 rows x 12 columns]"]},"execution_count":52,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["import pandas as pd\n","filepath = \"/content/drive/MyDrive/ACM NLP Summer School 2021/Day 3 - Language Modeling/Data/Donald-Tweets!.csv\"\n","df = pd.read_csv(filepath)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a15d9d1a754ca357ae79e8390069b08b05320458","id":"pp16Z8-vdvzE","trusted":true},"outputs":[],"source":["trump_corpus = list(df['Tweet_Text'].apply(word_tokenize))"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8724b8724cc52b685205047bd1fda649545a6a24","id":"_FMrYn_ZdvzE","trusted":true},"outputs":[],"source":["# Preprocess the tokenized text for 3-grams language modelling\n","n = 3\n","train_data, padded_sents = padded_everygram_pipeline(n, trump_corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c5010b607062f6f2dd0ccf011bb9004e1e3dcee7","id":"LebXx8gldvzF","trusted":true},"outputs":[],"source":["from nltk.lm import MLE\n","trump_model = MLE(n) # Lets train a 3-grams model, previously we set n=3\n","trump_model.fit(train_data, padded_sents)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8036f1ae5e0e1438d1d2981026766cdacd4479aa","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1625648651471,"user":{"displayName":"Prashant Kodali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLR7kqr_lvUgRRZBGdR7UZo6oiaBC-TEd7nXt_Xw=s64","userId":"13858115222436552549"},"user_tz":-330},"id":"tgpb7Zm-dvzF","outputId":"738f5eb4-40d5-4bdd-ff37-e5f51c527c62","trusted":true},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'do so many people on television. Just another desperate move by the media pile on against me in Rome ,'"]},"execution_count":56,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["generate_sent(trump_model, num_words=20, random_seed=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"defe4e4899cb14300710eb0c7786a6f2e894455d","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":710,"status":"ok","timestamp":1625648660330,"user":{"displayName":"Prashant Kodali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLR7kqr_lvUgRRZBGdR7UZo6oiaBC-TEd7nXt_Xw=s64","userId":"13858115222436552549"},"user_tz":-330},"id":"zLp3Lz3odvzF","outputId":"a437ebb8-8603-4fdc-b71b-a9808d3355ec","trusted":true},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'pretty sad situation. Go Jeb! You made winning MAJORS'"]},"execution_count":57,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["generate_sent(trump_model, num_words=10, random_seed=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"eba7934efa5a5e6b818e951c5daec9e9f621ca26","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1625648660331,"user":{"displayName":"Prashant Kodali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLR7kqr_lvUgRRZBGdR7UZo6oiaBC-TEd7nXt_Xw=s64","userId":"13858115222436552549"},"user_tz":-330},"id":"dscU34Y5dvzF","outputId":"bd60a9be-7b79-4eae-e59e-39e7d1fb94a6","trusted":true},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'and many other subjects! Bad times for divided USA! +Israel2 \"'"]},"execution_count":58,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["generate_sent(trump_model, num_words=50, random_seed=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"3ec48d8d44b2f57b16249e12b845f43f52faadca","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1625648661934,"user":{"displayName":"Prashant Kodali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLR7kqr_lvUgRRZBGdR7UZo6oiaBC-TEd7nXt_Xw=s64","userId":"13858115222436552549"},"user_tz":-330},"id":"FptPUALSdvzG","outputId":"4f4c2617-f158-4f51-8da4-850a1553710b","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["with you sir! Your friend and supporter tonight at Trump Winery Beautiful fall foliage\n"]}],"source":["print(generate_sent(trump_model, num_words=100, random_seed=52))"]},{"cell_type":"markdown","metadata":{"id":"ZjtCOrS0GIp3"},"source":["# **N-Gram Language Modelling (LM) without Smoothening Without Using Library**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pidITfz-xGEN"},"outputs":[],"source":["# importing required libraries\n","import string\n","from typing import List"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N96OVYnGxK_U"},"outputs":[],"source":["# the function of this method is to perform tokenization, ideally we would use some smart text tokenization methods discussed earlier, but for simplicity use this one\n","# it taken a sentence (one parameter) as input and returns a list of tokens present in the sentence \n","\n","def tokenize(text: str) -> List[str]:\n","    for punct in string.punctuation:\n","        text = text.replace(punct, ' '+punct+' ')\n","    t = text.split()\n","    return t\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xULD77YVxd__"},"outputs":[],"source":["# this function takes in the n value of ngram and the list of tokens present in a sentence and returns list of ngrams of tuple form: ((previous wordS!), target word) with sutaible padding\n","\n","def get_ngrams(n: int, tokens: list) -> list:\n","    # tokens.append('<END>')\n","    tokens = (n-1)*['<START>']+tokens\n","    l = [(tuple([tokens[i-p-1] for p in reversed(range(n-1))]), tokens[i]) for i in range(n-1, len(tokens))]\n","    return l\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGD4vZR1GQv9"},"outputs":[],"source":["# Variables: context and ngram : dictionaries  : for storing the counts and context of grams   \n","class NgramModel(object):\n","\n","  #constructor\n","  def __init__(self, n):\n","    self.n = n\n","    self.context = {}     # dictionary that keeps list of candidate words given context\n","    self.ngram_counter = {}     # keeps track of how many times ngram has appeared in the text before\n","\n","  #takes a sentence as input and hence iterates through the sentence and updates counts in dictionaries\n","  def update(self, sentence: str) -> None:\n","    n = self.n\n","    ngrams = get_ngrams(n, tokenize(sentence))\n","    for ngram in ngrams:\n","      if ngram in self.ngram_counter:\n","        self.ngram_counter[ngram] += 1.0\n","      else:\n","          self.ngram_counter[ngram] = 1.0\n","\n","      prev_words, target_word = ngram\n","      if prev_words in self.context:\n","        self.context[prev_words].append(target_word)\n","      else:\n","        self.context[prev_words] = [target_word]\n","  \n","  #Calculates probability of a candidate token to be generated given a context, it returns conditional probability\n","  def prob(self, context, token):\n","    try:\n","      count_of_token = self.ngram_counter[(context, token)]\n","      count_of_context = float(len(self.context[context]))\n","      result = count_of_token / count_of_context\n","\n","    except KeyError:\n","      result = 0.0\n","\n","    return result\n","\n","  #Given a context we \"semi-randomly\" select the next word to append in a sequence\n","  def next_token_selection(self, context):\n","    r = random.random()\n","    map_to_probs = {}\n","    token_of_interest = self.context[context]\n","    for token in token_of_interest:\n","      map_to_probs[token] = self.prob(context, token)\n","\n","    summ = 0\n","    for token in sorted(map_to_probs):\n","      summ += map_to_probs[token]\n","      if summ > r:\n","        return token\n","\n","  #takes no of words to be produced as input and hence generates sentence starting with bunch of n-1 padded <start> tokens\n","  def generate_text(self, token_count: int):\n","    n = self.n\n","    context_queue = (n - 1) * ['<START>']\n","    result = []\n","    for _ in range(token_count):\n","      obj = self.next_token_selection(tuple(context_queue))\n","      result.append(obj)\n","      if n > 1:\n","        context_queue.pop(0)\n","        if obj == '.':\n","          context_queue = (n - 1) * ['<START>']\n","        else:\n","          context_queue.append(obj)\n","    return ' '.join(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dlh4pz6Pzsus"},"outputs":[],"source":["def create_ngram_model(n, path):\n","  m = NgramModel(n)\n","  with open(path, 'r') as f:\n","    text = f.read()\n","    text = text.split('.')\n","    for sentence in text:\n","      # add back the fullstop\n","      sentence += '.'\n","      m.update(sentence)\n","  return m"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vY2IQ5ZkGaZ9"},"outputs":[],"source":["m = create_ngram_model(6, \"/content/drive/MyDrive/001 My Skills/002 CS Engineering   Automated Math (BPHC)/004 Data Science (DS)   Artificial Intelligence (AI)/005 Textual Data (Unstructured Data) (Sequential Data)/001 Mono Lingual Language /001 English/001 Language Modelling (Predicting Words Phrases Sentences)/001 Next Word Phrase Sentence Prediction/001 Probability Based Algorithms/Frankenstein.txt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1625933807300,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"},"user_tz":-330},"id":"UXe9IHiR-pNx","outputId":"b0ecff24-fd9d-45ba-c3ae-3bd0b72bb54d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generated text:\n","\n","What could I do ? He meant to please , and he tormented me . Suddenly , as I gazed\n"]}],"source":["print(\"Generated text:\\n\")\n","print(m.generate_text(20)) #that is generate next 20 words for the above text document"]},{"cell_type":"markdown","metadata":{"id":"zm-LF7soZAFG"},"source":["**Excercise** \n","\n","1.   Modify NgramModel.prob() to implement any smoothing technique.\n","2.   Modify NgramModel.next_token_selection() to return the token with maxiumum count/highest likelihood."]}],"metadata":{"colab":{"collapsed_sections":["VvL-FbmnFq2W","34FRx8gnelah","v3m4z0duBjYF","-0ujSJbydvyz","uWPwuldadvy2","3HO4aUhldvy5","l0ooKFlEdvy_","nQepoZVXdvzD","0A4tb2gGdvzE","ZjtCOrS0GIp3"],"name":"Language Modeling.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":0}
