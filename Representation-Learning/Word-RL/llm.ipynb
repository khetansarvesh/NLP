{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["device = \"mps\""],"metadata":{"id":"KhtcMeF9qvhh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset Building"],"metadata":{"id":"B5fLdbZGqBdo"}},{"cell_type":"code","source":["dataset = [\n","    \"dont forget to like share and subscribe\",\n","    \"dont forget machine learning is fun\",\n","    \"machine learning is fun and awesome\",\n","    \"if you like machine learning i like you\",\n","    \"i like you more than machine learning\"\n","]\n","\n","vocab = set()\n","special_tokens = [\"<pad>\", \"<start>\", \"<end>\"]\n","for sentence in dataset:\n","    vocab.update(sentence.split())\n","vocab = special_tokens + list(vocab)\n","\n","vocab_to_index = {word: index for index, word in enumerate(vocab)}"],"metadata":{"id":"jHkC6dgUqBBG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenizing Dataset"],"metadata":{"id":"ulnxaxWpqDx0"}},{"cell_type":"code","source":["def encode(sentence: str):\n","    return [vocab_to_index[word] for word in sentence.split()]\n","\n","def encode_batch(sentences: list[str]):\n","    encoded_sentences = [\n","        [vocab_to_index[\"<start>\"]] + encode(sentence) + [vocab_to_index[\"<end>\"]]\n","        for sentence in sentences\n","    ]\n","    max_length = max([len(encoded_sentence) for encoded_sentence in encoded_sentences])\n","    encoded_sentences = [\n","        encoded_sentence + [vocab_to_index[\"<pad>\"]] * (max_length - len(encoded_sentence))\n","        for encoded_sentence in encoded_sentences\n","    ]\n","    return encoded_sentences\n","\n","def decode(tokens: list[int]):\n","    return \" \".join([vocab[token] for token in tokens])\n","\n","tokenized_dataset = encode_batch(dataset)\n","tokenized_dataset = torch.tensor(tokenized_dataset)\n","\n","input_tokens = tokenized_dataset[:, :-1]\n","input_tokens = input_tokens.to(device)\n","\n","target_tokens = tokenized_dataset[:, 1:]\n","target_tokens = target_tokens.to(device)"],"metadata":{"id":"r5PJuSX4p_MF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Modelling"],"metadata":{"id":"bARbHB_QqmWP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9p_pf4PGpxNr"},"outputs":[],"source":["class SinusoidalPositionEncoding(nn.Module):\n","    def __init__(self, embed_size: int, max_seq_length: int):\n","        super().__init__()\n","        position = torch.arange(max_seq_length).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, embed_size, 2) * (-math.log(10000.0) / embed_size))\n","        pe = torch.zeros(20, embed_size)\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('positional_embedding', pe)\n","\n","    def forward(self, x: torch.Tensor):\n","        return x + self.positional_embedding[:x.size(1), :]\n","\n","class CausalLanguageModel(nn.Module):\n","    def __init__(self, embed_size: int, vocab_size: int, num_layers: int):\n","        super().__init__()\n","        self.embedding_layer = nn.Parameter(torch.randn(vocab_size, embed_size))\n","        transformer_decoder = nn.TransformerDecoderLayer(d_model=embed_size, nhead=12)\n","        transformer_decoder_stack = nn.TransformerDecoder(\n","                                                          decoder_layer=transformer_decoder,\n","                                                          num_layers=num_layers\n","                                                          )\n","        self.positional_encoding = SinusoidalPositionEncoding(embed_size, max_seq_length=20)\n","\n","    def forward(self, x: torch.Tensor, return_attention_scores: bool = False):\n","        x = torch.nn.functional.embedding(x, self.embedding_layer)\n","        x = self.positional_encoding(x)\n","        x, attention_scores = self.transformer(x)\n","        logits = torch.matmul(x, self.embedding_layer.T)\n","        if return_attention_scores:\n","            return logits, attention_scores\n","        return logits"]},{"cell_type":"code","source":["causal_language_model = CausalLanguageModel(embed_size=6, vocab_size=len(vocab), num_layers=2).to(device)"],"metadata":{"id":"ji7-GZliqZzt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"7JVEhA8Tq2F3"}},{"cell_type":"code","source":["optimizer = torch.optim.Adam(causal_language_model.parameters(), lr=2e-3)\n","\n","for i in range(600):\n","    logits = causal_language_model(input_tokens)\n","    loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), target_tokens.view(-1)) #since it is a classification prob we are using cross entropy loss\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","    if i % 10 == 0:\n","        print(f\"Epoch {i}, Loss: {loss.item()}\")\n","        pred = logits.argmax(dim=-1)"],"metadata":{"id":"KeLuaYasqYH4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference"],"metadata":{"id":"xcs2Y6b3r18i"}},{"cell_type":"code","source":["def generation(prefix, max_length=10):\n","    input_tokens = torch.tensor([vocab_to_index[\"<start>\"]] + encode(prefix)).to(device).unsqueeze(0)\n","    for _ in range(max_length):\n","        with torch.no_grad():\n","            logits = causal_language_model(input_tokens)\n","            last_token_logits = logits[0, -1].argmax(dim=-1, keepdim=True)\n","            print(decode(last_token_logits.tolist())[0][0])\n","            input_tokens = torch.cat([input_tokens, last_token_logits], dim=1)\n","            if input_tokens[0][-1] == vocab_to_index[\"<end>\"]:\n","                break\n","    return decode(input_tokens[0].tolist())"],"metadata":{"id":"tyQIhVIUr2sT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generation(\"i like\")"],"metadata":{"id":"_U7OZzFIr4oo"},"execution_count":null,"outputs":[]}]}