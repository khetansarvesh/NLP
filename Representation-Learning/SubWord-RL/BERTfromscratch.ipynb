{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/NLP/blob/main/Representation-Learning/SubWord-RL/BERTfromscratch.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WHYK8ZNG2Oe"},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mziyF7JFG3mx"},"outputs":[],"source":["class BERTEmbedding(nn.Module):\n","    def __init__(self,\n","                 vocab_size,\n","                 n_segments,\n","                 max_len,\n","                 embed_dim,\n","                 dropout):\n","        super().__init__()\n","        self.tok_embed = nn.Embedding(vocab_size, embed_dim)\n","        self.seg_embed = nn.Embedding(n_segments, embed_dim)\n","        self.pos_embed = nn.Embedding(max_len, embed_dim)\n","\n","        self.drop = nn.Dropout(dropout)\n","        self.pos_inp = torch.tensor([i for i in range(max_len)],)\n","\n","    def forward(self, seq, seg):\n","        embed_val = self.tok_embed(seq) + self.seg_embed(seg) + self.pos_embed(self.pos_inp)\n","        embed_val = self.drop(embed_val)\n","        return embed_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fTZbEwoRG0tj"},"outputs":[],"source":["class BERT(nn.Module):\n","    def __init__(self,\n","                 vocab_size,\n","                 n_segments,\n","                 max_len,\n","                 embed_dim,\n","                 n_layers,\n","                 attn_heads,\n","                 dropout):\n","        super().__init__()\n","        self.embedding = BERTEmbedding(vocab_size, n_segments, max_len, embed_dim, dropout)\n","        self.encoder_layer = nn.TransformerEncoderLayer(embed_dim, attn_heads, embed_dim*4)\n","        self.encoder_block = nn.TransformerEncoder(self.encoder_layer, n_layers)\n","\n","    def forward(self, seq, seg):\n","        out = self.embedding(seq, seg)\n","        out = self.encoder_block(out)\n","        return out\n","\n","\n","if __name__ == \"__main__\":\n","    VOCAB_SIZE = 30000\n","    N_SEGMENTS = 3\n","    MAX_LEN = 512\n","    EMBED_DIM = 768\n","    N_LAYERS = 12\n","    ATTN_HEADS = 12\n","    DROPOUT = 0.1\n","\n","    sample_seq = torch.randint(high=VOCAB_SIZE, size=[MAX_LEN,])\n","    sample_seg = torch.randint(high=N_SEGMENTS, size=[MAX_LEN,])\n","\n","    embedding = BERTEmbedding(VOCAB_SIZE, N_SEGMENTS, MAX_LEN, EMBED_DIM, DROPOUT)\n","    embedding_tensor = embedding(sample_seq, sample_seg)\n","    print(embedding_tensor.size())\n","\n","    bert = BERT(VOCAB_SIZE, N_SEGMENTS, MAX_LEN, EMBED_DIM, N_LAYERS, ATTN_HEADS, DROPOUT)\n","    out = bert(sample_seq, sample_seg)\n","    print(out.size())"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOG9gqw3o4zx6ONb13X+19E","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
