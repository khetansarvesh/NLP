{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ff60a70d",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/NLP/blob/main/dpo/dpo_from_scratch.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "mgkSpfnHN9qK",
      "metadata": {
        "id": "mgkSpfnHN9qK"
      },
      "outputs": [],
      "source": [
        "# # Install required packages\n",
        "# !pip install \"transformers==4.31.0\" \"trl==0.4.5\" \"datasets>=4.1.0\" \"torch>=2.8.0\"\n",
        "# !pip install \"accelerate>=1.10.1\" \"peft>=0.17.0\" \"trackio\"\n",
        "# !pip install trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "BhszUKRFN_m3",
      "metadata": {
        "id": "BhszUKRFN_m3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "import json\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Check available device\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AMfJh19cSYJb",
      "metadata": {
        "id": "AMfJh19cSYJb"
      },
      "source": [
        "# **Preference Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "Ng3sbJryN_iN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786,
          "referenced_widgets": [
            "4690496f7faa4168ad9b386db96240ba",
            "104a58dbff464daa8c7587fc0965bb1a",
            "53cb596517784805ac8976b16040fbcc",
            "d9e49bf46931499bb84579245908e11c",
            "e10f9db9aba645eebda3417ecf882f27",
            "334d74000b47433e99b03b4ca40c545c",
            "217c1a2cf9a14f90ba401b6c9f9ce140",
            "5f72016549af44aba93e264b15678b52",
            "bf24ab838674436eb1b28dd132044b81",
            "ab90770ff1fa489c8ad80e63cffa3e46",
            "c7e4d671211740ffad9def6fbe58fca5",
            "40e9773a1ed14164b61d401fcc9188ac",
            "538d437960a743aab0d3a24b9745ace4",
            "19de1f34e77e4306ac93d5b2e78960a9",
            "36ac371dd8f34db793032922514e31f7",
            "2c3f9daf9c384428a2046aa6452c5683",
            "1440ac08d36e4dfea75d1acd121b6b64",
            "f291318736bc482ca7b1e3e1e8af6d1a",
            "88388905e0ea4acb81bac8a4e6c3ef91",
            "d8c8f2563a4043f7aa0245788e1d109b",
            "886f50e24ac447cea924b105412380c7",
            "e420cec10393497caae7b5b713888512",
            "22e07fa21fd34370b460c13098215237",
            "45c32cad7f994dc7bafdb2e02467afda",
            "d07006dd389c43d285ded101f0613919",
            "a0cad0f259854b2a96246fa85f633b12",
            "d53c77e959aa4eb0afcbf167d8cd3c2e",
            "42ee68f451f14399a45e68fb7174978c",
            "9bf9741d27f94118b76d359615c13da2",
            "3f8f676d070645f4ba168a2f56b4a3eb",
            "59c3c5ae3b0a44478cbb9d90c5e06c23",
            "4bcf45f6c36948e395d14721257c78e8",
            "81531c8720b74050972f96dfb4dfaac5",
            "048c47eff6944ba388636d9ac37d37c6",
            "4110793f5d5d4e5494be1f9b32a8ac8b",
            "26132dcbbf44414097255c2305fc9081",
            "e2121a657e404669b48d291ce3bf0acc",
            "a0801fc9274b430ba97be556158aa935",
            "e619048c02184bdfbb6559e40d6b08bb",
            "8ef2ae09656c40e78b196aa515b26445",
            "a97d494852db4d7e928d7435423f8b6f",
            "8d2efe6148b4478cbed7116b05915921",
            "91558c5e5c9843b1843d3641061b5470",
            "e10f42f87cd4475f848f4b367ab0ba65",
            "fbc18b8fe6d343129c7c597fb22732c1",
            "d45998fe727b44c7b356ac1367f69d78",
            "b0799f87f42c4ad585d6b8074e79e7cf",
            "353af4f260314c3cacf9656048320480",
            "e378fb8c109144f9a9b99d9ef3317e70",
            "3f6f0cd3c3f04baa8cff741b20cb69be",
            "d1f47c8f0f4a4d4983852bcd83cadee2",
            "d02aafc15f14485f81e4179314a8160e",
            "4312dd6354b14cbc81b495e34d157fab",
            "097f2412cebb4bf1b642ad66502c214b",
            "f508549e8fda4a32b9f37e46fec0b6b6",
            "8efb0cdad0a64efea211672d7494dafc",
            "7b77c0863cc64e8c877983de975409dc",
            "4063973e73ce4197851f849ff02a0668",
            "7af70c8c625948d1800beedb6d69e528",
            "9ec53acd44604138983d777e854efbde",
            "f73fdbc66e16418d9932438d137c5c2a",
            "4a6d78a03a314aeb84e5f3950f997ebc",
            "98d8ad21fd8141d1af70ba483720b03f",
            "8562ae4d323c4bb1a69498242c292a54",
            "e5112765697c4882835a7799b5f61c53",
            "cc541c4109ee4518a6a147605060626f",
            "9e0e84d4019840dab220f931e777ee5a",
            "ecd4810af8be4e89b05d1dfd60b8ae9b",
            "2945c251f7f34658896b7b3a40db31c3",
            "5ef456fcbce94a40a1d10cb3da46100b",
            "d2092d3a04394e66991201dabd80b2f7",
            "29b5f9650fc043daa857c9aeafeddc2b",
            "fec5b06f962c4e8089423a64c8618661",
            "d056e13a795e471ea1858ff0499e7fac",
            "ab38dac52c974b5799f3fd1b815a257e",
            "97c9f4805f4e43fba444fac4325373e9",
            "a18f0155190d4fa79028dce417d39845",
            "855d6ce957144da391dd1f408668cc4d",
            "11e88fff3219455ca06fc1b340dfdd38",
            "434de9ad5b0245afabba0839ef8bf44a",
            "979f43eae07145e9b2ed33ab9ee65947",
            "38975dd024084a24ab506ef14820befa",
            "3032a6222da2481391ce0a53a1f8530c",
            "6deb3b59c5db44cc8adbce0008b4d97f",
            "ab62e094b631425cbf0265954ccb0772",
            "60e8fd39eeb649228830b45abe62679d",
            "0f9722557979438190484def2676d8d4",
            "210afd22fd5342cf98979508280acdb7",
            "75d7c1f26b1d4b888500d9c736e0ca6c",
            "d2411e23e68141e7b5e477671939922f",
            "a8fe7d17cf6e478783aed42ef5c4e6d3",
            "f80110f513474bddbd5104d5b1764a78",
            "567fa9c4664f427fad91775ff97f26d8",
            "69a88709b60b484bb2fa7dd72ad3bf0d",
            "10202d1bdf6d400f9077d39873563118",
            "59cafe78458c4fc79575dd1f0bf4f2ea",
            "1bfc60a543674d449b91b9772d6d2bbb",
            "947e3bae94a44dfd9b263413aaac3a43",
            "6cddf439eec84ee0b4fcfd9373182d4f",
            "bcb4217f4aad4773a1511869d34ec024",
            "bc9ea9d647cf4c4886e8841cfac2d7f0",
            "34617f59a43f4e3aaad8f92eec0cbbde",
            "e63d81252bc54f05b05eac70fb1e66eb",
            "e40f3ea64d5645bab266cce398ec5769",
            "99df6a3903c64c629dba1a5e9a5b2c60",
            "f2c2e09be96542e58289d99947fc870c",
            "e14a63e9bf084c59b900c09b846702cd",
            "b724d5b61e574a438249151a5562bda3",
            "4a938400c4054acfa5279f53ffe306dd",
            "0344b121b82e436b92b45805bb33ef42",
            "604a2a9763a148ff9f53e97a200b29b8",
            "884e415ebc224fba93ce3ff17e6d4861",
            "bc1abe0a9ec24c15b30d956b2cf35586",
            "0111655b62f64ceaaa83b1e97f543bc2",
            "878df9a5e7374c0f9e0e25e86aa58d54",
            "6f940153531041aa80f3829d432a41b9",
            "2f2f9545c8d34c3d8af8316af6b629ed",
            "3451328913e64ee7bccebcdc3ab3f393",
            "5097398a576643f3a78d71ad4b4c6871",
            "6e72d77b6fc840a499162a513149cb1b",
            "37112405ae7041808e2ca89761d44e27"
          ]
        },
        "id": "Ng3sbJryN_iN",
        "outputId": "c79aff1b-3c34-4faa-b19a-fb76b0a91924"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4690496f7faa4168ad9b386db96240ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40e9773a1ed14164b61d401fcc9188ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "harmless-base/train.jsonl.gz:   0%|          | 0.00/13.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22e07fa21fd34370b460c13098215237",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "helpful-base/train.jsonl.gz:   0%|          | 0.00/16.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "048c47eff6944ba388636d9ac37d37c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "helpful-online/train.jsonl.gz:   0%|          | 0.00/20.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbc18b8fe6d343129c7c597fb22732c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "helpful-rejection-sampled/train.jsonl.gz:   0%|          | 0.00/25.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8efb0cdad0a64efea211672d7494dafc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "harmless-base/test.jsonl.gz:   0%|          | 0.00/743k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e0e84d4019840dab220f931e777ee5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "helpful-base/test.jsonl.gz:   0%|          | 0.00/875k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "855d6ce957144da391dd1f408668cc4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "helpful-online/test.jsonl.gz:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75d7c1f26b1d4b888500d9c736e0ca6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "helpful-rejection-sampled/test.jsonl.gz:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcb4217f4aad4773a1511869d34ec024",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/160800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "604a2a9763a148ff9f53e97a200b29b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/8552 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset size: 160800\n",
            "Dataset features: dict_keys(['chosen', 'rejected'])\n",
            "\n",
            "Chosen (Preferred): \n",
            "\n",
            "Human: What are some cuss words in english?\n",
            "\n",
            "Assistant: Here’s an incomplete list.\n",
            "\n",
            "Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cu...\n",
            "\n",
            "Rejected (Non-preferred): \n",
            "\n",
            "Human: What are some cuss words in english?\n",
            "\n",
            "Assistant: Here’s an incomplete list.\n",
            "\n",
            "Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cu...\n"
          ]
        }
      ],
      "source": [
        "# Load a preference dataset to understand the format\n",
        "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\")\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(\"Dataset features:\", dataset.features.keys())\n",
        "\n",
        "# Examine a preference pair\n",
        "sample = dataset[0]\n",
        "print(f\"\\nChosen (Preferred): {sample['chosen'][:200]}...\")\n",
        "print(f\"\\nRejected (Non-preferred): {sample['rejected'][:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nABYHsGpTPh7",
      "metadata": {
        "id": "nABYHsGpTPh7"
      },
      "outputs": [],
      "source": [
        "# Load a small subset for local testing\n",
        "small_dataset = dataset.select(range(1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jxIApJ1kTUne",
      "metadata": {
        "id": "jxIApJ1kTUne"
      },
      "source": [
        "# **Load Pretrained with SFT Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WGXnoe4hN_f2",
      "metadata": {
        "id": "WGXnoe4hN_f2"
      },
      "outputs": [],
      "source": [
        "model_name = \"HuggingFaceTB/SmolLM3-3B-Instruct\" #\"HuggingFaceTB/SmolLM3-3B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oxEOE2h4TlRF",
      "metadata": {
        "id": "oxEOE2h4TlRF"
      },
      "source": [
        "# **DPO Training : Using Hugging Face's TRL Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eGR3ZJUpTn0H",
      "metadata": {
        "id": "eGR3ZJUpTn0H"
      },
      "outputs": [],
      "source": [
        "training_args = DPOConfig(\n",
        "    beta=0.1,                           # Preference optimization strength\n",
        "    learning_rate=5e-7,                 # Lower than SFT\n",
        "    per_device_train_batch_size=1,      # Small batch for local testing\n",
        "    gradient_accumulation_steps=4,      # Effective batch size = 4\n",
        "    max_steps=50,                       # Very short for testing\n",
        "    logging_steps=10,\n",
        "    output_dir=\"./local_dpo_test\",\n",
        "    report_to=\"trackio\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q4utW5xdUK-N",
      "metadata": {
        "id": "Q4utW5xdUK-N"
      },
      "outputs": [],
      "source": [
        "training_args = DPOConfig(\n",
        "    # Core DPO parameters\n",
        "    beta=0.1,                           # Preference optimization strength\n",
        "    max_prompt_length=512,              # Maximum prompt length\n",
        "    max_length=1024,                    # Maximum total sequence length\n",
        "\n",
        "    # Training configuration\n",
        "    learning_rate=5e-7,                 # Lower than SFT for stability\n",
        "    per_device_train_batch_size=1,      # batch size\n",
        "    gradient_accumulation_steps=4,      # Effective batch size = 4\n",
        "    max_steps=1000,                     # Sufficient for good alignment\n",
        "\n",
        "    # Optimization\n",
        "    warmup_steps=100,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    gradient_checkpointing=True,        # Memory efficiency\n",
        "    bf16=True,                          # Mixed precision\n",
        "\n",
        "    # Logging and saving\n",
        "    logging_steps=50,\n",
        "    save_steps=250,\n",
        "    output_dir=\"./smollm3-dpo-aligned\",\n",
        "\n",
        "    # Hub integration\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"your-username/smollm3-dpo-aligned\",  # Change this!\n",
        "    report_to=\"trackio\",\n",
        "\n",
        "    # Remove unused columns for cleaner training\n",
        "    remove_unused_columns=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Sbk1UfLyT3I4",
      "metadata": {
        "id": "Sbk1UfLyT3I4"
      },
      "outputs": [],
      "source": [
        "# Create trainer (but don't train yet - save resources for HF Jobs)\n",
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_dataset,\n",
        "    processing_class=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VpeF29LiT4GE",
      "metadata": {
        "id": "VpeF29LiT4GE"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l9aA1UpXK-gU",
      "metadata": {
        "id": "l9aA1UpXK-gU"
      },
      "source": [
        "# **DPO Training : Implementing from scratch**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iBLwYrp-N9V9",
      "metadata": {
        "id": "iBLwYrp-N9V9"
      },
      "source": [
        "### *Implementing DPO Loss Equation*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38CsrrwJIZiV",
      "metadata": {
        "id": "38CsrrwJIZiV"
      },
      "outputs": [],
      "source": [
        "def compute_dpo_loss( model_chosen_logprobs, model_rejected_logprobs, reference_chosen_logprobs, reference_rejected_logprobs, beta=0.1):\n",
        "\n",
        "    model_logratios = model_chosen_logprobs - model_rejected_logprobs\n",
        "    reference_logratios = reference_chosen_logprobs - reference_rejected_logprobs\n",
        "    logits = model_logratios - reference_logratios\n",
        "    dpo_losses = -F.logsigmoid(beta * logits)\n",
        "    dpo_loss = dpo_losses.mean() # .mean() to average over the samples in the batch\n",
        "\n",
        "    # Optional values to track progress during training\n",
        "    chosen_rewards = (model_chosen_logprobs - reference_chosen_logprobs).detach()\n",
        "    rejected_rewards = (model_rejected_logprobs - reference_rejected_logprobs).detach()\n",
        "\n",
        "    return dpo_loss, chosen_rewards.mean(), rejected_rewards.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dmd-_-Y7OC92",
      "metadata": {
        "id": "Dmd-_-Y7OC92"
      },
      "source": [
        "### *Implementing log logits calculator*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8UE6uJu7OHoF",
      "metadata": {
        "id": "8UE6uJu7OHoF"
      },
      "source": [
        "Now above dpo loss function expects us to input log probabilites and hence here we will see how to calculate log probabilites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e6507b-d2e2-4469-86b9-f057b08b5df9",
      "metadata": {
        "id": "71e6507b-d2e2-4469-86b9-f057b08b5df9"
      },
      "outputs": [],
      "source": [
        "def compute_logprobs(logits, # predicted output from llm\n",
        "                     labels, # real outputs\n",
        "                     selection_mask=None):\n",
        "    \"\"\"\n",
        "    Compute log probabilities.\n",
        "\n",
        "    Args:\n",
        "      logits: Tensor of shape (batch_size, num_tokens, vocab_size)\n",
        "      labels: Tensor of shape (batch_size, num_tokens)\n",
        "      selection_mask: Tensor for shape (batch_size, num_tokens)\n",
        "\n",
        "    Returns:\n",
        "      mean_log_prob: Mean log probability excluding padding tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    # Labels are the inputs shifted by one\n",
        "    labels = labels[:, 1:].clone()\n",
        "\n",
        "    # Truncate logits to match the labels num_tokens\n",
        "    logits = logits[:, :-1, :]\n",
        "\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    # Gather the log probabilities for the actual labels\n",
        "    selected_log_probs = torch.gather(\n",
        "        input=log_probs,\n",
        "        dim=-1,\n",
        "        index=labels.unsqueeze(-1)\n",
        "    ).squeeze(-1)\n",
        "\n",
        "    if selection_mask is not None:\n",
        "        mask = selection_mask[:, 1:].clone()\n",
        "\n",
        "        # Apply the mask to filter out padding tokens\n",
        "        selected_log_probs = selected_log_probs * mask\n",
        "\n",
        "        # Calculate the average log probability excluding padding tokens\n",
        "        # This averages over the tokens, so the shape is (batch_size,)\n",
        "        avg_log_prob = selected_log_probs.sum(-1) / mask.sum(-1)\n",
        "\n",
        "        return avg_log_prob\n",
        "\n",
        "    else:\n",
        "        return selected_log_probs.mean(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf6a71ac-3fcc-44a4-befc-1c56bbd378d7",
      "metadata": {
        "id": "cf6a71ac-3fcc-44a4-befc-1c56bbd378d7"
      },
      "source": [
        "- Note that this function above might look a bit intimidating at first due to the `torch.gather` function, but it's pretty similar to what happens under the hood in PyTorch's `cross_entropy` function\n",
        "- For example, consider the following example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59873470-464d-4be2-860f-cbb7ac2d80ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59873470-464d-4be2-860f-cbb7ac2d80ba",
        "outputId": "8f7b47d4-73fe-4605-c17d-ad6cfd909a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4185) tensor(1.4185)\n"
          ]
        }
      ],
      "source": [
        "# Sample data\n",
        "logits = torch.tensor(\n",
        "    [[2.0, 1.0, 0.1],\n",
        "     [0.5, 2.5, 0.3]])  # Shape: (2, 3)\n",
        "targets = torch.tensor([0, 2])  # Shape: (2,)\n",
        "\n",
        "\n",
        "# Manual loss using torch.gather\n",
        "log_softmax_logits = F.log_softmax(logits, dim=1)  # Shape: (2, 3)\n",
        "selected_log_probs = torch.gather(\n",
        "    input=log_softmax_logits,\n",
        "    dim=1,\n",
        "    index=targets.unsqueeze(1), # Shape 2, 1\n",
        ").squeeze(1)  # Shape: (2,)\n",
        "manual_loss = -selected_log_probs.mean()  # Averaging over the batch\n",
        "\n",
        "\n",
        "# PyTorch loss\n",
        "cross_entropy_loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "print(manual_loss, cross_entropy_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f86d7add-f7ff-4a87-9193-7878c42bf0e7",
      "metadata": {
        "id": "f86d7add-f7ff-4a87-9193-7878c42bf0e7"
      },
      "source": [
        "- So, above, we can see that the two implementations are equivalent, but let's narrow down a bit further to the `torch.gather` mechanics\n",
        "- Consider the following two tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "508db6ba-cc40-479f-a996-2250cf862388",
      "metadata": {
        "id": "508db6ba-cc40-479f-a996-2250cf862388"
      },
      "outputs": [],
      "source": [
        "t = torch.tensor(\n",
        "  [[1., 2.,],\n",
        "   [3., 4.]]\n",
        ")\n",
        "\n",
        "m = torch.tensor(\n",
        "  [[1, 1],\n",
        "   [0, 1]]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "821cbf45-8fbb-47b7-bae8-6c3271e36979",
      "metadata": {
        "id": "821cbf45-8fbb-47b7-bae8-6c3271e36979"
      },
      "source": [
        "- Above, `t` is a tensor we want to select from, and `m` is a mask to specify how we want to select\n",
        " - For instance, since `m` contains `[1, 1]` n the first row, it will select two times the value of `t` in index position `1`, which is the value 2.\n",
        " - The second row of `m`, `[0, 1]`, selects index positions 0 and 1 in the second row or `t`, which are `3.` and `4.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fdN5q1YPAbM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fdN5q1YPAbM",
        "outputId": "e935e8ad-1519-4c4b-dbff-65adae0a15a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[2., 2.],\n",
              "        [3., 4.]])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.gather(input=t, dim=-1, index=m)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d10eeaf4-f24b-4e79-916a-abedf74fe4a3",
      "metadata": {
        "id": "d10eeaf4-f24b-4e79-916a-abedf74fe4a3"
      },
      "source": [
        "- In other words, `torch.gather` is a selection function\n",
        "- When we computed the loss earlier, we used it to retrieve the log probabilities corresponding to the correct token in the 50,257-token vocabulary\n",
        "- The \"correct\" tokens are the tokens given in the response entry"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5d10a43-ee5b-47ed-9d55-ddd96e66cf0b",
      "metadata": {
        "id": "d5d10a43-ee5b-47ed-9d55-ddd96e66cf0b"
      },
      "source": [
        "- Regarding the `compute_logprobs` function above, we use `torch.gather` here because it gives us a bit more control than `cross_entropy`, but is, in essence, a similar idea\n",
        "- The `selection_mask` we use there is to optionally ignore prompt and padding tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WwknoDABOhgl",
      "metadata": {
        "id": "WwknoDABOhgl"
      },
      "source": [
        "### *DPO Loss Function -- Single Batch*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfa7a4db-eba0-47d8-ad6d-7b5e7676e318",
      "metadata": {
        "id": "dfa7a4db-eba0-47d8-ad6d-7b5e7676e318"
      },
      "outputs": [],
      "source": [
        "def compute_dpo_loss_batch(batch, policy_model, reference_model, beta):\n",
        "\n",
        "    policy_chosen_log_probas = compute_logprobs(\n",
        "        logits=policy_model(batch[\"chosen\"]),\n",
        "        labels=batch[\"chosen\"],\n",
        "        selection_mask=batch[\"chosen_mask\"]\n",
        "    )\n",
        "\n",
        "    policy_rejected_log_probas = compute_logprobs(\n",
        "        logits=policy_model(batch[\"rejected\"]),\n",
        "        labels=batch[\"rejected\"],\n",
        "        selection_mask=batch[\"rejected_mask\"]\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ref_chosen_log_probas = compute_logprobs(\n",
        "            logits=reference_model(batch[\"chosen\"]),\n",
        "            labels=batch[\"chosen\"],\n",
        "            selection_mask=batch[\"chosen_mask\"]\n",
        "        )\n",
        "        ref_rejected_log_probas = compute_logprobs(\n",
        "            logits=reference_model(batch[\"rejected\"]),\n",
        "            labels=batch[\"rejected\"],\n",
        "            selection_mask=batch[\"rejected_mask\"]\n",
        "        )\n",
        "\n",
        "    loss, chosen_rewards, rejected_rewards = compute_dpo_loss(\n",
        "        model_chosen_logprobs=policy_chosen_log_probas,\n",
        "        model_rejected_logprobs=policy_rejected_log_probas,\n",
        "        reference_chosen_logprobs=ref_chosen_log_probas,\n",
        "        reference_rejected_logprobs=ref_rejected_log_probas,\n",
        "        beta=beta\n",
        "    )\n",
        "\n",
        "    return loss, chosen_rewards, rejected_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd74fcc4-4280-41e9-9a22-838e85c84ee4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd74fcc4-4280-41e9-9a22-838e85c84ee4",
        "outputId": "65a70828-7dd2-4f72-ffec-45aeaf8afad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor(0.6931, device='cuda:0'), tensor(0., device='cuda:0'), tensor(0., device='cuda:0'))\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    loss = compute_dpo_loss_batch(batch, policy_model, reference_model, beta=0.1)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dMKSa5SiTPMB",
      "metadata": {
        "id": "dMKSa5SiTPMB"
      },
      "source": [
        "### *DPO Loss Function -- Entire Batch*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z4iui0bDTzuw",
      "metadata": {
        "id": "Z4iui0bDTzuw"
      },
      "source": [
        "Below, we extend this function to work for a specified `num_batches` in a data loader.  Why a specified `num_batches` and not the entire dataloader? That's purely for efficiency reasons (because calculating the loss on the whole dataset each time would slow down the training significantly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "682e9ad5-c5de-4d1b-9e93-3918bf5d5302",
      "metadata": {
        "id": "682e9ad5-c5de-4d1b-9e93-3918bf5d5302"
      },
      "outputs": [],
      "source": [
        "def compute_dpo_loss_loader(data_loader, policy_model, reference_model, beta, num_batches=None):\n",
        "\n",
        "    total_loss, total_chosen_rewards, total_rejected_rewards = 0., 0., 0.\n",
        "\n",
        "    for i, batch in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss, chosen_rewards, rejected_rewards = compute_dpo_loss_batch(\n",
        "                batch=batch,\n",
        "                policy_model=policy_model,\n",
        "                reference_model=reference_model,\n",
        "                beta=beta\n",
        "            )\n",
        "            total_loss += loss.item()\n",
        "            total_chosen_rewards += chosen_rewards.item()\n",
        "            total_rejected_rewards += rejected_rewards.item()\n",
        "\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # calculate average\n",
        "    total_loss /= num_batches\n",
        "    total_chosen_rewards /= num_batches\n",
        "    total_rejected_rewards /= num_batches\n",
        "    return total_loss, total_chosen_rewards, total_rejected_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hhMoNIEpTkM2",
      "metadata": {
        "id": "hhMoNIEpTkM2"
      },
      "source": [
        "### *DPO Loss Function -- Entire Train + Val Batch*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3d214ec-49ba-4bf0-ac80-f90fa0d832e9",
      "metadata": {
        "id": "c3d214ec-49ba-4bf0-ac80-f90fa0d832e9"
      },
      "outputs": [],
      "source": [
        "def evaluate_dpo_loss_loader(policy_model, reference_model, train_loader, val_loader, beta, eval_iter):\n",
        "    \"\"\"Compute the DPO loss for the training and validation dataset\"\"\"\n",
        "\n",
        "    policy_model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss, train_chosen_rewards, train_rejected_rewards = compute_dpo_loss_loader(\n",
        "            data_loader=train_loader,\n",
        "            policy_model=policy_model,\n",
        "            reference_model=reference_model,\n",
        "            beta=beta,\n",
        "            num_batches=eval_iter\n",
        "        )\n",
        "\n",
        "        val_loss, val_chosen_rewards, val_rejected_rewards = compute_dpo_loss_loader(\n",
        "            data_loader=val_loader,\n",
        "            policy_model=policy_model,\n",
        "            reference_model=reference_model,\n",
        "            beta=beta,\n",
        "            num_batches=eval_iter\n",
        "        )\n",
        "\n",
        "    res = {\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_chosen_reward\": train_chosen_rewards,\n",
        "        \"train_rejected_reward\": train_rejected_rewards,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_chosen_reward\": val_chosen_rewards,\n",
        "        \"val_rejected_reward\": val_rejected_rewards\n",
        "    }\n",
        "\n",
        "    policy_model.train()\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb8a8f18-536e-4d83-a0d0-ac518a85f157",
      "metadata": {
        "id": "cb8a8f18-536e-4d83-a0d0-ac518a85f157"
      },
      "source": [
        "### *Training*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b11d63d-3ddc-4070-9b2b-5ca0edb08d0c",
      "metadata": {
        "id": "4b11d63d-3ddc-4070-9b2b-5ca0edb08d0c"
      },
      "source": [
        "Training logic remains same as we saw in pretraining and instruction finetuning, with minor differences:\n",
        " - we swap the cross-entropy loss with our new DPO loss function\n",
        " - we also track the rewards and reward margins, which are commonly used in RLHF and DPO contexts to track the training progress\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f90d9325-77b2-417f-88ff-0a5174889413",
      "metadata": {
        "id": "f90d9325-77b2-417f-88ff-0a5174889413"
      },
      "outputs": [],
      "source": [
        "def train_model_dpo_simple(\n",
        "    policy_model, reference_model, train_loader, val_loader,\n",
        "    optimizer, num_epochs, beta,\n",
        "    eval_freq, eval_iter, start_context, tokenizer\n",
        "):\n",
        "\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    tracking = {\n",
        "        \"train_losses\": [],\n",
        "        \"train_chosen_rewards\": [],\n",
        "        \"train_rejected_rewards\": [],\n",
        "        \"val_losses\": [],\n",
        "        \"val_chosen_rewards\": [],\n",
        "        \"val_rejected_rewards\": [],\n",
        "        \"tokens_seen\": []\n",
        "    }\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        policy_model.train()  # Set model to training mode\n",
        "\n",
        "        for batch in train_loader:\n",
        "\n",
        "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
        "\n",
        "            loss, chosen_rewards, rejected_rewards = compute_dpo_loss_batch(\n",
        "                batch=batch,\n",
        "                policy_model=policy_model,\n",
        "                reference_model=reference_model,\n",
        "                beta=beta\n",
        "            )\n",
        "\n",
        "            loss.backward()  # Calculate loss gradients\n",
        "            optimizer.step()  # Update model weights using loss gradients\n",
        "\n",
        "            tokens_seen += batch[\"chosen\"].numel()\n",
        "            global_step += 1\n",
        "\n",
        "    return tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54b739be-871e-4c97-bf14-ffd2c58e1311",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54b739be-871e-4c97-bf14-ffd2c58e1311",
        "outputId": "d98b08b0-c325-411e-a1a4-05e7403f0345"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 0.692, Val loss 0.693, Train reward margins 0.019, Val reward margins 0.009\n",
            "Ep 1 (Step 000005): Train loss 0.690, Val loss 0.691, Train reward margins 0.070, Val reward margins 0.052\n",
            "Ep 1 (Step 000010): Train loss 0.687, Val loss 0.688, Train reward margins 0.126, Val reward margins 0.108\n",
            "Ep 1 (Step 000015): Train loss 0.676, Val loss 0.685, Train reward margins 0.362, Val reward margins 0.173\n",
            "Ep 1 (Step 000020): Train loss 0.676, Val loss 0.680, Train reward margins 0.351, Val reward margins 0.264\n",
            "Ep 1 (Step 000025): Train loss 0.666, Val loss 0.676, Train reward margins 0.564, Val reward margins 0.359\n",
            "Ep 1 (Step 000030): Train loss 0.672, Val loss 0.672, Train reward margins 0.456, Val reward margins 0.441\n",
            "Ep 1 (Step 000035): Train loss 0.663, Val loss 0.669, Train reward margins 0.658, Val reward margins 0.511\n",
            "Ep 1 (Step 000040): Train loss 0.666, Val loss 0.666, Train reward margins 0.597, Val reward margins 0.574\n",
            "Ep 1 (Step 000045): Train loss 0.648, Val loss 0.662, Train reward margins 0.982, Val reward margins 0.660\n",
            "Ep 1 (Step 000050): Train loss 0.648, Val loss 0.659, Train reward margins 0.993, Val reward margins 0.734\n",
            "Ep 1 (Step 000055): Train loss 0.647, Val loss 0.656, Train reward margins 1.014, Val reward margins 0.799\n",
            "Ep 1 (Step 000060): Train loss 0.652, Val loss 0.653, Train reward margins 0.893, Val reward margins 0.870\n",
            "Ep 1 (Step 000065): Train loss 0.631, Val loss 0.650, Train reward margins 1.361, Val reward margins 0.948\n",
            "Ep 1 (Step 000070): Train loss 0.618, Val loss 0.646, Train reward margins 1.699, Val reward margins 1.038\n",
            "Ep 1 (Step 000075): Train loss 0.617, Val loss 0.642, Train reward margins 1.733, Val reward margins 1.121\n",
            "Ep 1 (Step 000080): Train loss 0.592, Val loss 0.639, Train reward margins 2.333, Val reward margins 1.194\n",
            "Ep 1 (Step 000085): Train loss 0.610, Val loss 0.636, Train reward margins 1.907, Val reward margins 1.275\n",
            "Ep 1 (Step 000090): Train loss 0.650, Val loss 0.633, Train reward margins 0.964, Val reward margins 1.353\n",
            "Ep 1 (Step 000095): Train loss 0.607, Val loss 0.630, Train reward margins 1.962, Val reward margins 1.423\n",
            "Ep 1 (Step 000100): Train loss 0.600, Val loss 0.627, Train reward margins 2.127, Val reward margins 1.500\n",
            "Ep 1 (Step 000105): Train loss 0.590, Val loss 0.624, Train reward margins 2.458, Val reward margins 1.564\n",
            "Ep 1 (Step 000110): Train loss 0.607, Val loss 0.622, Train reward margins 1.976, Val reward margins 1.621\n",
            "Ep 1 (Step 000115): Train loss 0.621, Val loss 0.620, Train reward margins 1.605, Val reward margins 1.682\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Rewrite the sentence using a metaphor.  ### Input: The book is very interesting.  ### Response: The book is a treat.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: The assignment was written by the student.  ### Response\n",
            "Training completed in 1.69 minutes.\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(policy_model.parameters(), # we are only passing the parameters of the policy model into the `AdamW` optimizer; that's the model we want to optimize (we don't want to modify the reference model)\n",
        "                              lr=5e-6, # in DPO, it's best to use a very small learning rate\n",
        "                              weight_decay=0.01)\n",
        "\n",
        "\n",
        "tracking = train_model_dpo_simple(\n",
        "    policy_model=policy_model,\n",
        "    reference_model=reference_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=1, # we only train for 1 epoch; that's because DPO is very prone to collapse (the loss might improve, but the model will start generating nonsensical texts)\n",
        "    beta=0.1, # the beta value can be increased from 0.1 to 0.5 to reduce the effect of DPO (we use 0.1 here to make the results more noticeable)\n",
        "    eval_freq=5,\n",
        "    eval_iter=5,\n",
        "    start_context=format_input(val_data[2]),\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "AMfJh19cSYJb",
        "jxIApJ1kTUne",
        "Ee7Q-jnZK7pw"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
